{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V: Necessary and Sufficient Conditions for Optimality\n",
    "\n",
    "We begin by generalizing the necessary conditions for optimality from 2D. The **interior** of a set $X\\subset\\mathbb{R}^d$, denoted $\\text{int}(X)$ is the largest open subset of $\\mathbb{R}^d$ in $X$. In particular, ${\\bf x}\\in \\text{int}(X)$ if and only if there is an $\\varepsilon>0$ such that $B({\\bf x}, \\varepsilon)\\subset X$.\n",
    "\n",
    "#### Theorem (Necessary Conditions for Optimality): If $X\\subset\\mathbb{R}^d$, $f\\in C^1(X)$, and ${\\bf x}^\\ast\\in\\text{int}(X)$ is a minimizer of $f$, then $\\nabla f({\\bf x}^\\ast)={\\bf 0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points where $\\nabla f({\\bf x})={\\bf 0}$ are called **critical points**. There are several important types of critical points.\n",
    "\n",
    "1. ${\\bf x}$ is a **local minimizer** of $f:X\\rightarrow\\mathbb{R}$ if there is an $\\varepsilon>0$ such that $f({\\bf x})\\leq f({\\bf y})$ for all ${\\bf y}\\in B({\\bf x},\\varepsilon)\\cap X$.\n",
    "2. ${\\bf x}$ is a **local maximizer** of $f:X\\rightarrow\\mathbb{R}$ if there is an $\\varepsilon>0$ such that $f({\\bf x})\\geq f({bf y})$ for all ${\\bf y}\\in B({\\bf x},\\varepsilon)\\cap X$\n",
    "3. ${\\bf x}$ is a **global minimizer** of $f:X\\rightarrow\\mathbb{R}$ if $f({\\bf x})\\leq f({\\bf y})$ for all ${\\bf y}\\in X$. \n",
    "4. ${\\bf x}$ is a **global maximizer** of $f:X\\rightarrow\\mathbb{R}$ if $f({\\bf x})\\geq f({\\bf y})$ for all ${\\bf y}\\in X$. \n",
    "\n",
    "Note that a global minimizer in the interior of a set must also be a local minimizer. Thus, the second derivative test allows us to exclude all local maximizers from our search.\n",
    "\n",
    "#### Theorem (Second Derivative Test): Let $X\\subset\\mathbb{R}^d$ and $f\\in C^2(X)$. If ${\\bf x}^\\ast\\in \\text{int}(X)$ is a global minimizer, then $\\nabla^2 f({\\bf x}^\\ast)$ is positive definite.\n",
    "\n",
    "With a one more condition on $f$, the necessary conditions also become sufficient conditions.\n",
    "\n",
    "#### Theorem (Sufficient Conditions for Optimality): If $X\\subset\\mathbb{R}^d$ is a convex set, $f\\in C^1(X)$ is convex on $X$, and ${\\bf x}^\\ast\\in X$ satisfies $\\nabla f({\\bf x}^\\ast)={\\bf 0}$, then ${\\bf x}^\\ast$ is a minimizer of $f$ on $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "Consider the least squares program\n",
    "\n",
    "$$\n",
    "\\min_{(\\beta_0, \\beta_1,\\beta_2)\\in\\mathbb{R}^3} \\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1x_1^{(i)} - \\beta_2x_2^{(i)})^2,\n",
    "$$\n",
    "where we are given data\n",
    "\n",
    "| i        | 1 | 2 | 3 | 4 | 5 |\n",
    "| ---------|:-:|:-:|:-:|:-:|:-:|\n",
    "| $y_i$      | -1| -1|  0|  1|  2|\n",
    "| $x_1^{(i)}$| -1| -1|  0|  1|  0|\n",
    "| $x_2^{(i)}$|  0|  1| -1|  0|  1|\n",
    "\n",
    "Setting \n",
    "$$\n",
    "{\\bf y} = \\begin{pmatrix}\n",
    "-1\\\\\n",
    "-1\\\\\n",
    "0\\\\\n",
    "1\\\\\n",
    "2\n",
    "\\end{pmatrix},\\:\n",
    "X = \\begin{pmatrix}\n",
    "1 & -1 & 0\\\\\n",
    "1 & -1 & 1\\\\\n",
    "1 & 0 & -1\\\\\n",
    "1 & 1 & 0\\\\\n",
    "1 & 0 & 1\n",
    "\\end{pmatrix},\\text{ and } \\beta = \\begin{pmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\beta_2\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "we have that\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1x_1^{(i)} \\beta_2x_2^{(i)})^2 & = & \\sum_{i=1}^N \\left(y_i -\\begin{pmatrix}1 & x_1^{(i)} & x_2^{(i)}\\end{pmatrix}\\begin{pmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\end{pmatrix} \\right)^2\\\\\n",
    "&=&\\left\\Vert {\\bf y} - \\begin{pmatrix}\n",
    "1 & x_1^{(1)} & x_2^{(1)}\\\\\n",
    "1 & x_1^{(2)} & x_2^{(2)}\\\\\n",
    "1 & x_1^{(3)} & x_2^{(3)}\\\\\n",
    "1 & x_1^{(4)} & x_2^{(4)}\\\\\n",
    "1 & x_1^{(5)} & x_2^{(5)}\n",
    "\\end{pmatrix}\\begin{pmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\end{pmatrix}\\right\\Vert^2\\\\\n",
    "&=& \\Vert {\\bf y} - X \\beta \\Vert^2\n",
    "\\end{eqnarray}\n",
    "\n",
    "Now,\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Vert {\\bf y} - X \\beta \\Vert^2 &=& ({\\bf y} - X \\beta)^T({\\bf y} - X \\beta)\\\\\n",
    "&=&{\\bf y}^T{\\bf y} + {\\bf y}^T(-X\\beta)+ (-X\\beta)^T{\\bf y} + (-X\\beta)^T(-X\\beta)\\\\\n",
    "&=&\\Vert {\\bf y}\\Vert^2 - 2(X^T{\\bf y})^T\\beta + \\beta^T X^TX\\beta\\\\\n",
    "&=&7 - 2\\begin{pmatrix} 1 \\\\ 3 \\\\ 1\\end{pmatrix}^T\\beta +\\beta^T \\begin{pmatrix}\n",
    "5 & -1 & 1 \\\\\n",
    "-1 & 3 & -1\\\\\n",
    "1 & -1 & 3\n",
    "\\end{pmatrix}\\beta\n",
    "\\end{eqnarray}\n",
    "\n",
    "Let's use the **matrix product rule** ($\\partial(AB)=\\partial(A)B + A\\partial(B)$ for compatible matrices $A$ and $B$) to evaluate the partial derivatives of the sum of squares error function. We have\n",
    "\\begin{eqnarray}\n",
    "&\\partial_{\\beta_i}\\left(\\Vert {\\bf y}\\Vert^2 - 2(X^T{\\bf y})^T\\beta + \\beta^T X^TX\\beta\\right)\\\\\n",
    "=&-2\\partial_{\\beta_i}{\\bf y}^TX\\beta + \\partial_{\\beta_i}\\beta^T X^TX\\beta\\\\\n",
    "=&-2\\left(\\partial_{\\beta_i}({\\bf y}^TX)\\beta + {\\bf y}^T X\\partial_{\\beta_i}\\beta\\right) + (\\partial_{\\beta_i}\\beta)^T X^TX\\beta + \\beta^T(\\partial_{\\beta_i}(X^TX))\\beta + \\beta^T X^TX\\partial_{\\beta_i}\\beta\\\\\n",
    "=&-2{\\bf y}^T X {\\bf e}^{(i)} + ({\\bf e}^{(i)})^T X^TX\\beta + \\beta^T X^TX{\\bf e}^{(i)}\\\\\n",
    "=&2({\\bf e}^{(i)})^TX^T(X\\beta - {\\bf y}).\n",
    "\\end{eqnarray}\n",
    "\n",
    "That is, the $i$th entry of the gradient of the SSE function is the $i$th entry of $2X^T(X\\beta - {\\bf y})$, and hence $\\nabla\\text{SSE}(\\beta) = 2X^T(X\\beta - {\\bf y})$. \n",
    "\n",
    "We use the matrix product rule again to compute the columns of the Hessian. Noting that $\\partial_{\\beta_i}\\nabla\\text{SSE}(\\beta)$ is the $i$th column of the Hessian, we see that\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\partial_{\\beta_i} \\nabla\\text{SSE}(\\beta) &=& \\partial_{\\beta_i}\\left(2X^T(X\\beta - {\\bf y})\\right)\\\\\n",
    "&=& \\partial_{\\beta_i}\\left(2(X^T)(X\\beta - {\\bf y})\\right)\\\\\n",
    "&=&2 \\partial_{\\beta_i}(X)^T(X\\beta - {\\bf y}) + 2 X^T\\partial_{\\beta_i}(X\\beta - {\\bf y})\\\\\n",
    "&=&2 (\\partial_{\\beta_i}X)^T(X\\beta - {\\bf y}) + 2 X^T\\partial_{\\beta_i}(X\\beta - {\\bf y})\\\\\n",
    "&=&{\\bf 0} + 2 X^T(X{\\bf e}^{(i)}-{\\bf 0}) \\\\\n",
    "&=& 2X^TX {\\bf e}^{(i)}.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Thus, the $i$th column of the Hessian is the $i$th column of $2X^TX$, and hence $\\nabla^2\\text{SSE}(\\beta) = 2 X^TX$. That is, the Hessian is the matrix-valued function which outputs the constant matrix $2X^TX$ for any $\\beta\\in\\mathbb{R}^3$. Because $2X^TX$ is of the form $Y^TY$, it is always positive semidefinite. To verify positive definiteness, we check the Sylvester criterion: the first principal minor of $2X^TX$ is $10$, \n",
    "$$\n",
    "\\det\\left(2\\begin{pmatrix}\n",
    "5 & -1\\\\\n",
    "-1 & 3\n",
    "\\end{pmatrix}\\right) = 4(15-1) = 56>0,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\det\\left(2\\begin{pmatrix}\n",
    "5 & -1 & 1\\\\\n",
    "-1 & 3 & -1\\\\\n",
    "1 & -1 & 3\n",
    "\\end{pmatrix}\\right) = 8\\left((5)(9-1) -(-1)(-3+1)+(1)(1 - 3)\\right)=8(40-2-2)=8(36)>0.\n",
    "$$\n",
    "Since the Hessian is positive definite for all $\\beta\\in\\mathbb{R}^3$, $\\text{SSE}(\\beta)$ is strictly convex by the second order conditions for convexity. This also means that the necessary conditions for optimality $2X^T(X\\beta-{\\bf y})={\\bf 0}$ become sufficient conditions, and we have that the solution to the original minimization program is\n",
    "\n",
    "$$\n",
    "\\beta^\\ast = (X^TX)^{-1}X^T{\\bf y}.\n",
    "$$\n",
    "\n",
    "In linear regression, $X^T X\\beta =X^T{\\bf y}$ are the **normal equations**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VI: Equality Constrained Optimization\n",
    "\n",
    "\n",
    "## Linear Algebra Review\n",
    "\n",
    "To perform a deep analysis of equality constrained optimization, we recall the following definitions from linear algebra:\n",
    "\n",
    "1. The **span** of a collection of vectors $\\{{\\bf v}^{(i)}\\}_{i=1}^N\\subset\\mathbb{R}^d$ is the set of **linear combinations** of the ${\\bf v}^{(i)}$'s:\n",
    "$$\n",
    "\\text{span}(\\{{\\bf v}^{(i)}\\}_{i=1}^N) = \\left\\{{\\bf x}\\in\\mathbb{R}^d:{\\bf x}=\\sum_{i=1}^Nc_i{\\bf v}^{(i)}\\text{ for some }{\\bf c}\\in\\mathbb{R}^N\\right\\}.\n",
    "$$\n",
    "2. A collection of vectors $\\{{\\bf v}^{(i)}\\}_{i=1}^N\\subset\\mathbb{R}^d$ is called **linearly independent** if $\\sum_{i=1}^n c_i {\\bf v}^{(i)}=0$ if and only if ${\\bf c}={\\bf 0}\\in\\mathbb{R}^N$.\n",
    "3. A collection of vectors $\\{{\\bf v}^{(i)}\\}_{i=1}^N\\subset\\mathbb{R}^d$ is called a **basis** for $\\mathbb{R}^d$ if $\\text{span}(\\{{\\bf v}^{(i)}\\}_{i=1}^N)=\\mathbb{R}^d$ (in this case, we say that the ${\\bf v}^{(i)}$'s span $\\mathbb{R}^d$) and $\\{{\\bf v}^{(i)}\\}_{i=1}^N$ are a linearly independent collection. Note that if $\\{{\\bf v}^{(i)}\\}_{i=1}^N$ is a basis for $\\mathbb{R}^d$, it necessarily follows that $N=d$.\n",
    "4. A collection of vectors $\\{{\\bf u}^{(i)}\\}_{i=1}^d\\subset\\mathbb{R}^d$ is an **orthonormal basis** for $\\mathbb{R}^d$ if $\\Vert {\\bf u}^{(i)}\\Vert=1$ for all $i=1, 2,\\ldots, d$ and $({\\bf u}^{(i)})^T{\\bf u}^{(j)}=0$ for all $i\\not=j$. An orthonormal basis is necessarily a basis.\n",
    "\n",
    "For example, the standard orthonormal basis, $\\{{\\bf e}^{(i)}\\}_{i=1}^d\\subset\\mathbb{R}^d$ is an orthonormal basis for $\\mathbb{R}^d$.\n",
    "\n",
    "Recall that a **subspace** $\\mathcal{V}$ of $\\mathbb{R}^d$ is a set which is closed under scalar multiplication and vector addition. That is, if $a\\in\\mathbb{R}$, ${\\bf x},{\\bf y}\\in\\mathcal{V}$, then $a\\cdot{\\bf x}, {\\bf x}+{\\bf y}\\in\\mathcal{V}$. Note that $\\text{span}(\\{{\\bf v}^{(i)}\\}_{i=1}^N)$ is always a subspace of $\\mathbb{R}^d$ if $\\{{\\bf v}^{(i)}\\}_{i=1}^N\\subset\\mathbb{R}^d$. For any matrix $A\\in M_{m,n}$, the **kernel** or **null space** of $A$ is the set $\\text{ker}(A)=\\{{\\bf x}\\in\\mathbb{R}^n:A{\\bf x}={\\bf 0}\\}$. It is easy to show that $\\text{ker}(A)$ is always a subspace of $\\mathbb{R}^n$.\n",
    "\n",
    "As above, a basis for $\\mathcal{V}$ is a linearly indpendent collection which spans $\\mathcal{V}$. The **dimension** of $\\mathcal{V}$, $\\text{dim}(\\mathcal{V})$, is the number of elements in any basis of $\\mathcal{V}$. For example, if $\\{{\\bf e}^{(i)}\\}_{i=1}^d\\subset\\mathbb{R}^3$, then $\\mathcal{V}=\\text{span}(\\{{\\bf e}^{(1)},{\\bf e}^{(2)}\\})$ has $\\text{dim}(\\mathcal{V})=2$.\n",
    "\n",
    "We also need to recall the notion of the **rank** of a matrix $A\\in\\mathcal{M}_{m,n}$. The rank of the matrix $A$, $\\text{rank}(A)$ is any one of the following equivalent quantities:\n",
    "\n",
    "1. The number of pivot columns in the (reduced) row echelon form of $A$\n",
    "2. The largest number of column vectors of $A$ which forms a linearly independent set\n",
    "3. The largest number of row vectors of $A$ which forms a linearly independent set\n",
    "4. The dimension of the **row space**, $\\text{span}(\\{{\\bf r}^{(i)}\\}_{i=1}^m)$ where $A=\\begin{pmatrix} {\\bf r}^{(1)}\\\\\\vdots\\\\ {\\bf r}^{(m)}\\end{pmatrix}$.\n",
    "5. The dimension of the **column space**, $\\text{span}(\\{{\\bf c}^{(i)}\\}_{i=1}^n)$ where $A=\\begin{pmatrix} {\\bf c}^{(1)}& \\cdots & {\\bf c}^{(m)}\\end{pmatrix}$.\n",
    "6. $m - \\text{dim}(\\text{ker}(A^T))$ (the Rank-Nullity theorem)\n",
    "7. $n - \\text{dim}(\\text{ker}(A))$\n",
    "\n",
    "A matrix $A\\in M_{m,n}$ is said to be **full rank** if $\\text{rank}(A)=\\min(m,n)$. A useful way to characterize full rank matrices is in terms of the linear transformation $f_A:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$. To state this characterization, recall that a function $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$ is said to be **surjective** or **onto** if for every ${\\bf z}\\in\\mathbb{R}^m$ there is a ${\\bf x}\\in\\mathbb{R}^n$ such that $f({\\bf x})={\\bf z}$, and the function is **injective** or **one-to-one** if $f({\\bf x})=f({\\bf y})$ implies ${\\bf x}={\\bf y}$ for all ${\\bf x}, {\\bf y}\\in\\mathbb{R}^n$. \n",
    "\n",
    "#### Theorem: Suppose $A\\in M_{m,n}$. If $m\\leq n$, then $A$ has full rank if and only if $f_A$ is surjective. If $n\\leq m$, then $A$ has full rank if and only if $f_A$ is injective. \n",
    "\n",
    "Finally, we note that in the case $n\\leq m$, $A$ has full rank if and only if $\\text{ker}(A)=\\{{\\bf 0}\\}$, if and only if the columns of $A$ form a linearly independent set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariable Differentiation\n",
    "\n",
    "A **vector-valued** function $g:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$ is written as \n",
    "$$\n",
    "g({\\bf x}) = \\begin{pmatrix}\n",
    "g_1({\\bf x})\\\\\n",
    "g_2({\\bf x})\\\\\n",
    "\\vdots\\\\\n",
    "g_m({\\bf x})\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "where $g_1,g_2,\\ldots, g_m:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ are the **component functions**. We shall say that $g\\in C^1(\\mathbb{R}^n;\\mathbb{R}^m)$ if $g_i\\in C^1(\\mathbb{R}^n)$ for all $i=1, 2,\\ldots, m$. \n",
    "\n",
    "Much of the theory of optimization (in particular, necessary conditions) is built from first order approximations because first order approximations encode certain local behavior of a function. Therefore it is useful to understand the first order approximation to $g\\in C^1(\\mathbb{R}^n;\\mathbb{R}^m)$. In particular, we can simply write the first order approximation of $g$ in terms of the first order approximations of the component functions. Given ${\\bf x},{\\bf y}\\in \\mathbb{R}^n$,\n",
    "\n",
    "$$\n",
    "g({\\bf y}) = \\begin{pmatrix}\n",
    "g_1({\\bf y})\\\\\n",
    "g_2({\\bf y})\\\\\n",
    "\\vdots\\\\\n",
    "g_m({\\bf y})\n",
    "\\end{pmatrix} \\approx \\begin{pmatrix}\n",
    "g_1({\\bf x}) + \\nabla g_1({\\bf x})^T({\\bf y}-{\\bf x})\\\\\n",
    "g_2({\\bf x}) + \\nabla g_2({\\bf x})^T({\\bf y}-{\\bf x})\\\\\n",
    "\\vdots\\\\\n",
    "g_m({\\bf x}) + \\nabla g_m({\\bf x})^T({\\bf y}-{\\bf x})\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and some linear algebra gives us\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "g_1({\\bf x}) + \\nabla g_1({\\bf x})^T({\\bf y}-{\\bf x})\\\\\n",
    "g_2({\\bf x}) + \\nabla g_2({\\bf x})^T({\\bf y}-{\\bf x})\\\\\n",
    "\\vdots\\\\\n",
    "g_m({\\bf x}) + \\nabla g_m({\\bf x})^T({\\bf y}-{\\bf x})\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "g_1({\\bf x})\\\\\n",
    "g_2({\\bf x})\\\\\n",
    "\\vdots\\\\\n",
    "g_m({\\bf x})\n",
    "\\end{pmatrix} + \\begin{pmatrix}\n",
    "\\nabla g_1({\\bf x})^T\\\\\n",
    "\\nabla g_2({\\bf x})^T\\\\\n",
    "\\vdots\\\\\n",
    "\\nabla g_m({\\bf x})^T\n",
    "\\end{pmatrix}({\\bf y}-{\\bf x}) = g({\\bf x}) + Dg({\\bf x})({\\bf y}-{\\bf x}),\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "Dg({\\bf x}) =  \\begin{pmatrix}\n",
    "\\nabla g_1({\\bf x})^T\\\\\n",
    "\\nabla g_2({\\bf x})^T\\\\\n",
    "\\vdots\\\\\n",
    "\\nabla g_m({\\bf x})^T\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "\\frac{\\partial g_1}{\\partial x_1}({\\bf x}) & \\frac{\\partial g_1}{\\partial x_2}({\\bf x}) &\\cdots & \\frac{\\partial g_1}{\\partial x_{n}}({\\bf x})\\\\\n",
    "\\frac{\\partial g_2}{\\partial x_1}({\\bf x}) & \\frac{\\partial g_2}{\\partial x_2}({\\bf x}) & \\cdots & \\frac{\\partial g_2}{\\partial x_{n}}({\\bf x})\\\\\n",
    "\\vdots & \\vdots &\\ddots & \\vdots\\\\\n",
    "\\frac{\\partial g_m}{\\partial x_1}({\\bf x}) & \\frac{\\partial g_m}{\\partial x_2}({\\bf x}) &\\cdots & \\frac{\\partial g_m}{\\partial x_{n}}({\\bf x})\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "is the **Jacobian** of $g$ at ${\\bf x}$. Thus, the Jacbian encodes the first order approximation of a differentiable vector valued function. \n",
    "\n",
    "The definition of the Jacobian also allows us to write down a general **multivariate chain rule**. If $f\\in C^1(\\mathbb{R}^n;\\mathbb{R}^k)$ and $g\\in C^1(\\mathbb{R}^k;\\mathbb{R}^m)$, then $g\\circ f\\in C^1(\\mathbb{R}^n;\\mathbb{R}^m)$ where $(g\\circ f)({\\bf x})=g(f({\\bf x}))$. In addition, the **multivariate chain rule** holds:\n",
    "\n",
    "$$\n",
    "D(g\\circ f)({\\bf x}) = Dg(f({\\bf x)})) Df({\\bf x}).\n",
    "$$\n",
    "\n",
    "If $g:\\mathbb{R}^k\\rightarrow\\mathbb{R}$, $Dg({\\bf y})=\\nabla g({\\bf y})^T$, and hence we have the formula\n",
    "\n",
    "$$\n",
    "\\nabla (g\\circ f){\\bf x} = Df({\\bf x})^T \\nabla g(f({\\bf x})).\n",
    "$$\n",
    "\n",
    "In these formulas, $Dg(f({\\bf x}))$ is understood to be the Jacobian of $g$ evaluated at the point $f({\\bf x})$, and $\\nabla g(f({\\bf x}))$ is the gradient of $g$ evaluated at $f({\\bf x})$. That is, the differential operators only apply to the first function to their immediate right.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Equality Constraint\n",
    "\n",
    "For $f,g\\in C^1(\\mathbb{R}^d)$, consider the program\n",
    "\n",
    "$$\n",
    "\\min f({\\bf x})\\text{ subject to }g({\\bf x})=0,\n",
    "$$\n",
    "\n",
    "and suppose ${\\bf x}^\\ast$ is a solution to this program. In the 2D case, we found a local parameterization of the constriant $g({\\bf x})=0$ around ${\\bf x}^\\ast$, and used the chain rule to justify the Lagrange conditions from the necessary conditions for optimality in 1D unconstrained optimization. \n",
    "\n",
    "In 2D, a **local parameterization** of $g({\\bf x})=0$ at ${\\bf x}^\\ast$ is a map $\\varphi:(-\\varepsilon,\\varepsilon)\\rightarrow\\mathbb{R}^d$ satisfying\n",
    "\n",
    "1. $\\varphi(0)={\\bf x}^\\ast$\n",
    "2. $g(\\varphi(y))=0$ for all $y\\in(-\\varepsilon,\\varepsilon)$\n",
    "3. $\\varphi^\\prime(0)\\not={\\bf 0}$\n",
    "\n",
    "In general, a **local parameterization** of $g({\\bf x})=0$ at ${\\bf x}^\\ast$ is a map $\\varphi\\in C^1(B_{\\mathbb{R}^{d-1}}({\\bf 0},\\varepsilon);\\mathbb{R}^d)$ satisfying\n",
    "\n",
    "1. $\\varphi({\\bf 0})={\\bf x}^\\ast$\n",
    "2. $g(\\varphi({\\bf y}))=0$ for all ${\\bf y}\\in B_{\\mathbb{R}^{d-1}}({\\bf 0},\\varepsilon)$\n",
    "3. $D\\varphi({\\bf 0})$ has full rank\n",
    "\n",
    "If we have local parameterization of $g({\\bf x})=0$ at the minimizer ${\\bf x}^\\ast$, we see that ${\\bf 0}$ is a minimizer of $f\\circ \\varphi$ over $B_{\\mathbb{R}^{d-1}}({\\bf 0},\\varepsilon)$: $(f\\circ \\varphi)({\\bf 0})=f(\\varphi({\\bf 0}))=f({\\bf x}^\\ast)\\leq f(\\varphi({\\bf y}))$ for all ${\\bf y}\\in B_{\\mathbb{R}^{d-1}}({\\bf 0},\\varepsilon)$ since $g(\\varphi({\\bf y}))=0$. Moreover, ${\\bf 0}$ is in the interior of $B_{\\mathbb{R}^{d-1}}({\\bf 0},\\varepsilon)$ and thus the necessary conditions for optimality hold at ${\\bf 0}). Combining this with the chain rule, we have\n",
    "\n",
    "$$\n",
    "{\\bf 0} = \\nabla (f\\circ \\varphi)({\\bf 0})=D\\varphi({\\bf 0})^T\\nabla f(\\varphi({\\bf 0}))=D\\varphi({\\bf 0})^T\\nabla f({\\bf x}^\\ast).\n",
    "$$\n",
    "\n",
    "On the other hand,\n",
    "\n",
    "$$\n",
    "{\\bf 0} = \\nabla (g\\circ \\varphi)({\\bf 0})=D\\varphi({\\bf 0})^T\\nabla g({\\bf x}^\\ast).\n",
    "$$\n",
    "\n",
    "This means that $\\nabla f({\\bf x}^\\ast), \\nabla g({\\bf x}^\\ast)\\in \\text{ker}(D\\varphi({\\bf 0})^T)$. Since $D\\varphi({\\bf 0})$ has full rank, it has rank $d-1$. By the rank-nullity theorem, this means that $\\text{dim}(\\text{ker}(D\\varphi({\\bf 0})^T))=d-(d-1)=1$. To summarize, $\\nabla f({\\bf x}^\\ast), \\nabla g({\\bf x}^\\ast)$ must necessarily live in the same one-dimensional subspace. If we have that $\\nabla g({\\bf x}^\\ast)\\not={\\bf 0}$, we are then guaranteed a Lagrange multiplier $\\lambda\\in\\mathbb{R}$ satisfying \n",
    "\n",
    "$$\n",
    "\\nabla f({\\bf x}^\\ast) + \\lambda \\nabla g({\\bf x}^\\ast)={\\bf 0}.\n",
    "$$\n",
    "\n",
    "However, the above analysis was all predicated on the existence of a local parameterization of the constraint at ${\\bf x}^\\ast$, and also that $\\nabla g({\\bf x}^\\ast)\\not={\\bf 0}$. The Implicit Function Theorem I justifies why we only need to establish $\\nabla g({\\bf x}^\\ast)\\not={\\bf 0}$.\n",
    "\n",
    "#### Theorem (Implicit Function Theorem I): If $g\\in C^1(\\mathbb{R}^d)$, and ${\\bf x}^\\ast\\in\\mathbb{R}^d$ satisfies $g({\\bf x}^\\ast)=0$ and $\\nabla g({\\bf x}^\\ast)\\not={\\bf 0}$, then there exists an $\\varepsilon>0$ and a local parameterization, $\\varphi\\in C^1(B_{\\mathbb{R}^{d-1}}({\\bf 0},\\varepsilon);\\mathbb{R}^d)$ of the constraint $g({\\bf x})=0$ at ${\\bf x}^\\ast$. \n",
    "\n",
    "Because of the Implict Function Theorem I, we get a general theory for Lagrange multipliers.\n",
    "\n",
    "#### Theorem (Necessary Conditions for Constrained Optimality): Suppose $f,g\\in C^1(\\mathbb{R}^d)$, ${\\bf x}^\\ast$ is a solution to $\\min f({\\bf x}) \\text{ subject to }g({\\bf x})=0$, and $\\nabla g({\\bf x}^\\ast)\\not={\\bf 0}$. Then there exists a $\\lambda\\in\\mathbb{R}$ such that $\\nabla f({\\bf x}^\\ast) = \\lambda\\nabla g({\\bf x}^\\ast)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on the hypersphere\n",
    "\n",
    "Consider the program\n",
    "$$\n",
    "\\min_{{\\bf x}\\in\\mathbb{R}^d} {\\bf x}^T A {\\bf x}\\text{ subject to } \\Vert {\\bf x}\\Vert=1.\n",
    "$$\n",
    "Setting $f({\\bf x}) = {\\bf x}^T A {\\bf x}$ and $g({\\bf x})=\\Vert x\\Vert^2-1$ converts the program to the standard form\n",
    "$$\n",
    "\\min f({\\bf x})\\text{ subject to } g({\\bf x})=0\n",
    "$$\n",
    "where both functions are differentiable. Now, $\\nabla f({\\bf x})=2A{\\bf x}$ and $\\nabla g({\\bf x})=2{\\bf x}$. If ${\\bf x}^\\ast$ is a solution, we have to have that $\\Vert {\\bf x}^\\ast\\Vert=1$, so ${\\bf x}^\\ast\\not=0$ by positivity of the norm, and hence $\\nabla g({\\bf x}^\\ast)=2{\\bf x}^\\ast\\not=0$. Thus, the hypotheses of the necessary conditions hold and we can begin searching for a Lagrange multiplier.\n",
    "\n",
    "Now, the Lagrange condition becomes $A{\\bf x}=\\lambda {\\bf x}$ for some $\\lambda\\in\\mathbb{R}^d$. Thus, any solution ${\\bf x}^\\ast$ is an eigenvector of $A$ with eigenvalue $\\lambda$. Moreover, \n",
    "$$\n",
    "({\\bf x}^\\ast)^T A{\\bf x}^\\ast = ({\\bf x}^\\ast)^T (\\lambda {\\bf x}^\\ast)= \\lambda \\Vert {\\bf x}^\\ast\\Vert^2=\\lambda.\n",
    "$$\n",
    "Thus, the solution ${\\bf x}^\\ast$ is any eigenvector whose eigenvalue coincides with the lowest eigenvalue of the matrix $A$. \n",
    "\n",
    "One way to find the eigenvectors and eigenvalues of a symmetric positive definite matrix $A$ is by solving the sequence of problems\n",
    "$$\n",
    "\\min - {\\bf x}^T A {\\bf x}\\text{ subject to }\\Vert {\\bf x}\\Vert^2=1,\n",
    "$$\n",
    "$$\n",
    "\\min - {\\bf x}^T A {\\bf x}\\text{ subject to }\\Vert {\\bf x}\\Vert^2=1\\text{ and } ({\\bf u}^{(1)})^T{\\bf x}=0,\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\min - {\\bf x}^T A {\\bf x}\\text{ subject to }\\Vert {\\bf x}\\Vert^2=1\\text{ and } ({\\bf u}^{(i)})^T{\\bf x}=0\\text{ for }i=1,\\ldots, k\n",
    "$$\n",
    "for $k=1,\\ldots, d-1$. The resulting sequence of ${\\bf u}^{(i)}$'s will then form an orthonormal basis of $\\mathbb{R}$ consisting of eigenvectors of $A$ with eigenvalues listed in non-increasing order. To determine solutions to this, we need a theory for Lagrange multipliers when there are multiple equality constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Conditions for Multiple Equality Constraints\n",
    "\n",
    "An optimization program with multiple equality constraints may be written as \n",
    "$$\n",
    "\\min f({\\bf x})\\text{ subject to } g({\\bf x})={\\bf 0}\n",
    "$$\n",
    "where $f\\in C^1(\\mathbb{R}^d)$ and $g\\in C^1(\\mathbb{R}^d;\\mathbb{R}^k)$ with $k\\leq d$. To get necessary conditions for optimality of ${\\bf x}^\\ast\\in\\mathbb{R}^d$, we now search for a $\\varphi\\in C^1(B_{\\mathbb{R}^{d-k}}({\\bf 0},\\varepsilon);\\mathbb{R}^d)$ such that\n",
    "\n",
    "1. $\\varphi({\\bf 0})={\\bf x}^\\ast$\n",
    "2. $g(\\varphi({\\bf y}))={\\bf 0}$ for all ${\\bf y}\\in B_{\\mathbb{R}^{d-k}}({\\bf 0},\\varepsilon)$\n",
    "3. $D\\varphi({\\bf 0})$ has full rank.\n",
    "\n",
    "That is, $\\varphi$ is a local parameterization of the constraint $g({\\bf x})={\\bf 0}$ at ${\\bf x}^\\ast$. Again, we will have that ${\\bf 0}$ is a minimizer of $f\\circ\\varphi$ over $B_{\\mathbb{R}^{d-k}}({\\bf 0},\\varepsilon)$ in the interior, and hence\n",
    "\n",
    "$$\n",
    "{\\bf 0} = \\nabla (f\\circ \\varphi)({\\bf 0})=D\\varphi({\\bf 0})^T \\nabla f(\\varphi({\\bf 0}))= D\\varphi({\\bf 0})^T \\nabla f({\\bf x}^\\ast).\n",
    "$$\n",
    "\n",
    "Moreover,\n",
    "\n",
    "$$\n",
    "{\\bf 0} = D(g\\circ\\varphi)({\\bf 0}) = Dg(\\varphi({\\bf 0})) D\\varphi({\\bf 0})=Dg({\\bf x}^\\ast) D\\varphi({\\bf 0}).\n",
    "$$\n",
    "\n",
    "Thus, we have that $\\nabla f({\\bf x}^\\ast)$ and the rows of $Dg({\\bf x}^\\ast)$ are all in $\\text{ker}(D\\varphi({\\bf 0})^T$. In other works, $\\nabla f({\\bf x}^\\ast)$, $\\nabla g_1({\\bf x}^\\ast),\\ldots, \\nabla g_n({\\bf x}^\\ast)$ are all in $\\text{ker}(D\\varphi({\\bf 0})^T)$. \n",
    "\n",
    "If we further suppose that $D g({\\bf x}^\\ast)$ has full rank, then the gradients of the components of $g$ are a linearly independent set of $n$ vectors in the $\\text{ker}(D\\varphi({\\bf 0})^T)$. Since $\\text{dim}(\\text{ker}(D\\varphi({\\bf 0})^T))=d-(d-n)=n$, it follows that $\\{\\nabla g_i({\\bf x}^\\ast)\\}_{i=1}^n$ form a basis for the kernel of $D\\varphi({\\bf 0})^T$. Since $\\nabla f({\\bf x}^\\ast)$ is also in the kernel, there are coefficients $\\lambda_1,\\ldots,\\lambda_n\\in\\mathbb{R}$ such that\n",
    "\n",
    "$$\n",
    "\\nabla f({\\bf x}^\\ast) = \\lambda_1 \\nabla g_1({\\bf x}^\\ast) +\\lambda_2 \\nabla g_2({\\bf x}^\\ast)+\\cdots + \\lambda_n \\nabla g_n({\\bf x}^\\ast).\n",
    "$$\n",
    "\n",
    "Again, we have to impose the condition that the gradients of the component functions at ${\\bf x}^\\ast$ form a linearly independent set to ensure this representation. The general Implicit Function Theorem tells us that this hypothesis also ensures the existence of the parameterization $\\varphi$.\n",
    "\n",
    "#### Theorem (Implicit Function Theorem): If $g\\in C^1(\\mathbb{R}^d;\\mathbb{R}^n)$, and ${\\bf x}^\\ast\\in\\mathbb{R}^d$ satisfies $g({\\bf x}^\\ast)={\\bf 0}$ and $D g({\\bf x}^\\ast)$ has full rank, then there exists an $\\varepsilon>0$ and a local parameterization, $\\varphi\\in C^1(B_{\\mathbb{R}^{d-n}}({\\bf 0},\\varepsilon);\\mathbb{R}^d)$ of the constraint $g({\\bf x})={\\bf 0}$ at ${\\bf x}^\\ast$. \n",
    "\n",
    "Because of the Implict Function Theorem, we get a general theory for Lagrange multipliers in the case of multiple inequality constraints.\n",
    "\n",
    "#### Theorem (Necessary Conditions for Constrained Optimality): Suppose $f\\in C^1(\\mathbb{R}^d)$, $g\\in C^1(\\mathbb{R}^d;\\mathbb{R}^n)$, ${\\bf x}^\\ast$ is a solution to $\\min f({\\bf x}) \\text{ subject to }g({\\bf x})={\\bf 0}$, and that $Dg({\\bf x}^\\ast)$ has full rank. Then there exists $\\lambda_1,\\ldots,\\lambda_n\\in\\mathbb{R}$ such that $\\nabla f({\\bf x}^\\ast) = \\lambda_1\\nabla g_1({\\bf x}^\\ast)+\\cdots+\\lambda_n g_n({\\bf x}^\\ast)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VII: General Constrained Optimization\n",
    "\n",
    "In 2D constrained optimization, we were able to decompose inequality constrained optimization programs into a search over the interior of the feasible region and the various parts of the boundary of the feasible region. The intution from 2D generalizes nicely, but now we may encounter general constrained programs of the form\n",
    "\n",
    "$$\n",
    "\\min f({\\bf x})\\text{ subject to } g_1({\\bf x})=0, \\ldots, g_n({\\bf x})=0, h_1({\\bf x})\\leq 0,\\ldots, h_m({\\bf x})\\leq 0\n",
    "$$\n",
    "\n",
    "There are a few special types programs to consider:\n",
    "\n",
    "1. If $f,g_1,\\ldots, g_n, h_1,\\ldots, h_m\\in C^1(\\mathbb{R}^d)$, then this is a differentiable optimization program.\n",
    "2. If $f,g_1,\\ldots, g_n, h_1,\\ldots, h_m:\\mathbb{R}^d\\rightarrow\\mathbb{R}$ are all affine functions, then this is a **linear program**.\n",
    "3. If $f, h_1,\\ldots, h_m:\\mathbb{R}^d\\rightarrow\\mathbb{R}$ are all convex functions and $g_1,\\ldots, g_n:\\mathbb{R}^d\\rightarrow\\mathbb{R}$ are all affine, then this is a **convex program**. \n",
    "\n",
    "For any differentiable optimization program, we consider the domain of optimization piece by piece. For every single piece, if ${\\bf x}^\\ast$ is a solution to the program on that piece, we will have that certain inequality constraints are **binding** ($h_j({\\bf x}^\\ast)=0$ for $j\\in B$) and some are **non-binding** ($h_j({\\bf x}^\\ast)<0$ for $j\\not\\in B$). In this case, ${\\bf x}^\\ast$ is also a solution to the program \n",
    "\n",
    "$$\n",
    "\\min f({\\bf x})\\text{ subject to } g_i({\\bf x})=0\\text{ for all }i=1,\\ldots,n; h_j({\\bf x}=0\\text{ for all }j\\in B, h_j({\\bf x})<0\\text{ for all }j\\not\\in B.\n",
    "$$\n",
    "\n",
    "In particular, setting $B=\\{j_1, j_2,\\ldots, j_k\\}$, if the Jacobian of map\n",
    "\n",
    "$$\n",
    "{\\bf x}\\longmapsto \\begin{pmatrix}\n",
    "g_1({\\bf x})\\\\\n",
    "\\vdots\\\\\n",
    "g_n({\\bf x})\\\\\n",
    "h_{j_1}({\\bf x})\\\\\n",
    "\\vdots\\\\\n",
    "h_{j_k}({\\bf x})\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "has full rank at ${\\bf x}^\\ast$, then the Lagrange conditions must hold at ${\\bf x}^\\ast$. The full rank condition is equivalent to the **linear independence constraint qualification** (LICQ): the gradients\n",
    "\n",
    "$$\n",
    "\\nabla g_1({\\bf x}^\\ast), \\ldots, \\nabla g_n({\\bf x}^\\ast), \\nabla h_{j_1}({\\bf x}^\\ast), \\ldots, \\nabla h_{j_2}({\\bf x}^\\ast)\n",
    "$$\n",
    "\n",
    "form a linearly independent collection. If the LICQ holds at an optimal ${\\bf x}^\\ast$, then the Lagrange conditions must also hold, and the subsystem of equations from the KKT conditions holds:\n",
    "\n",
    "1. (Stationarity) $\\nabla f({\\bf x}^\\ast) + \\lambda_1 g_1({\\bf x}^\\ast) + \\cdots + \\lambda_n g_n({\\bf x}^\\ast) + \\mu_{j_1} h_{j_1}({\\bf x}^\\ast) + \\cdots + \\mu_{j_k} h_{j_k}({\\bf x}^\\ast)={\\bf 0}$\n",
    "2. (Primal Feasibility) $g_i({\\bf x})=0\\text{ for all }i=1,\\ldots,n; h_j({\\bf x}=0\\text{ for all }j\\in B, h_j({\\bf x})<0\\text{ for all }j\\not\\in B$\n",
    "3. (Dual Feasibility) $\\mu_{j_1}\\geq 0, \\ldots, \\mu_{j_k}\\geq 0$\n",
    "\n",
    "The choice of binding inequality constraints ensures that the complementary slackness conditions hold in the following way: $h_{j_a}({\\bf x}^\\ast)=0$, and therefore $\\mu_{j_a}h_{j_a}({\\bf x}^\\ast)=0$ for all $j_a\\in B$; for $j_a\\not\\in B$, $\\mu_{j_a}=0$, and therefore $\\mu_{j_a}h_{j_a}({\\bf x}^\\ast)$. This is a good place to emphasize the fact that the \"piece\" of the feasible region is determined by choosing whether $\\mu_j=0$ or $h_j({\\bf x})=0$ for each $j$. Thus, ${\\bf x}^\\ast$ satisfies the KKT conditions for some choice of $\\lambda_1,\\ldots, \\lambda_n,\\mu_1,\\ldots,\\mu_m\\in\\mathbb{R}$:\n",
    "\n",
    "1. (Stationarity) $\\nabla f({\\bf x}^\\ast) + \\lambda_1 g_1({\\bf x}^\\ast) + \\cdots + \\lambda_n g_n({\\bf x}^\\ast) + \\mu_{1} h_{1}({\\bf x}^\\ast) + \\cdots + \\mu_{m} h_{m}({\\bf x}^\\ast)={\\bf 0}$\n",
    "2. (Primal Feasibility) $g_i({\\bf x})=0\\text{ for all }i=1,\\ldots,n; h_j({\\bf x}\\leq 0\\text{ for all }j=1,\\ldots, m$\n",
    "3. (Dual Feasibility) $\\mu_{1}\\geq 0, \\ldots, \\mu_{m}\\geq 0$\n",
    "4. (Complementary Slackness) $\\mu_j h_j({\\bf x})=0$ for all $j=1,\\ldots, m$\n",
    "\n",
    "Of course, we do not know a priori which inequality constraints are binding at a solution ${\\bf x}^\\ast$, and having ${\\bf x}^\\ast$ unkown means that we cannot immediately check the LICQ. However, we can still perform an exhaustive search:\n",
    "\n",
    "1. Identify all points ${\\bf x}$ where the LICQ fails\n",
    "2. For the points where the LICQ holds, find all points for which there are $\\lambda_1,\\ldots,\\lambda_n,\\mu_1,\\ldots, \\mu_m\\in\\mathbb{R}$ such that the KKT conditions hold.\n",
    "3. Check all points from (1) and all points from (2) to determine the minimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Consider the linear program \n",
    "\n",
    "$$\n",
    "\\min x + y + z\\text{ subject to } 3x + 2y + z = 1, x\\geq 0, y\\geq 0, z\\geq 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifications of the LICQ and Slater's Condition\n",
    "\n",
    "Showing that the LICQ holds is very tedious, so it is nice to know circumstances where the LICQ simplifies. For example, if the constraint functions are all affine, then the KKT conditions must hold for any solution. In particular, solutions to linear programs satisfy the KKT conditions. \n",
    "\n",
    "For convex programs, the KKT conditions hold for any solution provided that **Slater's condition** holds: the program is *strictly feasible*. Recall that this just means that there is an ${\\bf x}$ such that $g_i({\\bf x})=0$ and $h_j({\\bf x})<0$ for all $i=1,\\ldots, n$ and all $j=1,\\ldots, m$. Also recall that such a point is called an **interior point**. Slater's condition is even more important in convex program because of the following theorem:\n",
    "\n",
    "#### Theorem (Sufficient Conditions for Convex Programming): For any convex program satisfying Slater's condition, any point solving the KKT conditions is a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "This is an extended example to illustrate some of the complexities that arise when solving some programs. We will abbreviate \"symmetric positive semidefinite\" as SPD. Consider the program\n",
    "\n",
    "$$\n",
    "\\min \\text{trace} \\begin{pmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f\\\\\n",
    "g & h & i\n",
    "\\end{pmatrix}\\text{ subject to }\\begin{pmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f\\\\\n",
    "g & h & i\n",
    "\\end{pmatrix}\\text{ is SPD and }\\text{trace} \\left(\\begin{pmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "1 & 1 & 1\\\\\n",
    "1 & 1 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f\\\\\n",
    "g & h & i\n",
    "\\end{pmatrix}\\right)=3\n",
    "$$\n",
    "\n",
    "where the **trace** of a square matrix is the sum of the diagonal entries of the matrix. Using Sylvester's criterion for positive semidefiniteness, the SPD constraint can be represented by the conditions\n",
    "\n",
    "$$\n",
    "a\\geq 0, e\\geq 0, i\\geq 0, ae-bd\\geq 0, ai-cg\\geq 0, ei-fh\\geq 0, aei+bfg + cdh - gec-hfa-idb\\geq 0.\n",
    "$$\n",
    "\n",
    "Thus, we can recast this program as\n",
    "\n",
    "$$\n",
    "\\min a+e+i\n",
    "$$\n",
    "\n",
    "subject to the equality constraints\n",
    "\n",
    "$$\n",
    "b=d, c=g, f=h, \\text{ and } a+ b + c + d + e + f + g + h + i=3\n",
    "$$\n",
    "\n",
    "and the inequality constraints\n",
    "\n",
    "$$\n",
    "a\\geq 0, e\\geq 0, i\\geq 0, ae-bd\\geq 0, ai-cg\\geq 0, ei-fh\\geq 0, aei+bfg + cdh - gec-hfa-idb\\geq 0.\n",
    "$$\n",
    "\n",
    "In order to write down the stationarity condition, it is useful to represent know that we can represent the gradient of $f\\in C^1( M_{3, 3}; \\mathbb{R})$ as a function $\\nabla f: M_{3, 3}\\rightarrow M_{3, 3}$:\n",
    "\n",
    "$$\n",
    "\\nabla f(A) = \\begin{pmatrix}\n",
    "\\frac{\\partial f}{\\partial a}(A) & \\frac{\\partial f}{\\partial b}(A) & \\frac{\\partial f}{\\partial c}(A)\\\\\n",
    "\\frac{\\partial f}{\\partial d}(A) & \\frac{\\partial f}{\\partial e}(A) & \\frac{\\partial f}{\\partial f}(A)\\\\\n",
    "\\frac{\\partial f}{\\partial g}(A) & \\frac{\\partial f}{\\partial h}(A) & \\frac{\\partial f}{\\partial i}(A)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "So, the gradient of the trace is\n",
    "\n",
    "$$\n",
    "\\nabla f(A) = \\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\\text{ where } A = \\begin{pmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f\\\\\n",
    "g & h & i\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Setting $g_1(A) = b-d$, $g_2(A)=c-g$, $g_3(A)=f - h$, $g_4(A)=a+b+c+d+e+f+g+h+i$, $h_1(A)=-a$, $h_2(A)=-e$, $h_3(A)=-i$, $h_4(A)=-(ae-bd)$, $h_5(A)=-(ai-cg)$, $h_6(A)=-(ei-fh)$, and\n",
    "\n",
    "$$\n",
    "h_5(A)=-(aei+bfg + cdh - gec-hfa-idb),\n",
    "$$\n",
    "\n",
    "we can compute the stationary condition as\n",
    "\n",
    "$$\\tiny\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix} + \\lambda_1 \\begin{pmatrix}\n",
    "0 & 1 & 0\\\\\n",
    "-1 & 0 & 0\\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix} + \\lambda_2 \\begin{pmatrix}\n",
    "0 & 0 & 1\\\\\n",
    "0 & 0 & 0\\\\\n",
    "-1 & 0 & 0\n",
    "\\end{pmatrix} + \\lambda_3 \\begin{pmatrix}\n",
    "0 & 0 & 0\\\\\n",
    "0 & 0 & 1\\\\\n",
    "0 & -1 & 0\n",
    "\\end{pmatrix} + \\lambda_4 \\begin{pmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "1 & 1 & 1\\\\\n",
    "1 & 1 & 1\n",
    "\\end{pmatrix} + \\mu_1 \\begin{pmatrix}\n",
    "-1 & 0 & 0\\\\\n",
    "0 & 0 & 0\\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix} + \\mu_2 \\begin{pmatrix}\n",
    "0 & 0 & 0\\\\\n",
    "0 & -1 & 0\\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix} + \\mu_3 \\begin{pmatrix}\n",
    "0 & 0 & 0\\\\\n",
    "0 & 0 & 0\\\\\n",
    "0 & 0 & -1\n",
    "\\end{pmatrix} - \\mu_4 \\begin{pmatrix}\n",
    "e & -d & 0\\\\\n",
    "-b & a & 0\\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix} - \\mu_5 \\begin{pmatrix}\n",
    "i & 0 & -g\\\\\n",
    "0 & 0 & 0\\\\\n",
    "-c & 0 & a\n",
    "\\end{pmatrix} - \\mu_6 \\begin{pmatrix}\n",
    "e & -d & 0\\\\\n",
    "-b & a & 0\\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix} - \\mu_7 \\begin{pmatrix}\n",
    "ei-fh& -(di-fg) & dh-ge\\\\\n",
    "-(bi-ch) & ai-cg & -(ah-bg)\\\\\n",
    "bf-ce & -(af-cd) & ae-bd\n",
    "\\end{pmatrix}=\\begin{pmatrix}\n",
    "0 & 0 & 0\\\\\n",
    "0 & 0 & 0\\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "While we can still write down the KKT conditions, verifying the LICQ and then solving the KKT conditions seems daunting. In certain cases, we **reparameterize** to solve the problem more easily. In particular, if $A$ is SPD, then we know it has the diagonalization $A = U\\Lambda U^T$ where $U$ is an orthogonal matrix, and $\\Lambda$ is a diagonal matrix with non-negative entries. Noting that the trace of a matrix is actually the sum of its eigenvalues, the program may be recast as\n",
    "\n",
    "$$\n",
    "\\min_{\\nu, U} \\nu_1 + \\nu_2 + \\nu_3 \\text{ subject to } \\nu_1\\geq 0, \\nu_2\\geq 0, \\nu_3\\geq 0\n",
    "$$\n",
    "\n",
    "and the equality constraint becomes\n",
    "\n",
    "$$\n",
    "\\nu_1({\\bf 1}^T {\\bf u}^{(1)})^2+\\nu_2({\\bf 1}^T {\\bf u}^{(2)})^2+\\nu_3({\\bf 1}^T {\\bf u}^{(3)})^2=3\n",
    "$$\n",
    "\n",
    "with the additional constraint that $U=\\begin{pmatrix} {\\bf u}^{(1)} & {\\bf u}^{(2)} & {\\bf u}^{(3)}\\end{pmatrix}$ satisfies $UU^T=I$. The reason for this is that\n",
    "\n",
    "$$\n",
    "\\text{trace}({\\bf 1}{\\bf 1}^T U\\Lambda U^T) = (U^T{\\bf 1})^T\\Lambda (U^T{\\bf 1}).\n",
    "$$\n",
    "\n",
    "Setting $c_i = ({\\bf 1}^T {\\bf u}^{i)})^2$, we can try to solve the $\\nu_i$'s in terms of the $c_i$'s and then determine $U$ later. That is, we try to first solve\n",
    "\n",
    "$$\n",
    "\\min_{\\nu} \\nu_1 + \\nu_2 + \\nu_3 \\text{ subject to } \\nu_1c_1 + \\nu_2 c_2 + \\nu_3 c_3 = 3, \\nu_1\\geq 0, \\nu_2\\geq 0, \\nu_3\\geq 0.\n",
    "$$\n",
    "\n",
    "The KKT conditions for this program are much easier to write down:\n",
    "\n",
    "1. $\\displaystyle \\begin{pmatrix}1\\\\\n",
    "1\\\\\n",
    "1\n",
    "\\end{pmatrix} + \\lambda_1\\begin{pmatrix}c_1\\\\\n",
    "c_2\\\\\n",
    "c_3\n",
    "\\end{pmatrix} + \\mu_1 \\begin{pmatrix}-1\\\\ 0 \\\\ 0\\end{pmatrix} + \\mu_2 \\begin{pmatrix}0\\\\ -1 \\\\ 0\\end{pmatrix} + + \\mu_3 \\begin{pmatrix}0\\\\ 0 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}0\\\\ 0 \\\\ 0\\end{pmatrix}$\n",
    "2. ${\\bf c}^T\\nu=3$, $\\nu\\geq {\\bf 0}$\n",
    "3. $\\mu\\geq 0$\n",
    "4. $\\mu \\cdot \\nu = 0$\n",
    "\n",
    "Without loss of generality, we assume that $c_1\\geq c_2\\geq c_3$. In this case, it can be shown that $(\\nu_1,\\nu_2,\\nu_3)=(3/c_1, 0, 0)$ is the only point satisfying the KKT conditions provided that $c_1>0$. Substitution back into the program give us\n",
    "\n",
    "$$\n",
    "\\min_{U} 3/c_1\\text{ subject to } c_1 = ({\\bf 1}^T{\\bf u}^{(1)})^2\\text{ and }UU^T=I.\n",
    "$$\n",
    "\n",
    "The Cauchy-Schwarz inequality tells us then that ${\\bf u}^{(1)}$ should be parallel to ${\\bf 1}$ to maximize $c_1$. This leaves us with ${\\bf u}^{(1)} = \\pm \\frac{1}{\\sqrt{3}}{\\bf 1}$, from which it follows that\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1/3 & 1/3 & 1/3\\\\\n",
    "1/3 & 1/3 & 1/3\\\\\n",
    "1/3 & 1/3 & 1/3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "solves the original program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Problems\n",
    "\n",
    "Solve the following programs by finding the solutions to the KKT conditions:\n",
    "\n",
    "1. $\\min x+y+z\\text{ subject to } x^2+y^2+z^2=1\\text{ and } 3x+2y+z = 1$\n",
    "2. $\\min x+y+z\\text{ subject to } 3x^2 + 2y^2 + z^2=1\\text{ and }x\\geq 0, y\\geq0,z\\geq 0$\n",
    "3. $\\min 3x+2y+z\\text{ subject to } xyz=1\\text{ and } x\\geq0, y\\geq 0, z\\geq 0$\n",
    "4. $\\min 3x+2y+z\\text{ subject to } x+y+z=1\\text{ and }x\\geq -1, y\\geq -1, z\\geq -1$\n",
    "5. $\\min x^2 + y^2 + z^2\\text{ subject to } (x-3)^2\\leq 1, (y-4)^2\\leq 1, (z-5)^2\\leq 1$\n",
    "6. $\\min x^2 + y^2 + z^2\\text{ subject to } x+y+z=1\\text{ and } 3x^2+2y^2+z^2\\geq 1$\n",
    "7. $\\min 3x^2 + 2y^2 + z^2\\text{ subject to } x^2+y^2+z^2=1\\text{ and } x^2-2y^2+z^2\\leq 0$\n",
    "8. $\\min 3x^2 + 2y^2 + z^2\\text{ subject to } xyz=1\\text{ and }x^2+y^2+z^2\\leq 4$\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
