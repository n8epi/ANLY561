{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics 561. Optimization\n",
    "## HW 10, Sample solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Consider the example feedforward neural network and block backtracking code in Lecture 4 Part II. This code creates a loss function and computes the gradient of this loss function for training a three layer neural network having 30 nodes in the input layer, 20 logistic units in a single hidden layer, and softmax activations for a two dimensional vector at the output layer. Modify this code to create a loss function and its gradient for a **four** layer feedforward neural network, where there are now two hidden layers each with 20 logistic units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original neural network has three layers:\n",
    "\n",
    "1. input layer of 30 nodes\n",
    "2. hidden layer of 20 nodes with logistic activation functions\n",
    "3. output layer of a single node giving a vector in $\\mathbb{R}^{2}$\n",
    "\n",
    "Let $\\mathbf{x}_{i}$ be the $i\\text{th}$ observation for $i\\in\\left\\{1,\\ldots,N\\right\\}$, where $N$ is the number of observations. To ease notation, we will drop the subscript, so that $\\mathbf{x}^{\\left(1\\right)}$ is the input to the (input layer of the) network. Then, the input to the hidden layer is the affine transformation \n",
    "\n",
    "$$\n",
    "\\phi_{1}\\left(\\mathbf{x}^{\\left(1\\right)};\\mathbf{W}^{\\left(1\\right)},\\mathbf{b}^{\\left(1\\right)}\\right)=\\mathbf{W}^{\\left(1\\right)}\\mathbf{x}^{\\left(1\\right)}+\\mathbf{b}^{\\left(1\\right)},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}^{\\left(1\\right)}$ and $\\mathbf{b}^{\\left(1\\right)}$ are the weights and biases associated with the input layer, respectively. Each node of hidden layer uses the logistic activation function, so we can express the output of the hidden layer as\n",
    "\n",
    "$$\n",
    "\\psi_{1}\\left(\\mathbf{z}^{\\left(1\\right)}\\right)=\\text{logit}\\left(\\mathbf{z}^{\\left(1\\right)}\\right)=\\text{logit}\\left(\\phi_{1}\\left(\\mathbf{x}^{\\left(1\\right)};\\mathbf{W}^{\\left(1\\right)},\\mathbf{b}^{\\left(1\\right)}\\right)\\right),\n",
    "$$\n",
    "\n",
    "where the logistic function is understood to be vectorized, i.e., applied to each component of $\\mathbf{z}^{\\left(1\\right)}$. The input to the output (final) layer is the affine transformation\n",
    "\n",
    "$$\n",
    "\\phi_{2}\\left(\\mathbf{x}^{\\left(2\\right)};\\mathbf{W}^{\\left(2\\right)},\\mathbf{b}^{\\left(2\\right)}\\right)=\\mathbf{W}^{\\left(2\\right)}\\mathbf{x}^{\\left(2\\right)}+\\mathbf{b}^{\\left(2\\right)}=\\mathbf{W}^{\\left(2\\right)}\\psi_{1}\\left(\\mathbf{z}^{\\left(1\\right)}\\right)+\\mathbf{b}^{\\left(2\\right)},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}^{\\left(2\\right)}$ and $\\mathbf{b}^{\\left(2\\right)}$ are the weights and biases associated with the hidden layer, respectively. The output layer applies the softmax function, so we can express the output of the network as\n",
    "\n",
    "$$\n",
    "\\psi_{2}\\left(\\mathbf{z}^{\\left(2\\right)}\\right)=\\text{softmax}\\left(\\mathbf{z}^{\\left(2\\right)}\\right)=\\text{softmax}\\left(\\phi_{2}\\left(\\mathbf{x}^{\\left(2\\right)};\\mathbf{W}^{\\left(2\\right)},\\mathbf{b}^{\\left(2\\right)}\\right)\\right).\n",
    "$$\n",
    "\n",
    "Applying cross-entropy, the loss function for the network can be written as\n",
    "\n",
    "$$\n",
    "\\ell\\left(\\mathbf{y},\\mathbf{q}\\right)=-\\left(y_{0}\\log\\left(q_{0}\\right)+y_{1}\\log\\left(q_{1}\\right)\\right),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{q}=\\psi_{2}\\left(\\mathbf{z}^{\\left(2\\right)}\\right)$. Expanding the arguments, we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ell\\left(\\mathbf{y},\\mathbf{q}\\right) & =\\ell\\left(\\mathbf{y},\\psi_{2}\\left(\\mathbf{z}^{\\left(2\\right)}\\right)\\right) \\\\\n",
    " & =\\ell\\left(\\mathbf{y},\\psi_{2}\\left(\\phi_{2}\\left(\\mathbf{x}^{\\left(2\\right)};\\mathbf{W}^{\\left(2\\right)},\\mathbf{b}^{\\left(2\\right)}\\right)\\right)\\right) \\\\\n",
    " & =\\ell\\left(\\mathbf{y},\\psi_{2}\\left(\\phi_{2}\\left(\\psi_{1}\\left(\\mathbf{z}^{\\left(1\\right)}\\right);\\mathbf{W}^{\\left(2\\right)},\\mathbf{b}^{\\left(2\\right)}\\right)\\right)\\right) \\\\\n",
    " & =\\ell\\left(\\mathbf{y},\\psi_{2}\\left(\\phi_{2}\\left(\\psi_{1}\\left(\\phi_{1}\\left(\\mathbf{x}^{\\left(1\\right)};\\mathbf{W}^{\\left(1\\right)},\\mathbf{b}^{\\left(1\\right)}\\right)\\right);\\mathbf{W}^{\\left(2\\right)},\\mathbf{b}^{\\left(2\\right)}\\right)\\right)\\right) \\\\\n",
    " & = r\\left(\\mathbf{W}^{\\left(1\\right)},\\mathbf{b}^{\\left(1\\right)},\\mathbf{W}^{\\left(2\\right)},\\mathbf{b}^{\\left(2\\right)}\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We have seen that the Jacobians of $r$ with respect to the weight and bias variables are\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "D_{\\mathbf{W}^{\\left(1\\right)}}r & = D_{\\mathbf{q}}\\ell\\star D_{\\mathbf{z}^{\\left(2\\right)}}\\psi_{2}\\star D_{\\mathbf{x}^{\\left(2\\right)}}\\phi_{2}\\star D_{\\mathbf{z}^{\\left(1\\right)}}\\psi_{1}\\star D_{\\mathbf{W}^{\\left(1\\right)}}\\phi_{1} \\\\\n",
    "D_{\\mathbf{b}^{\\left(1\\right)}}r & = D_{\\mathbf{q}}\\ell\\star D_{\\mathbf{z}^{\\left(2\\right)}}\\psi_{2}\\star D_{\\mathbf{x}^{\\left(2\\right)}}\\phi_{2}\\star D_{\\mathbf{z}^{\\left(1\\right)}}\\psi_{1}\\star D_{\\mathbf{b}^{\\left(1\\right)}}\\phi_{1} \\\\\n",
    "D_{\\mathbf{W}^{\\left(2\\right)}}r & = D_{\\mathbf{q}}\\ell\\star D_{\\mathbf{z}^{\\left(2\\right)}}\\psi_{2}\\star D_{\\mathbf{W}^{\\left(2\\right)}}\\phi_{2} \\\\\n",
    "D_{\\mathbf{b}^{\\left(2\\right)}}r & = D_{\\mathbf{q}}\\ell\\star D_{\\mathbf{z}^{\\left(2\\right)}}\\psi_{2}\\star D_{\\mathbf{b}^{\\left(2\\right)}}\\phi_{2},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where we have suppressed the arguments, and where $\\star$ denotes tensor contraction over the appropriate indices, i.e., those associated with the variables of differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wish to add a second hidden layer of 20 logistic units to the network. Let $\\phi_{1}$, $\\psi_{1}$, and $\\phi_{2}$ be as above, and observe that $\\phi_{2}$ is now the input to the second hidden layer. Resetting notation, the output of this hidden layer is \n",
    "\n",
    "$$\n",
    "\\psi_{2}\\left(\\mathbf{z}^{\\left(2\\right)}\\right)=\\text{logit}\\left(\\mathbf{z}^{\\left(2\\right)}\\right)=\\text{logit}\\left(\\phi_{2}\\left(\\mathbf{x}^{\\left(2\\right)};\\mathbf{W}^{\\left(2\\right)},\\mathbf{b}^{\\left(2\\right)}\\right)\\right),\n",
    "$$\n",
    "\n",
    "where the logistic function is again understood to be vectorized. The input to the output layer is the affine transformation\n",
    "\n",
    "$$\n",
    "\\phi_{3}\\left(\\mathbf{x}^{\\left(3\\right)};\\mathbf{W}^{\\left(3\\right)},\\mathbf{b}^{\\left(3\\right)}\\right)=\\mathbf{W}^{\\left(3\\right)}\\mathbf{x}^{\\left(3\\right)}+\\mathbf{b}^{\\left(3\\right)}=\\mathbf{W}^{\\left(3\\right)}\\psi_{2}\\left(\\mathbf{z}^{\\left(2\\right)}\\right)+\\mathbf{b}^{\\left(3\\right)},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}^{\\left(3\\right)}$ and $\\mathbf{b}^{\\left(3\\right)}$ are the weights and biases associated with the second hidden layer, respectively. The output layer applies the softmax function, so we can express the output of the network as\n",
    "\n",
    "$$\n",
    "\\psi_{3}\\left(\\mathbf{z}^{\\left(3\\right)}\\right)=\\text{softmax}\\left(\\mathbf{z}^{\\left(3\\right)}\\right)=\\text{softmax}\\left(\\phi_{3}\\left(\\mathbf{x}^{\\left(3\\right)};\\mathbf{W}^{\\left(3\\right)},\\mathbf{b}^{\\left(3\\right)}\\right)\\right).\n",
    "$$\n",
    "\n",
    "We again apply cross-entropy, so that the loss function for the network is\n",
    "\n",
    "$$\n",
    "\\ell\\left(\\mathbf{y},\\mathbf{q}\\right)=-\\left(y_{0}\\log\\left(q_{0}\\right)+y_{1}\\log\\left(q_{1}\\right)\\right),\n",
    "$$\n",
    "\n",
    "but where we now have $\\mathbf{q}=\\psi_{3}\\left(\\mathbf{z}^{\\left(3\\right)}\\right)$. Expanding the arguments, we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ell\\left(\\mathbf{y},\\mathbf{q}\\right) & =\\ell\\left(\\mathbf{y},\\psi_{3}\\left(\\mathbf{z}^{\\left(3\\right)}\\right)\\right) \\\\\n",
    " & =\\ell\\left(\\mathbf{y},\\psi_{3}\\left(\\phi_{3}\\left(\\mathbf{x}^{\\left(3\\right)};\\mathbf{W}^{\\left(3\\right)},\\mathbf{b}^{\\left(3\\right)}\\right)\\right)\\right) \\\\\n",
    " & =\\ell\\left(\\mathbf{y},\\psi_{3}\\left(\\phi_{3}\\left(\\psi_{2}\\left(\\mathbf{z}^{\\left(2\\right)}\\right);\\mathbf{W}^{\\left(3\\right)},\\mathbf{b}^{\\left(3\\right)}\\right)\\right)\\right) \\\\\n",
    " & =\\ell\\left(\\mathbf{y},\\psi_{3}\\left(\\phi_{3}\\left(\\psi_{2}\\left(\\phi_{2}\\left(\\mathbf{x}^{\\left(2\\right)};\\mathbf{W}^{\\left(2\\right)},\\mathbf{b}^{\\left(2\\right)}\\right)\\right);\\mathbf{W}^{\\left(3\\right)},\\mathbf{b}^{\\left(3\\right)}\\right)\\right)\\right) \\\\\n",
    " & =\\ell\\left(\\mathbf{y},\\psi_{3}\\left(\\phi_{3}\\left(\\psi_{2}\\left(\\phi_{2}\\left(\\psi_{1}\\left(\\mathbf{z}^{\\left(1\\right)}\\right);\\mathbf{W}^{\\left(2\\right)},\\mathbf{b}^{\\left(2\\right)}\\right)\\right);\\mathbf{W}^{\\left(3\\right)},\\mathbf{b}^{\\left(3\\right)}\\right)\\right)\\right) \\\\ \n",
    " & =\\ell\\left(\\mathbf{y},\\psi_{3}\\left(\\phi_{3}\\left(\\psi_{2}\\left(\\phi_{2}\\left(\\psi_{1}\\left(\\phi_{1}\\left(\\mathbf{x}^{\\left(1\\right)};\\mathbf{W}^{\\left(1\\right)},\\mathbf{b}^{\\left(1\\right)}\\right)\\right);\\mathbf{W}^{\\left(2\\right)},\\mathbf{b}^{\\left(2\\right)}\\right)\\right);\\mathbf{W}^{\\left(3\\right)},\\mathbf{b}^{\\left(3\\right)}\\right)\\right)\\right) \\\\ \n",
    " & = r\\left(\\mathbf{W}^{\\left(1\\right)},\\mathbf{b}^{\\left(1\\right)},\\mathbf{W}^{\\left(2\\right)},\\mathbf{b}^{\\left(2\\right)},\\mathbf{W}^{\\left(3\\right)},\\mathbf{b}^{\\left(3\\right)}\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The Jacobians of $r$ are thus\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "D_{\\mathbf{W}^{\\left(1\\right)}}r & = D_{\\mathbf{q}}\\ell\\star D_{\\mathbf{z}^{\\left(3\\right)}}\\psi_{3}\\star D_{\\mathbf{x}^{\\left(3\\right)}}\\phi_{3}\\star D_{\\mathbf{z}^{\\left(2\\right)}}\\psi_{2}\\star D_{\\mathbf{x}^{\\left(2\\right)}}\\phi_{2}\\star D_{\\mathbf{z}^{\\left(1\\right)}}\\psi_{1}\\star D_{\\mathbf{W}^{\\left(1\\right)}}\\phi_{1} \\\\\n",
    "D_{\\mathbf{b}^{\\left(1\\right)}}r & = D_{\\mathbf{q}}\\ell\\star D_{\\mathbf{z}^{\\left(3\\right)}}\\psi_{3}\\star D_{\\mathbf{x}^{\\left(3\\right)}}\\phi_{3}\\star D_{\\mathbf{z}^{\\left(2\\right)}}\\psi_{2}\\star D_{\\mathbf{x}^{\\left(2\\right)}}\\phi_{2}\\star D_{\\mathbf{z}^{\\left(1\\right)}}\\psi_{1}\\star D_{\\mathbf{b}^{\\left(1\\right)}}\\phi_{1} \\\\\n",
    "D_{\\mathbf{W}^{\\left(2\\right)}}r & = D_{\\mathbf{q}}\\ell\\star D_{\\mathbf{z}^{\\left(3\\right)}}\\psi_{3}\\star D_{\\mathbf{x}^{\\left(3\\right)}}\\phi_{3}\\star D_{\\mathbf{z}^{\\left(2\\right)}}\\psi_{2}\\star D_{\\mathbf{W}^{\\left(2\\right)}}\\phi_{2} \\\\\n",
    "D_{\\mathbf{b}^{\\left(2\\right)}}r & = D_{\\mathbf{q}}\\ell\\star D_{\\mathbf{z}^{\\left(3\\right)}}\\psi_{3}\\star D_{\\mathbf{x}^{\\left(3\\right)}}\\phi_{3}\\star D_{\\mathbf{z}^{\\left(2\\right)}}\\psi_{2}\\star D_{\\mathbf{b}^{\\left(2\\right)}}\\phi_{2} \\\\\n",
    "D_{\\mathbf{W}^{\\left(3\\right)}}r & = D_{\\mathbf{q}}\\ell\\star D_{\\mathbf{z}^{\\left(3\\right)}}\\psi_{3}\\star D_{\\mathbf{W}^{\\left(3\\right)}}\\phi_{3} \\\\\n",
    "D_{\\mathbf{b}^{\\left(3\\right)}}r & = D_{\\mathbf{q}}\\ell\\star D_{\\mathbf{z}^{\\left(3\\right)}}\\psi_{3}\\star D_{\\mathbf{b}^{\\left(3\\right)}}\\phi_{3}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the second hidden layer, beginning by defining some convenience functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "\n",
    "def chain_rule(Dg, Df, var_shape):\n",
    "    '''\n",
    "    Compute the Jacobian D (g \\circ f)\n",
    "    '''\n",
    "    dim = len(var_shape)\n",
    "    Dg_axes = list(range(Dg.ndim - dim, Dg.ndim))\n",
    "    Df_axes = list(range(dim))\n",
    "    return np.tensordot(Dg, Df, axes=(Dg_axes, Df_axes))\n",
    "\n",
    "# Compute the Jacobian blocks of X @ W + b\n",
    "\n",
    "def DX_affine(X, W, b):\n",
    "    # (d_{x_{i, j}} (X @ W))_{a, b} = e_a^T e_ie_j^T W e_b, \n",
    "    # so a,i slices equal W.T\n",
    "    D = np.zeros((X.shape[0], W.shape[1], X.shape[0], X.shape[1]))\n",
    "    for k in range(X.shape[0]):\n",
    "        D[k,:,k,:] = W.T\n",
    "    return D, X.shape\n",
    "\n",
    "\n",
    "def DW_affine(X, W, b):\n",
    "    # (d_{w_{i, j}} (X @ W))_{a, b} = e_a^T X e_ie_j^T e_b, \n",
    "    # so b,j slices equal x\n",
    "    D = np.zeros((X.shape[0], W.shape[1], W.shape[0], W.shape[1]))\n",
    "    for k in range(W.shape[1]):\n",
    "        D[:,k,:,k] = X\n",
    "    return D, W.shape\n",
    "\n",
    "\n",
    "def Db_affine(X, W, b):\n",
    "    # (d_{b_i} (1 @ b))_{a, b} = e_a^T 1 e_i^T e_b, \n",
    "    # so b,i slices are all ones\n",
    "    D = np.zeros((X.shape[0], W.shape[1], b.shape[1]))\n",
    "    for k in range(b.shape[1]):\n",
    "        D[:,k,k] = 1\n",
    "    return D, b.shape\n",
    "\n",
    "    \n",
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def Dlogit(Z):\n",
    "    # The Jacobian of the matrix logit\n",
    "    D = np.zeros((Z.shape[0], Z.shape[1], Z.shape[0], Z.shape[1]))\n",
    "    A = logit(Z) * logit(-Z)\n",
    "    for i in range(Z.shape[0]):\n",
    "        for j in range(Z.shape[1]):\n",
    "            D[i,j,i,j] = A[i,j]\n",
    "    return D, Z.shape\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    v = np.exp(z)\n",
    "    return v / np.sum(v)\n",
    "\n",
    "\n",
    "def matrix_softmax(Z):\n",
    "    return np.apply_along_axis(softmax, 1, Z)\n",
    "\n",
    "\n",
    "def Dmatrix_softmax(Z):\n",
    "    D = np.zeros((Z.shape[0], Z.shape[1], Z.shape[0], Z.shape[1]))\n",
    "    for k in range(Z.shape[0]):\n",
    "        v = np.exp(Z[k,:])\n",
    "        v = v / np.sum(v)\n",
    "        D[k,:,k,:] = np.diag(v) - np.outer(v, v)\n",
    "    return D, Z.shape\n",
    "\n",
    "\n",
    "def cross_entropy(P, Q):\n",
    "    return -np.sum(P * np.log(Q)) / P.shape[0]\n",
    "\n",
    "\n",
    "def DQcross_entropy(P, Q):\n",
    "    return - P * (1 / Q) / P.shape[0], Q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a function to calculate the network output $\\psi_{3}\\left(\\mathbf{z}^{\\left(3\\right)}\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi3(X, var):\n",
    "    '''\n",
    "    Calculate the network output\n",
    "    :param X: array of observations\n",
    "    :param var: list of weights and biases, witih\n",
    "        var[0] = W_1, var[1] = b_1 \n",
    "        var[2] = W_2, var[3] = b_2\n",
    "        var[4] = W_3, var[5] = b_3\n",
    "    '''\n",
    "    return matrix_softmax(\n",
    "        # psi_2\n",
    "        logit(\n",
    "            # psi_1\n",
    "            logit(\n",
    "                # phi_1\n",
    "                X @ var[0] + var[1]\n",
    "            )\n",
    "            # phi_2\n",
    "            @ var[2] + var[3]\n",
    "        )\n",
    "        # phi_3\n",
    "        @ var[4] + var[5]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the loss function $\\ell$ and its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_loss_closure(X, Y):\n",
    "    f = lambda var: cross_entropy(Y, psi3(X, var))\n",
    "    return f\n",
    "\n",
    "\n",
    "def nn_loss_gradient_closure(X, Y):\n",
    "    def df(var):\n",
    "        # activation of first hidden layer\n",
    "        Z1 = (X @ var[0]) + var[1]\n",
    "        X2 = logit(Z1)\n",
    "        \n",
    "        # activation of second hidden layer\n",
    "        Z2 = (X2 @ var[2]) + var[3]\n",
    "        X3 = logit(Z2)\n",
    "        \n",
    "        # activation of output layer\n",
    "        Z3 = (X3 @ var[4]) + var[5]\n",
    "        Q = matrix_softmax(Z3)\n",
    "        \n",
    "        # Backpropagation tells us we can immediately contract DQ DZ3\n",
    "        D_Q, Qshape = DQcross_entropy(Y, Q)\n",
    "        D_Z3, Z3shape = Dmatrix_softmax(Z3)\n",
    "        back_prop3 = chain_rule(D_Q, D_Z3, Qshape)\n",
    "        \n",
    "        # Jacobians for phi_3\n",
    "        D_X3, X3shape = DX_affine(X3, var[4], var[5])\n",
    "        D_W3, W3shape = DW_affine(X3, var[4], var[5])\n",
    "        D_b3, b3shape = Db_affine(X3, var[4], var[5])\n",
    "        \n",
    "        # Jacobian for psi_2\n",
    "        D_Z2, Z2shape = Dlogit(Z2)\n",
    "        back_prop2 = chain_rule(\n",
    "            chain_rule(back_prop3, D_X3, X3shape),\n",
    "            D_Z2, \n",
    "            Z2shape\n",
    "        )\n",
    "        \n",
    "        # Jacobians for phi_2\n",
    "        D_X2, X2shape = DX_affine(X2, var[2], var[3])\n",
    "        D_W2, W2shape = DW_affine(X2, var[2], var[3])\n",
    "        D_b2, b2shape = Db_affine(X2, var[2], var[3])\n",
    "        \n",
    "        # Jacobian for psi_1\n",
    "        D_Z1, Z1shape = Dlogit(Z1)\n",
    "        back_prop1 = chain_rule(\n",
    "            chain_rule(back_prop2, D_X2, X2shape), \n",
    "            D_Z1, \n",
    "            Z1shape\n",
    "        )\n",
    "        \n",
    "        # Jacobians for phi_1\n",
    "        D_W1, W1shape = DW_affine(X, var[0], var[1])\n",
    "        D_b1, b1shape = Db_affine(X, var[0], var[1])\n",
    "        \n",
    "        # Compute all the gradients\n",
    "        W1grad = chain_rule(back_prop1, D_W1, W1shape)\n",
    "        b1grad = chain_rule(back_prop1, D_b1, b1shape)\n",
    "        W2grad = chain_rule(back_prop2, D_W2, W2shape)\n",
    "        b2grad = chain_rule(back_prop2, D_b2, b2shape)\n",
    "        W3grad = chain_rule(back_prop3, D_W3, W3shape)\n",
    "        b3grad = chain_rule(back_prop3, D_b3, b3shape)\n",
    "        \n",
    "        return [W1grad, b1grad, W2grad, b2grad, W3grad, b3grad]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define block backtracking and some remaining convenience functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_blocks(x, y, t):\n",
    "    # An auxiliary function for backtracking with blocks of variables\n",
    "    num_blocks = len(x)\n",
    "    z = [None] * num_blocks\n",
    "    for i in range(num_blocks):\n",
    "        z[i] = x[i] + t * y[i]\n",
    "    return z\n",
    "      \n",
    "    \n",
    "def block_backtracking(x0, f, dx, df0, alpha=0.1, beta=0.5):\n",
    "    num_blocks = len(x0)\n",
    "    \n",
    "    delta = 0\n",
    "    for i in range(num_blocks):\n",
    "        delta = delta + np.sum(dx[i] * df0[i])\n",
    "    delta = alpha * delta\n",
    "    \n",
    "    f0 = f(x0)\n",
    "    \n",
    "    t = 1\n",
    "    x = update_blocks(x0, dx, t)\n",
    "    fx = f(x)\n",
    "    while (not np.isfinite(fx)) or f0 + t * delta < fx:\n",
    "        t = beta * t\n",
    "        x = update_blocks(x0, dx, t)\n",
    "        fx = f(x)\n",
    "        \n",
    "    return x, fx\n",
    "\n",
    "\n",
    "def negate_blocks(x):\n",
    "    # Helper function for negating the gradient of block variables\n",
    "    num_blocks = len(x)\n",
    "    z = [None] * num_blocks\n",
    "    for i in range(num_blocks):\n",
    "        z[i] = -x[i]\n",
    "    return z\n",
    "\n",
    "\n",
    "def block_norm(x):\n",
    "    num_blocks=len(x)\n",
    "    z = 0\n",
    "    for i in range(num_blocks):\n",
    "        z = z + np.sum(x[i] ** 2)\n",
    "    return np.sqrt(z)\n",
    "\n",
    "\n",
    "def random_matrix(shape, sigma=0.1):\n",
    "    # Helper for random initialization\n",
    "    return np.reshape(sigma * rd.randn(shape[0] * shape[1]), shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a)\n",
    "Using the first 400 examples from the Wisconsin Breast Cancer dataset, run 100 steps of gradient descent with block backtracking to train your four layer neural network. Use the `random_matrix` function to randomly initialize your weight variables, and use the random seed `1234` to keep the behavior of your program deterministic. Keep all other variables (e.g., $\\alpha$ and $\\beta$) fixed, and report the final test accuracy after running gradient descent 100 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading the data and creating one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the Wisconsin Breast Cancer dataset \n",
    "# (569 examples in 30 dimensions)\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Parameters for the data\n",
    "dim_data = 30\n",
    "num_labels = 2\n",
    "num_examples = 569\n",
    "\n",
    "# Parameters for training\n",
    "num_train = 400\n",
    "\n",
    "X = data['data'] # Data in rows\n",
    "targets = data.target # 0-1 labels\n",
    "labels = np.zeros((num_examples, num_labels))\n",
    "for i in range(num_examples):\n",
    "    labels[i, targets[i]] = 1 # Conversion to one-hot representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize the weight and bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.seed(1234)\n",
    "\n",
    "# Prepare hyperparameters of the network\n",
    "hidden_nodes = 20\n",
    "\n",
    "# Initialize variables\n",
    "W1_init = random_matrix((dim_data, hidden_nodes))\n",
    "b1_init = np.zeros((1, hidden_nodes))\n",
    "\n",
    "W2_init = random_matrix((hidden_nodes, hidden_nodes))\n",
    "b2_init = np.zeros((1, hidden_nodes))\n",
    "\n",
    "W3_init = random_matrix((hidden_nodes, num_labels))\n",
    "b3_init = np.zeros((1, num_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we perform 100 steps of gradient descent with backtracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sean/Envs/optimization/lib/python3.6/site-packages/ipykernel_launcher.py:45: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0,  Mean loss: 0.684698,  Gradient norm: 0.573653,  Accuracy: 56.8\n",
      "Step: 10,  Mean loss: 0.683966,  Gradient norm: 0.001043,  Accuracy: 56.8\n",
      "Step: 20,  Mean loss: 0.683963,  Gradient norm: 0.000943,  Accuracy: 56.8\n",
      "Step: 30,  Mean loss: 0.683961,  Gradient norm: 0.001235,  Accuracy: 56.8\n",
      "Step: 40,  Mean loss: 0.683958,  Gradient norm: 0.000846,  Accuracy: 56.8\n",
      "Step: 50,  Mean loss: 0.683956,  Gradient norm: 0.001086,  Accuracy: 56.8\n",
      "Step: 60,  Mean loss: 0.683953,  Gradient norm: 0.001297,  Accuracy: 56.8\n",
      "Step: 70,  Mean loss: 0.683951,  Gradient norm: 0.001118,  Accuracy: 56.8\n",
      "Step: 80,  Mean loss: 0.683948,  Gradient norm: 0.001184,  Accuracy: 56.8\n",
      "Step: 90,  Mean loss: 0.683946,  Gradient norm: 0.001647,  Accuracy: 56.8\n",
      "Final test accuracy: 76.9 percent\n"
     ]
    }
   ],
   "source": [
    "x = [W1_init, b1_init, W2_init, b2_init, W3_init, b3_init]\n",
    "f = nn_loss_closure(X[:num_train], labels[:num_train])\n",
    "df = nn_loss_gradient_closure(X[:num_train], labels[:num_train])\n",
    "dx = lambda v: negate_blocks(df(v))\n",
    "    \n",
    "for i in range(100):\n",
    "    ngrad = dx(x)\n",
    "    x, fval = block_backtracking(x, f, ngrad, df(x), alpha=0.1)\n",
    "    \n",
    "    train_data = psi3(X[:num_train], x)\n",
    "    train_labels = np.argmax(train_data, axis=1)\n",
    "    per_correct = 100 * (1 - np.count_nonzero(\n",
    "        train_labels - targets[:num_train]) / num_train)\n",
    "\n",
    "    if ((i % 10) == 0):\n",
    "        print(\n",
    "            \"Step: {:2d}, \".format(i),\n",
    "            \"Mean loss: {:.6f}, \".format(fval),\n",
    "            \"Gradient norm: {:.6f}, \".format(block_norm(ngrad)),\n",
    "            \"Accuracy: {:.1f}\".format(per_correct)\n",
    "        )\n",
    "    \n",
    "test_data = psi3(X[num_train:], x)\n",
    "\n",
    "test_labels = np.argmax(test_data, axis=1)\n",
    "per_correct = 100 * (1 - np.count_nonzero(\n",
    "    test_labels - targets[num_train:]) / (num_examples - num_train))\n",
    "\n",
    "print('Final test accuracy: %.1f percent' % per_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that adding a second hidden layer to the network actually decreased accuracy on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b)\n",
    "List three ways that you could make this implementation more efficient (that is, make it use less memory or less time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give a partial list of techniques to make the implementation more efficient.\n",
    "\n",
    "1. **Apply sparse matrix methods.** Some of the Jacobians are sparse. We might make the implementation more efficient by sparse matrix methods, which neither store zeros (memory) nor compute products where one of the multiplicands is known to be zero (computation).\n",
    "\n",
    "2. **Vectorize when possible.** The current implementation uses multiple `for` loops, which are by nature serial. Vectorizing these operations could allow use of efficient **`numpy`** methods and implicit parallelization.\n",
    "\n",
    "3. **Use stochastic gradient descent.** The implementation relies on gradient descent using the full training set. Stochastic gradient descent might provide achieve comparable accuracy with reduced memory usage and training time.\n",
    "\n",
    "4. **Reduce the size of the training set.** We might choose to reduce the size of the training set either by using fewer observations, or by including fewer features. If we include fewer features, we might first perform dimensionality reduction, e.g., by applying principal components analysis.\n",
    "\n",
    "6. **Simplify network architecture.** We might choose to reduce the number of nodes in the hidden layers, or we might replace the logistic activation functions by ReLU activation functions, which can be computed more efficiently.\n",
    "\n",
    "7. **Implement in TensorFlow.** We might also opt to implement the network in TensorFlow, taking advantage of efficient implementations of many operations, as well as the implicit parallelization offered by partitioning the computational graph.\n",
    "\n",
    "The prudent programmer heed the advice of Donald Knuth:\n",
    "\n",
    "> Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We *should* forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.\n",
    ">\n",
    "> Knuth, \"Structured Programming with `go to` Statements\". *ACM Computing Surveys* **6**:4 (Dec. 1974), pp. 261-301."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Convolutional neural networks employ convolution of stacks of images that output a single image. For example, if we convolve the stack of images\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 1\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "with the *filter*\n",
    "\n",
    "$$\n",
    "\\mathsf{H}=\\left[\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "we get\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\ast\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\ast\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "3 & 3 \\\\\n",
    "3 & 3 \n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "5 & 3 \\\\\n",
    "3 & 5\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a)\n",
    "Express convolution of a 3 by 3 matrix $\\mathbf{X}$ with the matrix\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "as contraction of $\\mathbf{X}$ with a 2 by 2 by 3 by 3 tensor. In particular, explicitly write down this 4th order tensor and indicate the indices along which contraction should occur. *Hint:* This should be simple to write down if you choose the right slices for the 2 by 2 by 3 by 3 tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{n}=\\left(2,2,3,3\\right)$, and let $\\mathsf{A}\\in\\mathscr{F}_{\\mathbf{n}}$, so that $\\mathsf{A}$ is a fourth order tensor. Let $\\mathbf{F}$ denote the contraction of $\\mathbf{X}$ with $\\mathsf{A}$, and observe that \n",
    "\n",
    "$$\n",
    "\\mathbf{F} =\n",
    "\\mathbf{X}\\ast\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & x_{1,3} \\\\\n",
    "x_{2,1} & x_{2,2} & x_{2,3} \\\\\n",
    "x_{3,1} & x_{3,2} & x_{3,3}\n",
    "\\end{bmatrix}\n",
    "\\ast\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_{1,1}+x_{2,2} & x_{1,2}+x_{2,3} \\\\\n",
    "x_{2,1}+x_{3,2} & x_{2,2}+x_{3,3}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "which is a $2\\times 2$ matrix. It follows that $\\mathbf{F}$ is a second order tensor with shape $2\\times 2$. Let $\\mathbf{m}=\\left(3,3\\right)$, so that $\\mathbf{X}\\in\\mathscr{F}_{\\mathbf{m}}$, and let $\\mathbf{i}$ and $\\mathbf{j}$ be indices along which the contraction occurs. Thus, we have the tensor contraction operator\n",
    "\n",
    "$$\n",
    "c_{\\mathbf{i},\\mathbf{j}}:\\mathscr{F}_{\\mathbf{n}}\\times\\mathscr{F}_{\\mathbf{m}}\\rightarrow\\mathscr{F}_{\\mathbf{n}_{\\setminus\\mathbf{i}}\\oplus\\mathbf{m}_{\\setminus\\mathbf{j}}}=\\mathscr{F}_{\\left(2,2\\right)}.\n",
    "$$\n",
    "\n",
    "The order and shape of $\\mathbf{F}$ imply that contraction should occur along the $\\mathbf{i}=\\left(3,4\\right)$ and $\\mathbf{j}=\\left(1,2\\right)$ indices, i.e.,\n",
    "\n",
    "$$\n",
    "\\mathbf{n}_{\\setminus\\mathbf{i}}=\\left(2,2\\right)\\quad\\text{and}\\quad\\mathbf{m}_{\\setminus\\mathbf{j}}=\\left(\\right)\\implies\\mathbf{n}_{\\setminus\\mathbf{i}}\\oplus\\mathbf{m}_{\\setminus\\mathbf{j}}=\\left(2,2\\right).\n",
    "$$\n",
    "\n",
    "We can thus express the entries of $\\mathbf{F}$ as\n",
    "\n",
    "$$\n",
    "f_{i_{1},i_{2}}=\\sum_{k_{1}=1}^{3}\\sum_{k_{2}=1}^{3}a_{i_{1},i_{2},k_{1},k_{2}}x_{k_{1},k_{2}}.\n",
    "$$\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "f_{1,1} = \\sum_{k_{1}=1}^{3}\\sum_{k_{2}=1}^{3}a_{1,1,k_{1},k_{2}}x_{k_{1},k_{2}}= x_{1,1}+x_{2,2},\n",
    "$$\n",
    "\n",
    "which implies that $a_{1,1,k_{1},k_{2}}=1$ for $\\left(k_{1},k_{2}\\right)\\in\\left\\{\\left(1,1\\right),\\left(2,2\\right)\\right\\}$ and $a_{1,1,k_{1},k_{2}}=0$ otherwise. Similarly, \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_{1,2} & = x_{1,2}+x_{2,3}\\implies a_{1,2,k_{1},k_{2}}=\n",
    "\\begin{cases}\n",
    "1, & \\left(k_{1},k_{2}\\right)\\in\\left\\{\\left(1,2\\right),\\left(2,3\\right)\\right\\} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "f_{2,1} & = x_{2,1}+x_{3,2}\\implies a_{2,1,k_{1},k_{2}}=\n",
    "\\begin{cases}\n",
    "1, & \\left(k_{1},k_{2}\\right)\\in\\left\\{\\left(2,1\\right),\\left(3,2\\right)\\right\\} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "f_{2,2} & = x_{2,2}+x_{3,3}\\implies a_{2,2,k_{1},k_{2}}=\n",
    "\\begin{cases}\n",
    "1, & \\left(k_{1},k_{2}\\right)\\in\\left\\{\\left(2,2\\right),\\left(3,3\\right)\\right\\} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, the fourth order tensor is\n",
    "\n",
    "$$\n",
    "\\mathsf{A}=\n",
    "\\left[\n",
    "\\left[\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\right],\n",
    "\\left[\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\right]\n",
    "\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b)\n",
    "Express the convolution of a 2 by 3 by 3 tensor $\\mathsf{X}$ with the above 2 by 2 by 2 $\\mathsf{H}$ as contraction with a 2 by 2 by 2 by 3 by 3 tensor. In particular, explicitly write down this 5th order tensor and indicate the indices along which contraction occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{n}=\\left(2,2,2,3,3\\right)$, and let $\\mathsf{A}\\in\\mathscr{F}_{\\mathbf{n}}$, so that $\\mathsf{A}$ is a fifth order tensor. Let $\\mathbf{F}$ denote the contraction of $\\mathsf{X}$ with $\\mathsf{A}$, and observe that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{F} & = \\mathsf{X}\\ast\\mathsf{H} \\\\\n",
    "& = \\left[\n",
    "\\begin{bmatrix}\n",
    "x_{1,1,1} & x_{1,1,2} & x_{1,1,3} \\\\\n",
    "x_{1,2,1} & x_{1,2,2} & x_{1,2,3} \\\\\n",
    "x_{1,3,1} & x_{1,3,2} & x_{1,3,3}\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "x_{2,1,1} & x_{2,1,2} & x_{2,1,3} \\\\\n",
    "x_{2,2,1} & x_{2,2,2} & x_{2,2,3} \\\\\n",
    "x_{2,3,1} & x_{2,3,2} & x_{2,3,3}\n",
    "\\end{bmatrix}\n",
    "\\right]\n",
    "\\ast\n",
    "\\left[\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "\\right] \\\\\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "x_{1,1,1} & x_{1,1,2} & x_{1,1,3} \\\\\n",
    "x_{1,2,1} & x_{1,2,2} & x_{1,2,3} \\\\\n",
    "x_{1,3,1} & x_{1,3,2} & x_{1,3,3}\n",
    "\\end{bmatrix}\n",
    "\\ast\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "x_{2,1,1} & x_{2,1,2} & x_{2,1,3} \\\\\n",
    "x_{2,2,1} & x_{2,2,2} & x_{2,2,3} \\\\\n",
    "x_{2,3,1} & x_{2,3,2} & x_{2,3,3}\n",
    "\\end{bmatrix}\n",
    "\\ast\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Applying our result from part (a), the first summand is\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{1,1,1} + x_{1,2,2} & x_{1,1,2} + x_{1,2,3} \\\\\n",
    "x_{1,2,1} + x_{1,3,2} & x_{1,2,2} + x_{1,3,3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and the second is\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{2,1,1} + x_{2,1,2} + x_{2,2,1} + x_{2,2,2} & \n",
    "x_{2,1,2} + x_{2,1,3} + x_{2,2,2} + x_{2,2,3} \\\\\n",
    "x_{2,2,1} + x_{2,2,2} + x_{2,3,1} + x_{2,3,2} &\n",
    "x_{2,2,2} + x_{2,2,3} + x_{2,3,2} + x_{2,3,3}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "so that the result of the convolution is\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{1,1,1} + x_{1,2,2} + x_{2,1,1} + x_{2,1,2} + x_{2,2,1} + x_{2,2,2} &\n",
    "x_{1,1,2} + x_{1,2,3} + x_{2,1,2} + x_{2,1,3} + x_{2,2,2} + x_{2,2,3} \\\\\n",
    "x_{1,2,1} + x_{1,3,2} + x_{2,2,1} + x_{2,2,2} + x_{2,3,1} + x_{2,3,2} & \n",
    "x_{1,2,2} + x_{1,3,3} + x_{2,2,2} + x_{2,2,3} + x_{2,3,2} + x_{2,3,3}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We see that $\\mathbf{F}$ is a $2\\times 2$ matrix, i.e., a second order tensor with shape $2\\times 2$. Let $\\mathbf{m}=\\left(2,3,3\\right)$, so that $\\mathsf{X}\\in\\mathscr{F}_{\\mathbf{m}}$, and let $\\mathbf{i}$ and $\\mathbf{j}$ be indices along which the contraction occurs. Thus, we have the tensor contraction operator\n",
    "\n",
    "$$\n",
    "c_{\\mathbf{i},\\mathbf{j}}:\\mathscr{F}_{\\mathbf{n}}\\times\\mathscr{F}_{\\mathbf{m}}\\rightarrow\\mathscr{F}_{\\mathbf{n}_{\\setminus\\mathbf{i}}\\oplus\\mathbf{m}_{\\setminus\\mathbf{j}}}=\\mathscr{F}_{\\left(2,2\\right)}.\n",
    "$$\n",
    "\n",
    "The order and shape of $\\mathbf{F}$ imply that contraction should occur along the $\\mathbf{i}=\\left(3,4,5\\right)$ and $\\mathbf{j}=\\left(1,2,3\\right)$ indices, i.e.,\n",
    "\n",
    "$$\n",
    "\\mathbf{n}_{\\setminus\\mathbf{i}}=\\left(2,2\\right)\\quad\\text{and}\\quad\\mathbf{m}_{\\setminus\\mathbf{j}}=\\left(\\right)\\implies\\mathbf{n}_{\\setminus\\mathbf{i}}\\oplus\\mathbf{m}_{\\setminus\\mathbf{j}}=\\left(2,2\\right).\n",
    "$$\n",
    "\n",
    "We can thus express the entries of $\\mathbf{F}$ as\n",
    "\n",
    "$$\n",
    "f_{i_{1},i_{2}}=\\sum_{k_{1}=1}^{2}\\sum_{k_{2}=1}^{3}\\sum_{k_{3}=1}^{3}a_{i_{1},i_{2},k_{1},k_{2},k_{3}}x_{k_{1},k_{2},k_{3}}.\n",
    "$$\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "f_{1,1} = \\sum_{k_{1}=1}^{2}\\sum_{k_{2}=1}^{3}\\sum_{k_{3}=1}^{3}a_{1,1,k_{1},k_{2},k_{3}}x_{k_{1},k_{2},k_{3}}= x_{1,1,1} + x_{1,2,2} + x_{2,1,1} + x_{2,1,2} + x_{2,2,1} + x_{2,2,2},\n",
    "$$\n",
    "\n",
    "which implies that\n",
    "\n",
    "$$\n",
    "a_{1,1,k_{1},k_{2},k_{3}}=\n",
    "\\begin{cases}\n",
    "1, & \\left(k_{1},k_{2},k_{3}\\right)\\in\\left\\{\n",
    "\\left(1,1,1\\right),\n",
    "\\left(1,2,2\\right),\n",
    "\\left(2,1,1\\right),\n",
    "\\left(2,1,2\\right),\n",
    "\\left(2,2,1\\right),\n",
    "\\left(2,2,2\\right)\n",
    "\\right\\} \\\\\n",
    "0 , & \\text{otherwise}\n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "By similar reasoning, we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_{1,2,k_{1},k_{2},k_{3}} & =\n",
    "\\begin{cases}\n",
    "1, & \\left(k_{1},k_{2},k_{3}\\right)\\in\\left\\{\n",
    "\\left(1,1,2\\right),\n",
    "\\left(1,2,3\\right),\n",
    "\\left(2,1,2\\right),\n",
    "\\left(2,1,3\\right),\n",
    "\\left(2,2,2\\right),\n",
    "\\left(2,2,3\\right)\n",
    "\\right\\} \\\\\n",
    "0 , & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "a_{2,1,k_{1},k_{2},k_{3}} & =\n",
    "\\begin{cases}\n",
    "1, & \\left(k_{1},k_{2},k_{3}\\right)\\in\\left\\{\n",
    "\\left(1,2,1\\right),\n",
    "\\left(1,3,2\\right),\n",
    "\\left(2,2,1\\right),\n",
    "\\left(2,2,2\\right),\n",
    "\\left(2,3,1\\right),\n",
    "\\left(2,3,2\\right)\n",
    "\\right\\} \\\\\n",
    "0 , & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "a_{2,2,k_{1},k_{2},k_{3}} & =\n",
    "\\begin{cases}\n",
    "1, & \\left(k_{1},k_{2},k_{3}\\right)\\in\\left\\{\n",
    "\\left(1,2,2\\right),\n",
    "\\left(1,3,3\\right),\n",
    "\\left(2,2,2\\right),\n",
    "\\left(2,2,3\\right),\n",
    "\\left(2,3,2\\right),\n",
    "\\left(2,3,3\\right)\n",
    "\\right\\} \\\\\n",
    "0 , & \\text{otherwise}\n",
    "\\end{cases}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, the fifth order tensor is\n",
    "\n",
    "$$\n",
    "\\mathsf{A} =\n",
    "\\left[\n",
    "  \\left[\n",
    "    \\left[\n",
    "      \\mathbf{A}_{1,1,1}, \\mathbf{A}_{1,1,2}\n",
    "    \\right],\n",
    "    \\left[\n",
    "      \\mathbf{A}_{1,2,1}, \\mathbf{A}_{1,2,2}\n",
    "    \\right]\n",
    "  \\right],\n",
    "  \\left[\n",
    "    \\left[\n",
    "      \\mathbf{A}_{2,1,1}, \\mathbf{A}_{2,1,2}\n",
    "    \\right],\n",
    "    \\left[\n",
    "      \\mathbf{A}_{2,2,1}, \\mathbf{A}_{2,2,2}\n",
    "    \\right]\n",
    "  \\right]  \n",
    "\\right],\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{A}_{1,1,1} & =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\mathbf{A}_{1,1,2}=\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "\\mathbf{A}_{1,2,1} & =\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\mathbf{A}_{1,2,2} =\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "\\mathbf{A}_{2,1,1} & =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\mathbf{A}_{2,1,2} =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "1 & 1 & 0\n",
    "\\end{bmatrix} \\\\\n",
    "\\mathbf{A}_{2,2,1} & =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\mathbf{A}_{2,2,2} =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{bmatrix}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
