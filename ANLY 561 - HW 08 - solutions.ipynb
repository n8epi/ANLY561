{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics 561. Optimization\n",
    "## HW 08, Sample solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Use the code from Lecture 03 Part IX (Support Vector Machines and Duality) to generate a dataset $\\left\\{\\mathbf{x}^{\\left(i\\right)}\\right\\}_{i=0}^{19}\\subset\\mathbb{R}^{2}$ with labels $\\left\\{y^{\\left(i\\right)}\\right\\}_{i=0}^{19}\\in\\left\\{-1,1\\right\\}$ such that $\\mathbf{x}^{\\left(i\\right)}$ is drawn uniformly from the unit disc centered at $\\left(-2,-2\\right)$ and $y^{\\left(i\\right)}=-1$ for $i=0,\\ldots,9$, and $\\mathbf{x}^{\\left(i\\right)}$ is drawn uniformly from the unit disc centered at $\\left(2,2\\right)$ and $y^{\\left(i\\right)}=1$ for $i=10,\\ldots,19$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the data below, where $\\mathbf{X}\\in M_{10,2}$ is the matrix whose $i$th row is $\\mathbf{x}^{\\left(i\\right)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def random_circle(N):\n",
    "    x = rd.randn(N, 2)\n",
    "    # normalize each row of x\n",
    "    return x / np.linalg.norm(x=x, ord=2, axis=1).reshape(N, 1)\n",
    "\n",
    "\n",
    "def random_radius(N, R=1):\n",
    "    r = rd.rand(N)\n",
    "    # ensure uniform sampling from the disc\n",
    "    return R * np.sqrt(r) \n",
    "    \n",
    "\n",
    "def random_disc(N, mu=[0,0], R=1):\n",
    "    x = random_circle(N)\n",
    "    r = random_radius(N, R=R)\n",
    "    return r.reshape(N, 1) * x + mu\n",
    "\n",
    "\n",
    "N = 10\n",
    "\n",
    "# set seed for reproducibility\n",
    "rd.seed(0)\n",
    "X = np.concatenate(\n",
    "    (\n",
    "        random_disc(N, mu=[-2, -2]),\n",
    "        random_disc(N, mu=[2, 2])        \n",
    "    )\n",
    ")\n",
    "\n",
    "y = np.repeat([-1, 1], repeats=[10, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a)\n",
    "Code up a Phase I method to find an interior point for the (3D) program\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{v}\\in\\mathbb{R}^{2},b\\in\\mathbb{R}}\\frac{1}{2}\\left\\Vert\\mathbf{v}\\right\\Vert^{2}\\quad\\text{subject to}\\quad h_{i}\\left(\\mathbf{v},b\\right)\\leq 0\n",
    "$$\n",
    "\n",
    "where $h_{i}\\left(\\mathbf{v},b\\right)=1-y^{\\left(i\\right)}\\left(\\mathbf{v}^{\\mathsf{T}}\\mathbf{x}^{\\left(i\\right)}-b\\right)$. That is, use the log-barrier method to solve the (4D) program\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{v}\\in\\mathbb{R}^{2},b\\in\\mathbb{R},z\\in\\mathbb{R}}\\tilde{f}\\left(\\mathbf{v},b,z\\right)\\quad\\text{subject to}\\quad \\tilde{h}_{i}\\left(\\mathbf{v},b,z\\right)\\leq 0\n",
    "$$\n",
    "\n",
    "where $\\tilde{f}\\left(\\mathbf{v},b,z\\right)=z$ and $\\tilde{h}_{i}\\left(\\mathbf{v},b,z\\right)=1-y^{\\left(i\\right)}\\left(\\mathbf{v}^\\mathsf{T}\\mathbf{x}^{\\left(i\\right)}-b\\right)-z$, but stop as soon as you find an iterate $\\left(\\mathbf{v}^{\\left(k\\right)},b^{\\left(k\\right)},z^{\\left(k\\right)}\\right)$ such that $z^{\\left(k\\right)}<0$ (which then implies that $\\left(\\mathbf{v}^{\\left(k\\right)},b^{\\left(k\\right)}\\right)$ is an interior point). Initialize the Phase I method with the data $\\mathbf{v}^{\\left(0\\right)}=\\left(-20,20\\right)$, $b^{\\left(0\\right)}=10$, and determine $z^{\\left(0\\right)}$ (in your code) such that $\\left(\\mathbf{v}^{\\left(0\\right)},b^{\\left(0\\right)},z^{\\left(0\\right)}\\right)$ is an interior point of the 4D convex program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $\\mathbf{v}^{\\left(0\\right)}$, we can apply the method from Lecture 03 Part VIII to choose $z^{\\left(0\\right)}$, i.e., \n",
    "\n",
    "$$\n",
    "z^{\\left(0\\right)}=\\max\\left(h_{1}\\left(\\mathbf{v}^{\\left(0\\right)},b^{\\left(0\\right)}\\right),\\ldots,h_{20}\\left(\\mathbf{v}^{\\left(0\\right)},b^{\\left(0\\right)}\\right)\\right)+1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.015334982418949"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v0 = np.array([-20, 20])\n",
    "b0 = 10\n",
    "\n",
    "\n",
    "def h(v, b, X, y):\n",
    "    '''\n",
    "    Constraint function\n",
    "    :param v: vector of length 2\n",
    "    :param b: scalar\n",
    "    :param X: matrix whose ith row is \\mathbf{x}^{i}\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    '''\n",
    "    # compute \\mathbf{v}^{\\mathsf{T}}\\mathbf{x}^{i} for each i\n",
    "    prod = np.apply_along_axis(func1d=np.dot, axis=1, arr=X, b=v)\n",
    "    return 1 - y * (prod - b)\n",
    "\n",
    "z0 = max(h(v=v0, b=b0, X=X, y=y)) + 1\n",
    "z0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find an interior point by forming the log-barrier function\n",
    "\n",
    "$$\n",
    "\\tilde{f}_{\\text{lb}}\\left(\\mathbf{v},b,z\\right)=\\tilde{f}\\left(\\mathbf{v},b,z\\right)-\\sum_{i=0}^{19}\\frac{1}{t}\\log\\left(-\\tilde{h}_{i}\\left(\\mathbf{v},b,z\\right)\\right)=z-\\frac{1}{t}\\sum_{i=0}^{19}\\log\\left(z-h_{i}\\left(\\mathbf{v},b\\right)\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use gradient descent for the inner backtracking steps. Letting $\\mathbf{p}=\\left(\\mathbf{v},b,z\\right)$, we have shown in the accompanying PDF that\n",
    "\n",
    "$$\n",
    "\\nabla\\tilde{f}_{\\text{lb}}\\left(\\mathbf{p}\\right)=\\begin{bmatrix}0\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "1\n",
    "\\end{bmatrix}-\\frac{1}{t}\\sum_{i=0}^{19}\\frac{1}{p_{4}-h_{i}\\left(p_{1},p_{2},p_{3}\\right)}\\begin{bmatrix}x_{1}^{\\left(i\\right)}y^{\\left(i\\right)}\\\\\n",
    "x_{2}^{\\left(i\\right)}y^{\\left(i\\right)}\\\\\n",
    "-y^{\\left(i\\right)}\\\\\n",
    "1\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the objective and constraint functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_tilde = lambda p: p[3]\n",
    "grad_f_tilde = lambda p: np.array([0, 0, 0, 1])\n",
    "\n",
    "\n",
    "def lb_cons_tilde(p, h, X, y):\n",
    "    '''\n",
    "    Log-barrier constraints\n",
    "    :param p: point at which to evaluate constraints\n",
    "    :param h: constraint function\n",
    "    :param X: matrix whose ith row is \\mathbf{x}^{i}\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    '''\n",
    "    return np.sum(np.log(p[3] - h(v=p[:2], b=p[2], X=X, y=y)))\n",
    "\n",
    "\n",
    "def grad_lb_cons_tilde(p, h, X, y):\n",
    "    '''\n",
    "    Gradient of log-barrier constraints\n",
    "    :param p: point at which to evaluate gradient\n",
    "    :param h: constraint function\n",
    "    :param X: matrix whose ith row is \\mathbf{x}^{i}\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    '''\n",
    "    # build a matrix whose ith row is \n",
    "    # [x_{1}^{i} * y^{i}, x_{2}^{i} * y^{i}, -y^{i}, 1]\n",
    "    M = np.concatenate(\n",
    "        (\n",
    "            X * y[:, np.newaxis], \n",
    "            -y[:, np.newaxis],\n",
    "            np.ones(y.shape[0])[:, np.newaxis]\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    c = 1 / (p[3] - h(v=p[:2], b=p[2], X=X, y=y))\n",
    "\n",
    "    # column sums of the Hadamard product of M and c\n",
    "    return np.sum(M * c[:, np.newaxis], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now provide our generic backtracking function from HW 05, as well as a convenience function to perform a fixed number of backtracking iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backtrack(x0, dx, f, df0, alpha, beta, verbose=False):\n",
    "    '''\n",
    "    Backtracking for general functions\n",
    "    :param x0: initial point\n",
    "    :param dx: incremental factor for updating x0\n",
    "    :param f: objective function\n",
    "    :param df0: gradient of f at x0\n",
    "    :param alpha: sloping factor of stopping criterion\n",
    "    :param beta: aggressiveness parameter for backtracking steps\n",
    "    :param verbose: if True, return the objective function \n",
    "        value at the final x\n",
    "    '''\n",
    "    \n",
    "    # np.dot corresponds to a tensor product\n",
    "    delta = alpha * np.dot(dx, df0)\n",
    "    \n",
    "    t = 1\n",
    "    f0 = f(x0)\n",
    "    x = x0 + dx  # t = 1\n",
    "    fx = f(x)\n",
    "    \n",
    "    while (not np.isfinite(fx)) or fx > f0 + delta * t:\n",
    "        # this is cheaper than exponentiating beta each time\n",
    "        t = beta * t\n",
    "        x = x0 + t * dx\n",
    "        fx = f(x)\n",
    "        \n",
    "    if verbose:\n",
    "        return x, f(x)\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "\n",
    "def backtrack_n(n, x0, f, grad_f, alpha, beta, hess_f=None):\n",
    "    '''\n",
    "    Perform n steps of backtracking using the specified increment type\n",
    "    :param n: number of backtracking steps\n",
    "    :param x0: initial point\n",
    "    :param f: objective function\n",
    "    :param grad_f: gradient of objective function\n",
    "    :param alpha: sloping factor of stopping criterion\n",
    "    :param beta: aggressiveness parameter for backtracking steps\n",
    "    :param hess_f: optional Hessian of objective function, \n",
    "        invokes Newton's method\n",
    "    '''\n",
    "    x = x0\n",
    "    xs = x\n",
    "    fs = [f(x)]\n",
    "    \n",
    "    for i in range(n):\n",
    "        if hess_f is None:\n",
    "            dx = -grad_f(x)\n",
    "        else:\n",
    "            dx = -np.linalg.solve(hess_f(x), grad_f(x))\n",
    "        \n",
    "        x, fx = backtrack(\n",
    "            x0=x,\n",
    "            dx=dx,\n",
    "            f=f,\n",
    "            df0=grad_f(x),\n",
    "            alpha=alpha,\n",
    "            beta=beta,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        xs = np.append(xs, x)\n",
    "        fs.append(fx)\n",
    "        \n",
    "    # return a single numpy array with columns x_{i} and f(x)\n",
    "    return np.column_stack((xs.reshape((n + 1, x0.shape[0])), fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we modify our log-barrier implementation from HW 05 to accommodate the stopping criterion $z^{\\left(k\\right)}<0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lb(f, lb_cons, grad_f, grad_lb_cons, x0, alpha, beta, M, \n",
    "       iter_init, iter_outer, iter_inner, \n",
    "       x_threshold=None, hess_f=None, hess_lb_cons=None,\n",
    "       **kwargs):\n",
    "    '''\n",
    "    Log-barrier method with Newton's method\n",
    "    :param f: objective function\n",
    "    :param lb_cons: log-barrier constraints\n",
    "    :param grad_f: gradient of objective function\n",
    "    :param grad_lb_cons: gradient of log-barrier constraints\n",
    "    :param x0: initial point\n",
    "    :param alpha: sloping factor of stopping criterion\n",
    "    :param beta: aggressiveness parameter for backtracking steps\n",
    "    :param M: increase factor for t\n",
    "    :param iter_init: number of centering iterations\n",
    "    :param iter_outer: number of outer iterations\n",
    "    :param iter_inner: number of inner iterations\n",
    "    :param x_threshold: optional vector the same length as x0; \n",
    "        the algorithm terminates if any component of the iterate\n",
    "        is less than the corresponding component of x_threshold\n",
    "    :param hess_f: optional Hessian of objective function, \n",
    "        required for Newton's method\n",
    "    :param hess_lb_cons: optional Hessian of log-barrier constraints,\n",
    "        required for Newton's method\n",
    "    :param kwargs: additional arguments passed to lb_cons,\n",
    "        grad_lb_cons, and hess_lb_cons\n",
    "    '''\n",
    "    x = x0\n",
    "    \n",
    "    # form the gradient and Hessian of the log-barrier function\n",
    "    def grad_flb(z, t=1):\n",
    "        return grad_f(z) - (1 / t) * grad_lb_cons(z, **kwargs)\n",
    "    \n",
    "    # only form the Hessian of the log-barrier function \n",
    "    # if the appropriate arguments were provided\n",
    "    if hess_f is None or hess_lb_cons is None:\n",
    "        hess_flb = None\n",
    "    else:\n",
    "        def hess_flb(z, t=1):\n",
    "            return hess_f(z) - (1 / t) * hess_lb_cons(z, **kwargs)\n",
    "    \n",
    "    \n",
    "    # centering steps\n",
    "    x = backtrack_n(\n",
    "        n=iter_init,\n",
    "        x0=x,\n",
    "        # form the log-barrier function on the \n",
    "        # fly to accommodate varying t\n",
    "        f=lambda z: f(z) - lb_cons(z, **kwargs),\n",
    "        grad_f=grad_flb,\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        hess_f=hess_flb\n",
    "    )[-1,:x0.shape[0]]\n",
    "    print('Centering complete: ', x)\n",
    "    \n",
    "    # begin log-barrier iterations\n",
    "    t = 1\n",
    "\n",
    "    if x_threshold is not None:\n",
    "        print('Threshold provided, algorithm may terminate early')\n",
    "    \n",
    "    i = 1\n",
    "    while i <= iter_outer:\n",
    "        if x_threshold is not None:\n",
    "            if any(x < x_threshold): break\n",
    "        \n",
    "        print('Outer iteration', i)\n",
    "        t = t * M        \n",
    "        \n",
    "        # we need something slightly different from backtrack_n\n",
    "        for j in range(iter_inner):            \n",
    "            if hess_flb is None:\n",
    "                dx = -grad_flb(x)\n",
    "            else:\n",
    "                dx = -np.linalg.solve(hess_flb(x, t), grad_flb(x, t))\n",
    "            \n",
    "            x = backtrack(\n",
    "                x0=x,\n",
    "                dx=dx,\n",
    "                # form the log-barrier function \n",
    "                f=lambda z: f(z) - (1 / t) * lb_cons(z, **kwargs),\n",
    "                df0=grad_flb(x, t),\n",
    "                alpha=alpha,\n",
    "                beta=beta\n",
    "            )\n",
    "            print('  Inner iteration', j, ': ', x)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we carry out the log-barrier method to find a numerical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centering complete:  [ -7.79589092  32.36949547   7.71317363  12.72537674]\n",
      "Threshold provided, algorithm may terminate early\n",
      "Outer iteration 1\n",
      "  Inner iteration 0 :  [ -7.18380815  33.01408622   7.69064768  12.07337406]\n",
      "  Inner iteration 1 :  [ -6.58932751  33.64055225   7.67017364  11.41165308]\n",
      "  Inner iteration 2 :  [ -6.01068492  34.25068655   7.65149076  10.7411792 ]\n",
      "  Inner iteration 3 :  [ -5.44639269  34.84600343   7.63438563  10.06276765]\n",
      "  Inner iteration 4 :  [ -4.89518202  35.42779609   7.61868137   9.37711459]\n",
      "Outer iteration 2\n",
      "  Inner iteration 0 :  [ -4.35596016  35.9971797    7.60422965   8.68482025]\n",
      "  Inner iteration 1 :  [ -3.82777781  36.5551242    7.59090488   7.98640665]\n",
      "  Inner iteration 2 :  [ -3.30980388  37.10247966   7.57859975   7.28233123]\n",
      "  Inner iteration 3 :  [ -2.80130579  37.63999617   7.56722184   6.57299756]\n",
      "  Inner iteration 4 :  [ -2.30163378  38.16833969   7.55669099   5.85876393]\n",
      "Outer iteration 3\n",
      "  Inner iteration 0 :  [ -1.81020828  38.68810478   7.5469372    5.13995017]\n",
      "  Inner iteration 1 :  [ -1.32650968  39.19982493   7.53789898   4.41684322]\n",
      "  Inner iteration 2 :  [ -0.85007002  39.70398105   7.52952204   3.68970171]\n",
      "  Inner iteration 3 :  [ -0.38046595  40.2010085    7.52175819   2.95875974]\n",
      "  Inner iteration 4 :  [  0.08268694  40.69130293   7.51456448   2.22423005]\n",
      "Outer iteration 4\n",
      "  Inner iteration 0 :  [  0.53973902  41.17522522   7.50790242   1.48630665]\n",
      "  Inner iteration 1 :  [  0.99101072  41.65310558   7.50173744   0.74516704]\n",
      "  Inner iteration 2 :  [  1.43679596e+00   4.21252472e+01   7.49603834e+00   9.74165125e-04]\n",
      "  Inner iteration 3 :  [  1.87736517  42.59192905   7.49077687  -0.74612198]\n",
      "  Inner iteration 4 :  [  2.31296782  43.05340884   7.48592737  -1.49598297]\n"
     ]
    }
   ],
   "source": [
    "p0 = lb(\n",
    "    f=f_tilde, \n",
    "    lb_cons=lb_cons_tilde, \n",
    "    grad_f=grad_f_tilde, \n",
    "    grad_lb_cons=grad_lb_cons_tilde, \n",
    "    x0=np.append(v0, [b0, z0]),\n",
    "    alpha=0.2, \n",
    "    beta=0.8, \n",
    "    M=10, \n",
    "    iter_init=10,\n",
    "    iter_outer=10, \n",
    "    iter_inner=5,\n",
    "    x_threshold=np.array([-np.inf, -np.inf, -np.inf, 0]),\n",
    "    h=h, X=X, y=y\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the algorithm has terminated early, so that an interior point $\\left(\\mathbf{v}_{\\text{int}},b_{\\text{int}}\\right)$ for the 3D program is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.31296782,  43.05340884,   7.48592737])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b)\n",
    "Use the $\\left(\\mathbf{v}^{\\left(k\\right)},b^{\\left(k\\right)}\\right)$ from part (a) to initialize Phase II for your generated data. That is, use this interior point to initialize the log-barrier method for the 3D program. Use 3 centering steps, $M=10$, and 5 iterations in the outer loop with 2 inner loop iterations each. For each backtracking step, use Newton search directions, $\\alpha=0.1$, and $\\beta=0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve the 3D program using the log-barrier method. Let\n",
    "\n",
    "$$\n",
    "f\\left(\\mathbf{v},b\\right)=\\frac{1}{2}\\left\\Vert \\mathbf{v}\\right\\Vert ^{2}=\\frac{1}{2}\\mathbf{v}^{\\mathsf{T}}\\mathbf{v},\n",
    "$$\n",
    "\n",
    "and let $\\mathbf{q}=\\left(\\mathbf{v},b\\right)$, so that the log-barrier function is\n",
    "\n",
    "$$\n",
    "f_{\\text{lb}}\\left(\\mathbf{q}\\right)=f\\left(\\mathbf{q}\\right)-\\frac{1}{t}\\sum_{i=0}^{19}\\log\\left(-h_{i}\\left(\\mathbf{q}\\right)\\right).\n",
    "$$\n",
    "\n",
    "We have shown in the accompanying PDF that\n",
    "\n",
    "$$\n",
    "\\nabla f_{\\text{lb}}\\left(\\mathbf{q}\\right)=\\begin{bmatrix}q_{1}\\\\\n",
    "q_{2}\\\\\n",
    "0\n",
    "\\end{bmatrix}-\\frac{1}{t}\\sum_{i=0}^{19}-\\frac{y^{\\left(i\\right)}}{h_{i}\\left(\\mathbf{q}\\right)}\\tilde{\\mathbf{x}}^{\\left(i\\right)}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\nabla^{2}f_{\\text{lb}}\\left(\\mathbf{q}\\right)=\\begin{bmatrix}1 & 0 & 0\\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}-\\frac{1}{t}\\sum_{i=0}^{19}-\\frac{1}{\\left(h_{i}\\left(\\mathbf{q}\\right)\\right)^{2}}\\tilde{\\mathbf{x}}^{\\left(i\\right)}\\left(\\tilde{\\mathbf{x}}^{\\left(i\\right)}\\right)^{\\mathsf{T}},\n",
    "$$\n",
    "\n",
    "where $\\tilde{\\mathbf{x}}^{\\left(i\\right)}=\\left(x_{1}^{\\left(i\\right)},x_{2}^{\\left(i\\right)},-1\\right)$. We now define the objective and constraint functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = lambda q: np.dot(q, q) / 2\n",
    "\n",
    "grad_f = lambda q: np.append(q[:2], 0)\n",
    "\n",
    "hess_f = lambda q: np.array([[1, 0, 0], [0, 1, 0], [0, 0, 0]])\n",
    "\n",
    "\n",
    "# build a matrix whose ith row is [x_{1}^{i}, x_{2}^{i}, -1]\n",
    "X_tilde = np.concatenate(\n",
    "    (\n",
    "        X, \n",
    "        -np.ones(X.shape[0])[:, np.newaxis]\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "def lb_cons(q, h, X_tilde, y):\n",
    "    '''\n",
    "    Log-barrier constraints\n",
    "    :param q: point at which to evaluate constraints\n",
    "    :param h: constraint function\n",
    "    :param X_tilde: matrix whose ith row is (\\mathbf{x}^{i}, -1)\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    '''\n",
    "    return np.sum(np.log(-h(v=q[:2], b=q[2], X=X_tilde[:,:2], y=y)))\n",
    "\n",
    "\n",
    "def grad_lb_cons(q, h, X_tilde, y):\n",
    "    '''\n",
    "    Gradient of log-barrier constraints\n",
    "    :param q: point at which to evaluate gradient\n",
    "    :param h: constraint function\n",
    "    :param X_tilde: matrix whose ith row is (\\mathbf{x}^{i}, -1)\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    '''\n",
    "    c = -y / h(v=q[:2], b=q[2], X=X_tilde[:,:2], y=y)\n",
    "\n",
    "    # column sums of the Hadamard product of M and c\n",
    "    return np.sum(X_tilde * c[:, np.newaxis], axis=0)\n",
    "\n",
    "\n",
    "def hess_lb_cons(q, h, X_tilde, y):\n",
    "    '''\n",
    "    Hessian of log-barrier constraints\n",
    "    :param q: point at which to evaluate Hessian\n",
    "    :param h: constraint function\n",
    "    :param X_tilde: matrix whose ith row is (\\mathbf{x}^{i}, -1)\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    '''\n",
    "    # list whose ith element is \\tilde{x}^{i}(\\tilde{x}^{i})^{T}\n",
    "    xxT = [x[:, np.newaxis] * x for x in X_tilde]\n",
    "    \n",
    "    c = -1 / h(v=q[:2], b=q[2], X=X_tilde[:,:2], y=y) ** 2\n",
    "\n",
    "    # then sum the elements of the resulting list (element-wise)\n",
    "    return np.sum(\n",
    "        list(\n",
    "            # multiply each element of xxT (a matrix) by the \n",
    "            # corresponding element of c (a scalar)\n",
    "            map(lambda r, s: r * s, xxT, c)\n",
    "        ), \n",
    "        axis=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we carry out the log-barrier method to find a numerical solution, using the interior point from part (a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centering complete:  [ 2.35124983  2.57425822  2.07190756]\n",
      "Outer iteration 1\n",
      "  Inner iteration 0 :  [ 1.54742087  1.69250169  0.03385209]\n",
      "  Inner iteration 1 :  [ 1.00993524  1.09814664  0.45692042]\n",
      "Outer iteration 2\n",
      "  Inner iteration 0 :  [ 0.62950955  0.68395171 -0.01256503]\n",
      "  Inner iteration 1 :  [ 0.52137561  0.56363594  0.10962022]\n",
      "Outer iteration 3\n",
      "  Inner iteration 0 :  [ 0.41726686  0.45047657  0.05183293]\n",
      "  Inner iteration 1 :  [ 0.36561291  0.39164456  0.01799623]\n",
      "Outer iteration 4\n",
      "  Inner iteration 0 :  [ 0.36063277  0.38525316  0.00606433]\n",
      "  Inner iteration 1 :  [ 0.35895474  0.379472    0.00249235]\n",
      "Outer iteration 5\n",
      "  Inner iteration 0 :  [ 0.35892204  0.37835206  0.00325781]\n",
      "  Inner iteration 1 :  [ 0.3606204   0.37551381  0.00259428]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sean/Envs/optimization/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in log\n"
     ]
    }
   ],
   "source": [
    "q_ast = lb(\n",
    "    f=f, \n",
    "    lb_cons=lb_cons, \n",
    "    grad_f=grad_f, \n",
    "    grad_lb_cons=grad_lb_cons, \n",
    "    x0=p0[:3],\n",
    "    alpha=0.1, \n",
    "    beta=0.5, \n",
    "    M=10, \n",
    "    iter_init=3,\n",
    "    iter_outer=5, \n",
    "    iter_inner=2,\n",
    "    hess_f=hess_f,\n",
    "    hess_lb_cons=hess_lb_cons,\n",
    "    h=h, X_tilde=X_tilde, y=y\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the solution to the 3D program is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3606204 ,  0.37551381,  0.00259428])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (c)\n",
    "Use the $\\left(\\mathbf{v}^{\\left(k\\right)},b^{\\left(k\\right)}\\right)$ from part (a) to initialize the primal-dual algorithm. Take 10 steps with $\\nu=10$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the algorithm described in the accompanying PDF. Step 1 requires that we compute\n",
    "\n",
    "$$\n",
    "\\hat{\\eta}\\left(\\mathbf{q},\\boldsymbol{\\lambda}\\right)=-\\left(\\mathbf{h}\\left(\\mathbf{q}\\right)\\right)^{\\mathsf{T}}\\boldsymbol{\\lambda}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eta_hat(q, h, X, y, lmbda):\n",
    "    '''\n",
    "    Surrogate duality gap\n",
    "    :param q: point at which to evaluate gap\n",
    "    :param h: constraint function\n",
    "    :param X: matrix whose ith row is \\mathbf{x}^{i}\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    :param lmbda: dual variable, vector whose length is \n",
    "        equal to the number of constraints\n",
    "    '''\n",
    "    return -np.dot(h(v=q[:2], b=q[2], X=X, y=y), lmbda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 requires that we compute the primal-dual search direction\n",
    "\n",
    "$$\n",
    "\\Delta\\mathbf{z}_{\\text{pd}}=\\begin{bmatrix}\\Delta\\mathbf{q}_{\\text{pd}}\\\\\n",
    "\\Delta\\boldsymbol{\\lambda}_{\\text{pd}}\n",
    "\\end{bmatrix}=-\\begin{bmatrix}\\nabla^{2}f\\left(\\mathbf{q}\\right) & \\left(D\\mathbf{h}\\left(\\mathbf{q}\\right)\\right)^{\\mathsf{T}}\\\\\n",
    "-\\text{diag}\\left(\\boldsymbol{\\lambda}\\right)D\\mathbf{h}\\left(\\mathbf{q}\\right) & -\\text{diag}\\left(\\mathbf{h}\\left(\\mathbf{q}\\right)\\right)\n",
    "\\end{bmatrix}^{-1}\n",
    "\\begin{bmatrix}\n",
    "r_{\\text{dual}} \\\\\n",
    "r_{\\text{cent}}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "D\\mathbf{h}\\left(\\mathbf{q}\\right)=\\begin{bmatrix}\\left(\\nabla h_{0}\\left(\\mathbf{q}\\right)\\right)^{\\mathsf{T}}\\\\\n",
    "\\left(\\nabla h_{1}\\left(\\mathbf{q}\\right)\\right)^{\\mathsf{T}}\\\\\n",
    "\\vdots\\\\\n",
    "\\left(\\nabla h_{19}\\left(\\mathbf{q}\\right)\\right)^{\\mathsf{T}}\n",
    "\\end{bmatrix}=-\\begin{bmatrix}y^{\\left(0\\right)}\\left(\\tilde{\\mathbf{x}}^{\\left(0\\right)}\\right)^{\\mathsf{T}}\\\\\n",
    "y^{\\left(1\\right)}\\left(\\tilde{\\mathbf{x}}^{\\left(1\\right)}\\right)^{\\mathsf{T}}\\\\\n",
    "\\vdots\\\\\n",
    "y^{\\left(19\\right)}\\left(\\tilde{\\mathbf{x}}^{\\left(19\\right)}\\right)^{\\mathsf{T}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "r_{\\text{dual}} \\\\\n",
    "r_{\\text{cent}}\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\\nabla f\\left(\\mathbf{q}\\right)+\\left(D\\mathbf{h}\\left(\\mathbf{q}\\right)\\right)^{\\mathsf{T}}\\boldsymbol{\\lambda}\\\\\n",
    "-\\text{diag}\\left(\\boldsymbol{\\lambda}\\right)\\mathbf{h}\\left(\\mathbf{q}\\right)-\\left(1/t\\right)\\mathbf{1}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Dh(X_tilde, y):\n",
    "    '''\n",
    "    Jacobian of constraint vector\n",
    "    :param X_tilde: matrix whose ith row is (\\mathbf{x}^{i}, -1)\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}    \n",
    "    '''\n",
    "    return -X_tilde * y[:, np.newaxis]\n",
    "\n",
    "\n",
    "def r_dual(q, grad_f, Dh, lmbda, **kwargs):\n",
    "    '''\n",
    "    Dual residual\n",
    "    :param q: current iterate\n",
    "    :param grad_f: gradient of objective function\n",
    "    :param Dh: Jacobian of constraint vector\n",
    "    :param lmbda: dual variable\n",
    "    :param kwargs: additional arguments passed to Dh\n",
    "    '''\n",
    "    return grad_f(q) + np.matmul(Dh(**kwargs).T, lmbda)\n",
    " \n",
    "    \n",
    "def r_cent(q, h, t, lmbda, X, y):\n",
    "    '''\n",
    "    Centrality residual\n",
    "    :param q: current iterate\n",
    "    :param h: constraint function\n",
    "    :param t: parameter associated with current surrogate duality gap\n",
    "    :param lmbda: dual variable\n",
    "    :param X: matrix whose ith row is \\mathbf{x}^{i}\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}\n",
    "    '''\n",
    "    return -np.matmul(\n",
    "        np.diag(lmbda), \n",
    "        h(v=q[:2], b=q[2], X=X, y=y)\n",
    "    ) - 1 / t\n",
    "\n",
    "\n",
    "def zpd(Dh, grad_f, h, hess_f, lmbda, q, \n",
    "        r_cent, r_dual, t, X_tilde, y):\n",
    "    '''\n",
    "    Compute primal-dual search direction\n",
    "    :param Dh: Jacobian of constraint vector\n",
    "    :param grad_f: gradient of objective function    \n",
    "    :param h: constraint function\n",
    "    :param hess_f: Hessian of objective function\n",
    "    :param lmbda: dual variable\n",
    "    :param q: current iterate\n",
    "    :param r_cent: centrality residual\n",
    "    :param r_dual: dual residual\n",
    "    :param t: parameter associated with current surrogate duality gap\n",
    "    :param X_tilde: matrix whose ith row is (\\mathbf{x}^{i}, -1)\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}        \n",
    "    '''\n",
    "    X = X_tilde[:,:2]\n",
    "    \n",
    "    # construct the matrix to be inverted\n",
    "    M = np.concatenate(\n",
    "        (\n",
    "            # first row\n",
    "            np.concatenate(\n",
    "                (\n",
    "                    hess_f(q=q), \n",
    "                    Dh(X_tilde=X_tilde, y=y).T\n",
    "                ),\n",
    "                axis=1\n",
    "            ),\n",
    "            # second row\n",
    "            np.concatenate(\n",
    "                (\n",
    "                    -np.matmul(\n",
    "                        np.diag(lmbda), Dh(X_tilde=X_tilde, y=y)\n",
    "                    ),\n",
    "                    -np.diag(h(v=q[:2], b=q[2], X=X, y=y))\n",
    "                ),\n",
    "                axis=1\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # construct r_t\n",
    "    rt = np.concatenate(\n",
    "        (\n",
    "            r_dual(\n",
    "                q=q, grad_f=grad_f, Dh=Dh, lmbda=lmbda, \n",
    "                X_tilde=X_tilde, y=y\n",
    "            ),\n",
    "            r_cent(q=q, h=h, t=t, lmbda=lmbda, X=X, y=y)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return -np.linalg.solve(M, rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the complete algorithm, including backtracking line search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pd(alpha, beta, Dh, eta_hat, grad_f, h, hess_f, iter, \n",
    "       lmbda0, nu, q0, r_cent, r_dual, X_tilde, y):\n",
    "    '''\n",
    "    Primal-dual interior point method\n",
    "    :param alpha: sloping factor of stopping criterion\n",
    "    :param beta: aggressiveness parameter for backtracking steps    \n",
    "    :param Dh: Jacobian of constraint vector\n",
    "    :param eta_hat: surrogate duality gap\n",
    "    :param grad_f: gradient of objective function    \n",
    "    :param h: constraint function\n",
    "    :param hess_f: Hessian of objective function\n",
    "    :param iter: number of iterations to perform\n",
    "    :param lmbda0: initial dual variable\n",
    "    :param nu: parameter controlling t\n",
    "    :param q0: initial iterate\n",
    "    :param r_cent: centrality residual\n",
    "    :param r_dual: dual residual\n",
    "    :param X_tilde: matrix whose ith row is (\\mathbf{x}^{i}, -1)\n",
    "    :param y: vector of labels for each \\mathbf{x}^{i}    \n",
    "    '''    \n",
    "    q = q0\n",
    "    lmbda = lmbda0\n",
    "    \n",
    "    # used in calculation of t\n",
    "    nu_m = nu * lmbda.shape[0]\n",
    "    \n",
    "    X = X_tilde[:,:2]\n",
    "    \n",
    "    for i in range(iter):\n",
    "        # step 1: determine t\n",
    "        eh = eta_hat(q=q, h=h, X=X, y=y, lmbda=lmbda)\n",
    "        t = nu_m / eh\n",
    "            \n",
    "        # step 2: compute primal-dual search direction\n",
    "        z = zpd(\n",
    "            Dh=Dh,\n",
    "            grad_f=grad_f,\n",
    "            h=h,\n",
    "            hess_f=hess_f,\n",
    "            lmbda=lmbda,\n",
    "            q=q,\n",
    "            r_cent=r_cent,\n",
    "            r_dual=r_dual,\n",
    "            t=t,\n",
    "            X_tilde=X_tilde,\n",
    "            y=y\n",
    "        )\n",
    "        \n",
    "        # for convenience, split z        \n",
    "        q_pd = z[:q0.shape[0]]\n",
    "        lmbda_pd = z[q0.shape[0]:]\n",
    "\n",
    "        # step 3: backtracking line search\n",
    "        lmbda_neg = lmbda_pd < 0\n",
    "        \n",
    "        # np.amin will fail if no element of lmbda_pd is negative\n",
    "        # (the resulting vector will be of length zero)\n",
    "        if any(lmbda_neg):\n",
    "            c = -lmbda / lmbda_pd\n",
    "            s_max = min(1, np.amin(c[lmbda_neg]))\n",
    "        else:\n",
    "            s_max = 1        \n",
    "\n",
    "        # convenience function to compute the next iterate\n",
    "        xplus = lambda x, x_pd, s: x + s * x_pd\n",
    "        \n",
    "        # initial value of s\n",
    "        s = 0.99 * s_max\n",
    "        \n",
    "        # find s such that h(q^{+}) \\prec 0\n",
    "        qplus = xplus(q, q_pd, s)\n",
    "        \n",
    "        while any(h(v=qplus[:2], b=qplus[2], X=X, y=y) >= 0):\n",
    "            s = s * beta\n",
    "            qplus = xplus(q, q_pd, s)\n",
    "        \n",
    "        # convenience function to construct r_t\n",
    "        def r_t(q, lmbda):\n",
    "            '''\n",
    "            Construct r_t\n",
    "            '''\n",
    "            return np.concatenate(\n",
    "                (\n",
    "                    r_dual(\n",
    "                        q=q, grad_f=grad_f, Dh=Dh, lmbda=lmbda, \n",
    "                        X_tilde=X_tilde, y=y\n",
    "                    ),\n",
    "                    r_cent(q=q, h=h, t=t, lmbda=lmbda, X=X, y=y)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        \n",
    "        # construct r_t and r_{t}^{+}\n",
    "        rt = r_t(q, lmbda)\n",
    "        \n",
    "        lmbdaplus = xplus(lmbda, lmbda_pd, s)\n",
    "        rtplus = r_t(q=qplus, lmbda=lmbdaplus)\n",
    "        \n",
    "        rtnorm = np.linalg.norm(x=rt, ord=2)\n",
    "        \n",
    "        while (\n",
    "            np.linalg.norm(x=rtplus, ord=2) > (1 - alpha * s) * rtnorm\n",
    "        ):\n",
    "            s = s * beta\n",
    "            qplus = xplus(q, q_pd, s)\n",
    "            lmbdaplus = xplus(lmbda, lmbda_pd, s)\n",
    "            rtplus = r_t(q=qplus, lmbda=lmbdaplus)\n",
    "\n",
    "        # finally, update the iterate\n",
    "        q = qplus\n",
    "        lmbda = lmbdaplus\n",
    "        \n",
    "        print('Iteration', i, ': ', q)\n",
    "        \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take 10 steps with the primal-dual algorithm, taking $\\nu=10$ and \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\lambda}_{0}=-\\frac{1}{\\mathbf{h}\\left(\\mathbf{v}_{\\text{int}},b_{\\text{int}}\\right)},\n",
    "$$\n",
    "\n",
    "where $\\left(\\mathbf{v}_{\\text{int}},b_{\\text{int}}\\right)$ is the interior point from part (a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 :  [ 0.84608778  9.34182407  0.44208141]\n",
      "Iteration 1 :  [ 0.48212873  0.59547939  0.09511823]\n",
      "Iteration 2 :  [ 0.39057708  0.42758012  0.06345998]\n",
      "Iteration 3 :  [ 0.37040115  0.39336001  0.03548437]\n",
      "Iteration 4 :  [ 0.36588034  0.37429505  0.00505889]\n",
      "Iteration 5 :  [ 0.37260292  0.36261299 -0.00169696]\n",
      "Iteration 6 :  [ 0.38918246  0.34302408 -0.00622482]\n",
      "Iteration 7 :  [ 0.39225637  0.33877779 -0.00767272]\n",
      "Iteration 8 :  [ 0.39262416  0.33829536 -0.00776826]\n",
      "Iteration 9 :  [ 0.3926536   0.33825047 -0.00777909]\n"
     ]
    }
   ],
   "source": [
    "lmbda0 = -1 / h(v=p0[:2], b=p0[2], X=X, y=y)\n",
    "\n",
    "q_pd = pd(\n",
    "    alpha=0.2,\n",
    "    beta=0.8,\n",
    "    Dh=Dh, \n",
    "    eta_hat=eta_hat, \n",
    "    grad_f=grad_f, \n",
    "    h=h, \n",
    "    hess_f=hess_f, \n",
    "    iter=10, \n",
    "    lmbda0=lmbda0, \n",
    "    nu=10, \n",
    "    q0=p0[:3], \n",
    "    r_cent=r_cent, \n",
    "    r_dual=r_dual, \n",
    "    X_tilde=X_tilde, \n",
    "    y=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final iterate is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3926536 ,  0.33825047, -0.00777909])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (d)\n",
    "Compare the answers you get from parts (b) and (c), and provide a simultaneous plot of your data, the separating line corresponding to the interior point $\\left(\\mathbf{v}^{\\left(k\\right)},b^{\\left(k\\right)}\\right)$ from part (a), and the two separating lines from parts (b) and (c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results from parts (b) and (c) are similar, and also observe that the primal-dual method required fewer iterations to converge than the log-barrier method.\n",
    "\n",
    "For $\\mathbf{v},\\mathbf{x}\\in\\mathbb{R}^{2}$, we can plot the affine hyperplane defined by $\\mathbf{v}^{\\mathsf{T}}\\mathbf{x}-b=0$ by noting that\n",
    "\n",
    "$$\n",
    "v_{1}x_{1}+v_{2}x_{2}=0\\implies v_{2}x_{2}=b-v_{1}x_{1}\\implies x_{2}=-\\frac{v_{1}}{v_{2}}x_{1}+\\frac{b}{v_{2}},\n",
    "$$\n",
    "\n",
    "which is a line with slope $-v_{1}/v_{2}$ and intercept $b/v_{2}$. We plot the separating lines from parts (a), (b), and (c) in red, blue, and green, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxQfR//8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104058128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def abline(slope, intercept, *fmt):\n",
    "    '''\n",
    "    Plot a line from slope and intercept\n",
    "    adapted from https://stackoverflow.com/a/43811762/4557288\n",
    "    :param slope: slope\n",
    "    :param intercept: intercept\n",
    "    :param *fmt: format arguments passed to pyplot.plot\n",
    "    '''\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, *fmt)\n",
    "    \n",
    "    \n",
    "plt.scatter(X[:N,0], X[:N,1])\n",
    "plt.scatter(X[N:,0], X[N:,1])\n",
    "\n",
    "# interior point\n",
    "abline(-p0[0] / p0[1], p0[2] / p0[1], 'r')\n",
    "\n",
    "# log-barrier solution\n",
    "abline(-q_ast[0] / q_ast[1], q_ast[2] / q_ast[1], 'b--')\n",
    "\n",
    "# primal-dual solution\n",
    "abline(-q_pd[0] / q_pd[1], q_pd[2] / q_pd[1], 'g-.')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
