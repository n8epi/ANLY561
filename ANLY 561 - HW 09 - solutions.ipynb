{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics 561. Optimization\n",
    "## HW 09, Sample solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Use TensorFlow to implement gradient descent using backtracking for training logistic regression on the first 400 examples from the Wisconsin Breast Cancer Dataset. Perform $10^{3}$ gradient descent steps, and report the test accuracy of your final answer. In particular, your implementation should only compute gradients using TensorFlow operations. Use $\\alpha=0.1$ and $\\beta=0.5$ inside backtracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading the data $\\mathbf{X}$, to which we add a column of 1s for the bias term, with the resulting matrix $\\tilde{\\mathbf{X}}=\\left[\\mathbf{1},\\mathbf{X}\\right]$. We also convert the targets $y_{i}\\in\\left\\{0,1\\right\\}$ to $\\tilde{y}_{i}\\in\\left\\{-1,1\\right\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# data are in rows\n",
    "X_WBC = data['data']\n",
    "m, n = X_WBC.shape\n",
    "\n",
    "X_tilde = np.c_[np.ones((m, 1)), X_WBC]\n",
    "\n",
    "# the original labels are 0/1, convert them to -1/+1, \n",
    "# and also reshape to a column vector\n",
    "targets = data.target\n",
    "labels = 2 * targets.reshape(-1, 1) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = 400\n",
    "X_train = X_tilde[:train_size]\n",
    "X_test = X_tilde[train_size:]\n",
    "y_train = labels[:train_size]\n",
    "y_test = labels[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we can estimate the probability of the positive ($+1$) class using the logistic function, i.e., \n",
    "\n",
    "$$\n",
    "P\\left(y=1|\\mathbf{x},\\boldsymbol{\\beta}\\right)=\\sigma\\left(\\tilde{\\mathbf{x}}^{\\mathsf{T}}\\boldsymbol{\\theta}\\right),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\sigma\\left(x\\right)=\\frac{1}{1+\\mathrm{e}^{-x}}\\quad\\text{and}\\quad\\tilde{\\mathbf{x}}=\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "\\mathbf{x}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We have \n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{X}}=\n",
    "\\begin{bmatrix}\n",
    "\\left(\\tilde{\\mathbf{x}}^{\\left(1\\right)}\\right)^{\\mathsf{T}} \\\\\n",
    "\\left(\\tilde{\\mathbf{x}}^{\\left(2\\right)}\\right)^{\\mathsf{T}} \\\\\n",
    "\\vdots \\\\\n",
    "\\left(\\tilde{\\mathbf{x}}^{\\left(N\\right)}\\right)^{\\mathsf{T}}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\n",
    "P\\left(\\mathbf{y}=\\mathbf{1}|\\mathbf{X},\\boldsymbol{\\theta}\\right)=\n",
    "\\begin{bmatrix}\n",
    "\\sigma\\left(\\left(\\tilde{\\mathbf{x}}^{\\left(1\\right)}\\right)^{\\mathsf{T}}\\boldsymbol{\\theta}\\right) \\\\\n",
    "\\sigma\\left(\\left(\\tilde{\\mathbf{x}}^{\\left(2\\right)}\\right)^{\\mathsf{T}}\\boldsymbol{\\theta}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma\\left(\\left(\\tilde{\\mathbf{x}}^{\\left(N\\right)}\\right)^{\\mathsf{T}}\\boldsymbol{\\theta}\\right) \n",
    "\\end{bmatrix}\n",
    "=\\boldsymbol{\\sigma}\\left(\\tilde{\\mathbf{X}}\\boldsymbol{\\theta}\\right)\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\sigma}$ denotes applying the logistic function to $\\tilde{\\mathbf{X}}\\boldsymbol{\\theta}$ element-wise, and where $N$ is the number of observations. We can now build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will train on the entire training set at once (versus using a \n",
    "# subset), so strictly speaking, the placeholders could be constants\n",
    "X = tf.placeholder(tf.float32, shape=[None, n + 1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "# initialize theta to the zero tensor\n",
    "theta = tf.Variable(tf.zeros([n + 1, 1]))\n",
    "\n",
    "logits = tf.matmul(X, theta)\n",
    "y = tf.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our cost function, we will use the negative log likelihood, which is given by\n",
    "\n",
    "$$\n",
    "\\ell\\left(\\boldsymbol{\\theta}\\right)=-\\sum_{i=1}^{N}\\log\\left(\\text{logit}\\left(y^{\\left(i\\right)}\\left(\\tilde{\\mathbf{x}}^{\\left(i\\right)}\\right)^{\\mathsf{T}}\\boldsymbol{\\theta}\\right)\\right)\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\text{logit}\\left(x\\right)=\\frac{1}{1+\\mathrm{e}^{-x}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = -tf.reduce_mean(\n",
    "    tf.log(\n",
    "        tf.sigmoid(\n",
    "            tf.multiply(y_, tf.matmul(X, theta))\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set some parameters and build some nodes in the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "beta = 0.5\n",
    "iter = 1000\n",
    "\n",
    "# use automatic differentiation to calculate the \n",
    "# gradient of the objective function\n",
    "df = tf.gradients(f, theta)\n",
    "\n",
    "# we will use this variable to store backtracking iterates\n",
    "theta_ = theta\n",
    "\n",
    "# and a placeholder to update theta_\n",
    "theta_new = tf.placeholder(tf.float32)\n",
    "\n",
    "# defining these operations as nodes in the graph is \n",
    "# much faster than calling tf.assign each time\n",
    "theta_bt_op = tf.assign(theta_, theta_new)\n",
    "update_op = tf.assign(theta, theta_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a convenience function to create predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X_tilde, theta):\n",
    "    '''\n",
    "    Create predictions\n",
    "    :param X_tilde: matrix whose ith row is (\\mathbf{x}^{i}, -1)\n",
    "    :param theta: predictor weights \n",
    "    '''\n",
    "    probs = 1 / (1 + np.exp(-np.matmul(X_tilde, theta)))\n",
    "    return probs >= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we initialize the global variables and run the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Step: 0 \tobjective function: 0.668975 \taccuracy: 0.4325\n",
      "LR Step: 100 \tobjective function: 0.424893 \taccuracy: 0.9075\n",
      "LR Step: 200 \tobjective function: 0.3617 \taccuracy: 0.9125\n",
      "LR Step: 300 \tobjective function: 0.305695 \taccuracy: 0.91\n",
      "LR Step: 400 \tobjective function: 0.264991 \taccuracy: 0.91\n",
      "LR Step: 500 \tobjective function: 0.253728 \taccuracy: 0.91\n",
      "LR Step: 600 \tobjective function: 0.24462 \taccuracy: 0.9075\n",
      "LR Step: 700 \tobjective function: 0.236692 \taccuracy: 0.9125\n",
      "LR Step: 800 \tobjective function: 0.230503 \taccuracy: 0.9125\n",
      "LR Step: 900 \tobjective function: 0.225204 \taccuracy: 0.915\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(iter):\n",
    "        # evaluate the gradient\n",
    "        theta0 = theta.eval()\n",
    "        df0 = sess.run(\n",
    "            df, \n",
    "            feed_dict={X: X_train, y_: y_train, theta: theta0}\n",
    "        )\n",
    "        \n",
    "        # set the gradient descent direction\n",
    "        df0_arr = np.asarray(df0).reshape(-1,1)\n",
    "        dx = -df0_arr\n",
    "        \n",
    "        # dot product doesn't work the way you think it does because \n",
    "        # numpy doesn't distinguish between 1D row and column vectors\n",
    "        delta = alpha * np.dot(df0_arr.reshape(-1,), dx.reshape(-1,))\n",
    "        \n",
    "        # evaluate the objective function at theta0\n",
    "        f0 = f.eval(\n",
    "            feed_dict={X: X_train, y_: y_train, theta: theta0}\n",
    "        )\n",
    "\n",
    "        # theta = theta0 + dx\n",
    "        sess.run(\n",
    "            theta_bt_op, feed_dict={theta_new: theta0 + dx}\n",
    "        )\n",
    "        \n",
    "        fx = f.eval(\n",
    "            feed_dict={X: X_train, y_: y_train, theta: theta_.eval()}\n",
    "        )\n",
    "\n",
    "        t = 1\n",
    "        \n",
    "        # backtracking loop\n",
    "        while (not np.isfinite(fx)) or fx > f0 + delta * t:\n",
    "            # this is cheaper than exponentiating beta each time\n",
    "            t = beta * t\n",
    "            \n",
    "            # theta_ = theta0 + t * dx\n",
    "            sess.run(\n",
    "                theta_bt_op, feed_dict={theta_new: theta0 + t * dx}\n",
    "            )\n",
    "            \n",
    "            fx = f.eval(\n",
    "                feed_dict={\n",
    "                    X: X_train, y_: y_train, theta: theta_.eval()\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # update theta with the result of backtracking\n",
    "        sess.run(update_op)\n",
    "              \n",
    "        if ((i % 100) == 0):\n",
    "            y_pred = predict(X_tilde=X_train, theta=theta.eval())\n",
    "            \n",
    "            print(\n",
    "                \"LR Step:\", i, \n",
    "                \"\\tobjective function:\", fx,\n",
    "                \"\\taccuracy:\", \n",
    "                # need to use the 0/1 version here\n",
    "                accuracy_score(y_pred, targets[:train_size])\n",
    "            )\n",
    "            \n",
    "    # save the final iterate\n",
    "    theta_ast = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the accuracy of our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92899408284023666"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(\n",
    "    targets[train_size:], \n",
    "    predict(X_test, theta_ast)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model has achieved 93% accuracy on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Consider the 3 by 2 by 2 by 2 tensor $\\mathsf{A}$ given by\n",
    "\n",
    "$$\n",
    "\\mathsf{A}=\\left[\\left[\\begin{bmatrix}1 & -1\\\\\n",
    "-2 & 1\n",
    "\\end{bmatrix},\\begin{bmatrix}1 & 1\\\\\n",
    "-2 & 2\n",
    "\\end{bmatrix}\\right],\\left[\\begin{bmatrix}2 & -1\\\\\n",
    "-1 & 1\n",
    "\\end{bmatrix},\\begin{bmatrix}2 & 1\\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}\\right],\\left[\\begin{bmatrix}1 & -2\\\\\n",
    "-2 & 1\n",
    "\\end{bmatrix},\\begin{bmatrix}1 & 1\\\\\n",
    "-1 & 1\n",
    "\\end{bmatrix}\\right]\\right]\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\mathbf{B}=\\begin{bmatrix}b_{1,1} & b_{1,2}\\\\\n",
    "b_{2,1} & b_{2,2}\n",
    "\\end{bmatrix}=\\begin{bmatrix}1 & -1\\\\\n",
    "-2 & 2\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Let $\\mathcal{F}$ denote the contraction of $\\mathsf{A}$ with $\\mathbf{B}$ along the $\\mathbf{i}=\\left\\{ 2,3\\right\\}$  and $\\mathbf{j}=\\left\\{ 1,2\\right\\}$  indices. That is,\n",
    "\n",
    "$$\n",
    "f_{i_{1},i_{2}}=\\sum_{k_{1}=1}^{2}\\sum_{k_{2}=1}^{2}a_{i_{1},k_{1},k_{2},i_{2}}b_{k_{1},k_{2}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a)\n",
    "What are the order and shape of $\\mathcal{F}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{n}=\\left(3,2,2,2\\right)$, and observe that $\\mathsf{A}\\in\\mathscr{F}_{\\mathbf{n}}$, i.e., $\\mathsf{A}$ is a fourth order tensor. Similarly, let $\\mathbf{m}=\\left(2,2\\right)$, and observe that $\\mathbf{B}\\in\\mathscr{F}_{\\mathbf{m}}$, i.e., $\\mathbf{B}$ is a second order tensor. Noting that \n",
    "\n",
    "$$\n",
    "\\mathbf{n}_{\\setminus\\mathbf{i}}=\\left(3,2\\right)\\quad\\text{and}\\quad\\mathbf{m}_{\\setminus\\mathbf{j}}=\\left(\\right)\\implies\\mathbf{n}_{\\setminus\\mathbf{i}}\\oplus\\mathbf{m}_{\\setminus\\mathbf{j}}=\\left(3,2\\right),\n",
    "$$\n",
    "\n",
    "we have the tensor contraction operator\n",
    "\n",
    "$$\n",
    "c_{\\mathbf{i},\\mathbf{j}}:\\mathscr{F}_{\\mathbf{n}}\\times\\mathscr{F}_{\\mathbf{m}}\\rightarrow\\mathscr{F}_{\\mathbf{n}_{\\setminus\\mathbf{i}}\\oplus\\mathbf{m}_{\\setminus\\mathbf{j}}}=\\mathscr{F}_{\\left(3,2\\right)}.\n",
    "$$\n",
    "\n",
    "Thus, $\\mathcal{F}$ is a second order tensor with dimensions $3\\times2$, i.e., a matrix with 3 rows and 2 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b)\n",
    "Compute all the entries of $\\mathcal{F}$ by hand, showing your work. Check your answer using TensorFlow's `tensordot` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the indexing easier, let $\\mathbf{A}_{i,j}$ be the $2\\times2$ matrix defined by\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_{i,j}=\\begin{bmatrix}a_{i,j,1,1} & a_{i,j,1,2}\\\\\n",
    "a_{i,j,2,1} & a_{i,j,2,2}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "so that \n",
    "\n",
    "$$\n",
    "\\mathsf{A}=\\left[\\left[\\mathbf{A}_{1,1},\\mathbf{A}_{1,2}\\right],\\left[\\mathbf{A}_{2,1},\\mathbf{A}_{2,2}\\right],\\left[\\mathbf{A}_{3,1},\\mathbf{A}_{3,2}\\right]\\right].\n",
    "$$\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_{i_{1},i_{2}} & =\\sum_{k_{1}=1}^{2}\\sum_{k_{2}=1}^{2}a_{i_{1},k_{1},k_{2},i_{2}}b_{k_{1},k_{2}}\\\\\n",
    " & =\\sum_{k_{1}=1}^{2}\\left(a_{i_{1},k_{1},1,i_{2}}b_{k_{1},1}+a_{i_{1},k_{1},2,i_{2}}b_{k_{1},2}\\right)\\\\\n",
    " & =a_{i_{1},1,1,i_{2}}b_{1,1}+a_{i_{1},1,2,i_{2}}b_{1,2}+a_{i_{1},2,1,i_{2}}b_{2,1}+a_{i_{1},2,2,i_{2}}b_{2,2},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $i_{1}\\in\\left\\{ 1,2,3\\right\\}$  and $i_{2}\\in\\left\\{ 1,2\\right\\}$. Denote by $\\mathbf{C}_{i}$ the $i\\text{th}$ column of $\\mathbf{C}\\in M_{m,n}$, and observe that $f_{i_{1},i_{2}}$ is equal to\n",
    "\n",
    "$$\n",
    "\\left(\\mathbf{A}_{i_{1},1}\\right)_{i_{2}}^{\\mathsf{T}}\\left(\\mathbf{B}^{\\mathsf{T}}\\right)_{1}+\\left(\\mathbf{A}_{i_{1},2}\\right)_{i_{2}}^{\\mathsf{T}}\\left(\\mathbf{B}^{\\mathsf{T}}\\right)_{2},\n",
    "$$\n",
    "\n",
    "i.e., $f_{i_{1},i_{2}}$ is the dot product of the $i_{2}\\text{th}$ column of $\\mathbf{A}_{i_{1},1}$ and the first row of $\\mathbf{B}$ plus the dot product of the $i_{2}\\text{th}$ row of $\\mathbf{A}_{i_{1},2}$ and the second row of $\\mathbf{B}$. Then, we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f_{1,1} & =a_{1,1,1,1}b_{1,1}+a_{1,1,2,1}b_{1,2}+a_{1,2,1,1}b_{2,1}+a_{1,2,2,1}b_{2,2}\\\\\n",
    " & =1\\cdot1+\\left(-2\\right)\\left(-1\\right)+1\\left(-2\\right)+\\left(-2\\right)2\\\\\n",
    " & =-3\\\\\n",
    "f_{1,2} & =a_{1,1,1,2}b_{1,1}+a_{1,1,2,2}b_{1,2}+a_{1,2,1,2}b_{2,1}+a_{1,2,2,2}b_{2,2}\\\\\n",
    " & =\\left(-1\\right)1+1\\left(-1\\right)+1\\left(-2\\right)+2\\cdot2\\\\\n",
    " & =0\\\\\n",
    "f_{2,1} & =a_{2,1,1,1}b_{1,1}+a_{2,1,2,1}b_{1,2}+a_{2,2,1,1}b_{2,1}+a_{2,2,2,1}b_{2,2}\\\\\n",
    " & =2\\cdot1+\\left(-1\\right)\\left(-1\\right)+2\\left(-2\\right)+1\\cdot2\\\\\n",
    " & =1\\\\\n",
    "f_{2,2} & =a_{2,1,1,2}b_{1,1}+a_{2,1,2,2}b_{1,2}+a_{2,2,1,2}b_{2,1}+a_{2,2,2,2}b_{2,2}\\\\\n",
    " & =\\left(-1\\right)1+1\\left(-1\\right)+1\\left(-2\\right)+2\\cdot2\\\\\n",
    " & =0\\\\\n",
    "f_{3,1} & =a_{3,1,1,1}b_{1,1}+a_{3,1,2,1}b_{1,2}+a_{3,2,1,1}b_{2,1}+a_{3,2,2,1}b_{2,2}\\\\\n",
    " & =1\\cdot1+\\left(-2\\right)\\left(-1\\right)+1\\left(-2\\right)+\\left(-1\\right)2\\\\\n",
    " & =-1\\\\\n",
    "f_{3,2} & =a_{3,1,1,2}b_{1,1}+a_{3,1,2,2}b_{1,2}+a_{3,2,1,2}b_{2,1}+a_{3,2,2,2}b_{2,2}\\\\\n",
    " & =\\left(-2\\right)1+1\\left(-1\\right)+1\\left(-2\\right)+1\\cdot2\\\\\n",
    " & =-3,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\n",
    "\\mathcal{F}=\\begin{bmatrix}-3 & 0\\\\\n",
    "1 & 0\\\\\n",
    "-1 & -3\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We now check our expression for $\\mathcal{F}$ using `tensordot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3,  0],\n",
       "       [ 1,  0],\n",
       "       [-1, -3]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = tf.Variable(\n",
    "    [\n",
    "        [\n",
    "            [\n",
    "                [1, -1], [-2, 1]\n",
    "            ],\n",
    "            [\n",
    "                [1, 1], [-2, 2]\n",
    "            ]\n",
    "        ],\n",
    "        [\n",
    "            [\n",
    "                [2, -1], [-1, 1]\n",
    "            ],\n",
    "            [\n",
    "                [2, 1], [1, 2]\n",
    "            ]\n",
    "        ],\n",
    "        [\n",
    "            [\n",
    "                [1, -2], [-2, 1]\n",
    "            ],\n",
    "            [\n",
    "                [1, 1], [-1, 1]\n",
    "            ]\n",
    "        ]\n",
    "    ], \n",
    "    name='A'\n",
    ")\n",
    "\n",
    "B = tf.Variable(\n",
    "    [\n",
    "        [1, -1], [-2, 2]\n",
    "    ], name='B'\n",
    ")\n",
    "\n",
    "f = tf.tensordot(A, B, [[1, 2], [0, 1]])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    result = f.eval()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the result of `tensordot` agrees with our expression for $\\mathcal{F}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.5675\n",
      "Training accuracy:  0.5675\n",
      "Training accuracy:  0.5675\n",
      "Training accuracy:  0.5675\n",
      "Training accuracy:  0.5675\n",
      "Training accuracy:  0.5675\n",
      "Training accuracy:  0.5675\n",
      "Training accuracy:  0.5575\n",
      "Training accuracy:  0.5675\n",
      "Training accuracy:  0.56\n",
      "Training accuracy:  0.5675\n",
      "Training accuracy:  0.5575\n",
      "Training accuracy:  0.5675\n",
      "Training accuracy:  0.5625\n",
      "Training accuracy:  0.5675\n",
      "Training accuracy:  0.545\n",
      "Training accuracy:  0.5675\n",
      "Training accuracy:  0.505\n",
      "Training accuracy:  0.5675\n",
      "Training accuracy:  0.4575\n",
      "Training accuracy:  0.5575\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.5225\n",
      "Training accuracy:  0.1925\n",
      "Training accuracy:  0.55\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.4775\n",
      "Training accuracy:  0.1575\n",
      "Training accuracy:  0.525\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.3825\n",
      "Training accuracy:  0.14\n",
      "Training accuracy:  0.47\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.275\n",
      "Training accuracy:  0.145\n",
      "Training accuracy:  0.3175\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.415\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.2325\n",
      "Training accuracy:  0.1125\n",
      "Training accuracy:  0.265\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.3125\n",
      "Training accuracy:  0.105\n",
      "Training accuracy:  0.4375\n",
      "Training accuracy:  0.13\n",
      "Training accuracy:  0.1925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.2325\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.255\n",
      "Training accuracy:  0.105\n",
      "Training accuracy:  0.305\n",
      "Training accuracy:  0.11\n",
      "Training accuracy:  0.4125\n",
      "Training accuracy:  0.155\n",
      "Training accuracy:  0.1475\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.16\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.175\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.185\n",
      "Training accuracy:  0.105\n",
      "Training accuracy:  0.205\n",
      "Training accuracy:  0.105\n",
      "Training accuracy:  0.23\n",
      "Training accuracy:  0.105\n",
      "Training accuracy:  0.2475\n",
      "Training accuracy:  0.105\n",
      "Training accuracy:  0.2525\n",
      "Training accuracy:  0.1125\n",
      "Training accuracy:  0.2675\n",
      "Training accuracy:  0.1225\n",
      "Training accuracy:  0.2675\n",
      "Training accuracy:  0.125\n",
      "Training accuracy:  0.2625\n",
      "Training accuracy:  0.125\n",
      "Training accuracy:  0.2525\n",
      "Training accuracy:  0.1225\n",
      "Training accuracy:  0.2475\n",
      "Training accuracy:  0.1175\n",
      "Training accuracy:  0.2275\n",
      "Training accuracy:  0.1125\n",
      "Training accuracy:  0.2\n",
      "Training accuracy:  0.105\n",
      "Training accuracy:  0.1825\n",
      "Training accuracy:  0.105\n",
      "Training accuracy:  0.16\n",
      "Training accuracy:  0.105\n",
      "Training accuracy:  0.1475\n",
      "Training accuracy:  0.105\n",
      "Training accuracy:  0.13\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.1225\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.085\n",
      "Training accuracy:  0.1\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.1\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.1\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.1\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.1\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.1125\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.1225\n",
      "Training accuracy:  0.1125\n",
      "Training accuracy:  0.1375\n",
      "Training accuracy:  0.1325\n",
      "Training accuracy:  0.1525\n",
      "Training accuracy:  0.17\n",
      "Training accuracy:  0.1625\n",
      "Training accuracy:  0.1875\n",
      "Training accuracy:  0.1675\n",
      "Training accuracy:  0.2025\n",
      "Training accuracy:  0.17\n",
      "Training accuracy:  0.22\n",
      "Training accuracy:  0.1725\n",
      "Training accuracy:  0.2275\n",
      "Training accuracy:  0.17\n",
      "Training accuracy:  0.22\n",
      "Training accuracy:  0.17\n",
      "Training accuracy:  0.2125\n",
      "Training accuracy:  0.1675\n",
      "Training accuracy:  0.2025\n",
      "Training accuracy:  0.1675\n",
      "Training accuracy:  0.195\n",
      "Training accuracy:  0.1675\n",
      "Training accuracy:  0.1875\n",
      "Training accuracy:  0.1625\n",
      "Training accuracy:  0.1825\n",
      "Training accuracy:  0.16\n",
      "Training accuracy:  0.17\n",
      "Training accuracy:  0.1525\n",
      "Training accuracy:  0.16\n",
      "Training accuracy:  0.15\n",
      "Training accuracy:  0.1525\n",
      "Training accuracy:  0.145\n",
      "Training accuracy:  0.1425\n",
      "Training accuracy:  0.1425\n",
      "Training accuracy:  0.1275\n",
      "Training accuracy:  0.1325\n",
      "Training accuracy:  0.1175\n",
      "Training accuracy:  0.13\n",
      "Training accuracy:  0.115\n",
      "Training accuracy:  0.13\n",
      "Training accuracy:  0.11\n",
      "Training accuracy:  0.12\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.115\n",
      "Training accuracy:  0.1\n",
      "Training accuracy:  0.115\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.115\n",
      "Training accuracy:  0.08\n",
      "Training accuracy:  0.1125\n",
      "Training accuracy:  0.085\n",
      "Training accuracy:  0.1075\n",
      "Training accuracy:  0.085\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.1025\n",
      "Training accuracy:  0.085\n",
      "Training accuracy:  0.1\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0975\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.095\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.085\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.085\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.085\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.085\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.085\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.085\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.085\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.0925\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n",
      "Training accuracy:  0.09\n",
      "Training accuracy:  0.0875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def logit(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "def loglogit(x):\n",
    "    returnnp.log(1+np.exp(-x))\n",
    "    \n",
    "def loglogitlikelihood(X,y):\n",
    "    tildeX=np.hstack([np.ones((X.shape[0],1)),X])\n",
    "    \n",
    "    f=lambda b:np.mean(loglogit(np.multiply(y,tildeX@b)))\n",
    "    df=lambda b:-tildeX.T@np.multiply(y,logit(-np.multiply(y,tildeX@b)))/X.shape[0]\n",
    "    d2f=lambda b:tildeX.T@np.diag(logit(tildeX@b)*logit(-tildeX@b))@tildeX/X.shape[0]\n",
    "    return f,df,d2f\n",
    "\n",
    "def lr_accuracy(b,X,targets):\n",
    "    tildeX=np.hstack([np.ones((X.shape[0],1)),X])\n",
    "    y_guess=np.zeros(X.shape[0])\n",
    "    y_guess[logit(tildeX@b)>0.5]=1\n",
    "    return np.mean(np.equal(targets,y_guess))\n",
    "\n",
    "data=load_breast_cancer()\n",
    "# Parameters for the data\n",
    "dim_data=30\n",
    "num_labels=2\n",
    "num_examples=569\n",
    "# Parameters for training\n",
    "learning_rate=1e-6\n",
    "num_train=400\n",
    "X=data['data']\n",
    "# Data in rows\n",
    "X=np.concatenate([np.ones((X.shape[0],1)),X],axis=1)\n",
    "targets=data.target# 0-1 labels\n",
    "labels=2*targets-1 # Converts 0-1 labels to -1 +1 labels\n",
    "\n",
    "# 0-1 labelslabels=2*targets-1# Converts 0-1 labels to -1 +1 labels\n",
    "# Let's use TensorFlow to train logisitic regression \n",
    "x=tf.placeholder(tf.float32,shape=[None,dim_data+1])\n",
    "y_=tf.placeholder(tf.float32,shape=[None,1])\n",
    "b=tf.Variable(tf.zeros([dim_data+1,1]))\n",
    "b_bt=tf.Variable(tf.zeros([dim_data+1,1]))\n",
    "nll_bt=tf.reduce_mean(-tf.log(1/(1+tf.exp(y_*x@b_bt))))\n",
    "nll=tf.reduce_mean(-tf.log(1/(1+tf.exp(y_*x@b))))\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "alpha=0.1\n",
    "beta=0.5\n",
    "for i in range(1000):\n",
    "    df=np.reshape(sess.run(tf.gradients(nll,b),feed_dict={x:X[:num_train,:],y_:np.reshape(labels[:num_train],(num_train,1))}),[31,1])\n",
    "    df_a=np.reshape(df,31)\n",
    "    delta=-np.sum(df_a*df_a)*alpha\n",
    "    t=1\n",
    "    sess.run(tf.assign(b_bt,sess.run(b)-t*df))\n",
    "    fval=sess.run(nll_bt,feed_dict={x:X[:num_train,:],y_:np.reshape(labels[:num_train],(num_train,1))})\n",
    "    \n",
    "    f0=sess.run(nll,feed_dict={x:X[:num_train,:],y_:np.reshape(labels[:num_train],(num_train,1))})\n",
    "    while (not np.isfinite) or fval>f0-delta*t:\n",
    "        t=beta*t\n",
    "        sess.run(tf.assign(b_bt,sess.run(b)-t*df))\n",
    "        fval=sess.run(nll_bt,feed_dict={x:X[:num_train,:],y_:np.reshape(labels[:num_train],(num_train,1))})\n",
    "    \n",
    "    sess.run(tf.assign(b,sess.run(b_bt)))\n",
    "    print('Training accuracy: ',lr_accuracy(np.reshape(sess.run(b),31),data['data'][:400,:],targets[:400]))\n",
    "\n",
    "\n",
    "print('accuracy: ',lr_accuracy(np.reshape(sess.run(b),31),data['data'][400:,:],targets[400:]))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
