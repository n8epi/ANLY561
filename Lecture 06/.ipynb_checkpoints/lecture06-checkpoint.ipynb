{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lecture 06: Majorization-Minimization and Expectation Maximization\n",
    "\n",
    "## Majorization-Minimization\n",
    "\n",
    "We say that a function $g:\\mathbb{R}^d\\times\\mathbb{R}^d\\rightarrow\\mathbb{R}$ **majorizes** a function $f:\\mathbb{R}^d\\rightarrow\\mathbb{R}$ if\n",
    "\n",
    "1. $g({\\bf x}, {\\bf x}^\\prime)\\geq f({\\bf x})$ for all ${\\bf x}, {\\bf x}^\\prime\\in \\mathbb{R}^d$, and\n",
    "2. $g({\\bf x}^\\prime, {\\bf x}^\\prime)=f({\\bf x}^\\prime)$ for all ${\\bf x}^\\prime\\in \\mathbb{R}^d$.\n",
    "\n",
    "Whenever we have a majorization relationship, we can define a **majorization-minimization** optimization procedure by intializing ${\\bf x}^{(0)}$ and computing the sequence\n",
    "\n",
    "$$\n",
    "{\\bf x}^{(k+1)} = \\arg\\min_{\\bf x} g({\\bf x}, {\\bf x}^{(k)}).\n",
    "$$\n",
    "\n",
    "We then have that\n",
    "\n",
    "$$\n",
    "f({\\bf x}^{(k+1)})\\leq g({\\bf x}^{(k+1)}, {\\bf x}^{(k)})\\leq g({\\bf x}^{(k)}, {\\bf x}^{(k)})=f({\\bf x}^{(k)})\n",
    "$$\n",
    "\n",
    "and it follows from the transitive property that $f({\\bf x}^{(k)})$ is a monotonically decreasing sequence of values.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Let $f(x)=x\\arctan(x)-\\log(1+x^2)/2$. Then\n",
    "\n",
    "$$\n",
    "g(x,x^\\prime) = f(x^\\prime) + \\arctan(x^\\prime)(x-x^\\prime) + \\frac{1}{2}(x-x^\\prime)^2\n",
    "$$\n",
    "\n",
    "majorizes $f$. It is easy to see that $g(x^\\prime,x^\\prime)=f(x^\\prime)$ for all $x^\\prime$. On the other hand, Taylor's theorem gives a $\\xi$ between $x$ and $x^\\prime$ such that\n",
    "\n",
    "$$\n",
    "f(x) = f(x^\\prime) + f^\\prime(x^\\prime)(x-x^\\prime) + \\frac{1}{2} f^{\\prime\\prime}(\\xi)(x-x^\\prime)^2=f(x^\\prime) + \\arctan(x^\\prime)(x-x^\\prime) + \\frac{1}{2} \\frac{1}{1+\\xi^2}(x-x^\\prime)^2\\leq f(x^\\prime) + \\arctan(x^\\prime)(x-x^\\prime) + \\frac{1}{2} (x-x^\\prime)^2 = g(x,x^\\prime).\n",
    "$$\n",
    "\n",
    "Noting that $g(x,x^\\prime)$ is convex as a function of $x$, we have that $0=\\frac{d}{dx} g(x, x^\\prime) = \\arctan(x^\\prime) + (x-x^\\prime)$ is necessary and sufficient for optimality. Thus, $x = x^\\prime - \\arctan(x^\\prime)$ is the only minimizer of $g(x, x^\\prime)$, so the iterates in majorization-minimization will be\n",
    "\n",
    "$$\n",
    "x^{(k+1)} = x^{(k)} - \\arctan(x^{(k)}).\n",
    "$$\n",
    "\n",
    "## A Large Class of Examples\n",
    "\n",
    "\n",
    "Based on the reasoning in this example, if $f\\in C^2(\\mathbb{R})$ with $\\vert f^{\\prime\\prime}(x)\\vert\\leq C$ for all $x\\in\\mathbb{R}$, we have that\n",
    "\n",
    "$$\n",
    "g(x,x^\\prime) = f(x^\\prime) + f^\\prime(x^\\prime)(x-x^\\prime) + \\frac{C}{2}(x-x^\\prime)^2.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The generalization to higher dimensions requires us to define the **operator** or **spectral** norm of a square matrix:\n",
    "\n",
    "$$\n",
    "\\Vert A\\Vert_{\\text{op}} = \\min_{\\Vert {\\bf u}\\Vert=1} {\\bf u}^T A {\\bf u}.\n",
    "$$\n",
    "\n",
    "We then get that, if $f\\in C^2(\\mathbb{R}^d)$ and $\\Vert \\nabla^2 f({\\bf x})\\Vert_{\\text{op}}\\leq C$ for all ${\\bf x}\\in\\mathbb{R}^d$, then \n",
    "\n",
    "$$\n",
    "g({\\bf x}, {\\bf x}^\\prime)= f({\\bf x}^\\prime) + \\nabla f({\\bf x}^\\prime)^T({\\bf x}-{\\bf x}^\\prime) + \\frac{C}{2}\\Vert {\\bf x}-{\\bf x}^\\prime\\Vert^2.\n",
    "$$\n",
    "\n",
    "The reason we get this generalization is that Taylor's theorem extends to higher dimensions. In particular, for $f\\in C^2(\\mathbb{R}^d)$, there is a $\\xi$ on the *line segment connecting* ${\\bf x}$ and ${\\bf x}^\\prime$ with\n",
    "\n",
    "$$\n",
    "f({\\bf x}) = f({\\bf x}^\\prime) + \\nabla f({\\bf x}^\\prime)^T({\\bf x}-{\\bf x}^\\prime) + \\frac{1}{2}({\\bf x}-{\\bf x}^\\prime)^T\\nabla^2 f(\\xi)({\\bf x}-{\\bf x}^\\prime).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation-Maximization\n",
    "\n",
    "Expectation-Maximization is a technique for fitting **latent mixture models** using (essentially) majorization minimization.\n",
    "\n",
    "### Latent mixture models\n",
    "\n",
    "A latent mixture model is a random variable $X$ whose probability density function has the form\n",
    "\n",
    "$$\n",
    "p(X=x) = \\sum_{i=1}^k p(X=x, C=i).\n",
    "$$\n",
    "\n",
    "That is, the density function is a marginal of a joint probability density over random variables $X$ and $C$, where $C$ takes values in the finite set $\\{1,\\ldots, k\\}$. We call $C$ the **latent variable** because it is generally never observed. Using the multiplicative law of probability, we have\n",
    "\n",
    "$$\n",
    "p(X=x) = \\sum_{i=1}^k p(X=x\\vert C=i) p(C=i),\n",
    "$$\n",
    "\n",
    "which tells us that $p(X=x)$ is a weighted mixture of the conditional densities $p(X=x\\vert C=i)$, with weights given by $p(C=i)$. It therefore makes sense to call this a *latent mixture model*.\n",
    "\n",
    "This representation of $p(X)$ also indicates that $X$ can be generated in a *hierarchical manner*:\n",
    "\n",
    "1. Draw $c$ according to the density $p(C)$\n",
    "2. Draw $X$ according to the conditional density $p(X\\vert C=c)$\n",
    "\n",
    "### Fitting parametric latent mixture models\n",
    "\n",
    "**Density estimation** seeks to estimate the true density of a random variable $X$. That is, we are given data $\\{ X_n\\}_{n=1}^N$ and we use this to estimate $\\widehat{p}(X)$ which is suitably close to the true density $p(X)$. One way to do this is by setting up a **parameteric latent mixture model**. To do this we restrict our estimate to densities of the form\n",
    "\n",
    "$$\n",
    "p(X=x; \\theta_1,\\ldots, \\theta_k) = \\sum_{i=1}^k p(X=x;\\theta_i)p(C=i)\n",
    "$$\n",
    "\n",
    "where $p(X=x;\\theta)$ is a density that depends on the parameter $\\theta$, and $k$ is a fixed hyperparameter. For example, we could use the Gaussian parametric family\n",
    "\n",
    "$$\n",
    "p(X=x; \\theta) = \\frac{1}{\\sqrt{2\\pi}} e^{-(x-\\theta)^2/2}\\text{ for }\\theta\\in\\mathbb{R}\n",
    "$$\n",
    "\n",
    "and then\n",
    "\n",
    "$$\n",
    "p(X=x; \\theta_1,\\ldots, \\theta_k) = \\sum_{i=1}^k \\frac{1}{\\sqrt{2\\pi}} e^{-(x-\\theta_i)^2/2}p(C=i).\n",
    "$$\n",
    "\n",
    "Now, we also don't know $\\alpha_i=p(C=i)$, so this must be estimated as well. Thus, a simple **Gaussian mixture model** would look like\n",
    "\n",
    "$$\n",
    "p(X=x; \\theta_1,\\ldots, \\theta_k,\\alpha_1,\\ldots,\\alpha_k) = \\sum_{i=1}^k \\frac{\\alpha_i}{\\sqrt{2\\pi}} e^{-(x-\\theta_i)^2/2}.\n",
    "$$\n",
    "\n",
    "where $\\theta_i,\\alpha_i\\in \\mathbb{R}$, $\\alpha_i\\geq0$, and $\\sum_{i=1}^k\\alpha_i=1$. \n",
    "\n",
    "A more flexible Gaussian mixture model would allow us to also fit the variance inside of the *mixture components*. Thus, the general Gaussian mixture model for a 1D random variable is given by\n",
    "\n",
    "$$\n",
    "p(X=x; \\mu_1,\\ldots, \\mu_k,\\sigma_1,\\ldots,\\sigma_k,\\alpha_1,\\ldots,\\alpha_k) = \\sum_{i=1}^k \\frac{\\alpha_i}{\\sqrt{2\\pi\\sigma_i^2}} e^{-\\frac{1}{2\\sigma_i^2}(x-\\mu_i)^2}.\n",
    "$$\n",
    "\n",
    "where $\\mu_i,\\sigma_i,\\alpha_i\\in\\mathbb{R}$, $\\sigma_i>0$, $\\alpha_i\\geq 0$, and $\\sum_{i=1}^k\\alpha_i=1$.\n",
    "\n",
    "In particular, if we wanted to try to approximate a *bimodal distribution*, we might try to fit\n",
    "\n",
    "$$\n",
    "p(X=x;\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2)= \\alpha_1 p(X=x;\\mu_1,\\sigma_1) + \\alpha_2 p(X-x;\\mu_2,\\sigma_2)=\\frac{\\alpha_1}{\\sqrt{2\\pi\\sigma_1^2}}e^{-\\frac{1}{2\\sigma_1^2}(x-\\mu_1)^2} + \\frac{\\alpha_2}{\\sqrt{2\\pi\\sigma_2^2}}e^{-\\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2}\n",
    "$$\n",
    "\n",
    "to a dataset $\\{x_n\\}_{n=1}^N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood for parameter estimation in the two-component Gaussian mixture model\n",
    "\n",
    "The most straightforward way to fit this parametric model to data is to form a likelihood to maximize:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2) = \\prod_{n=1}^N p(X=x_n;\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2).\n",
    "$$\n",
    "\n",
    "This is converted to a negative log-likelihood \n",
    "\n",
    "$$\n",
    "\\ell(\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2) = -\\sum_{n=1}^N\\log p(X=x_n;\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2)= -\\sum_{n=1}^N\\log \\left(\\alpha_1p(X=x_n;\\mu_1,\\sigma_1) + \\alpha_2 p(X=x_n;\\mu_2,\\sigma_2)\\right)\n",
    "$$\n",
    "\n",
    "To find a function which majorizes this sum, we first find a function $g_x(\\theta,\\theta^\\prime)$ which majorizes\n",
    "$$\n",
    "f_x(\\theta)=-\\log\\left(\\alpha_1p(X=x;\\mu_1,\\sigma_1) + \\alpha_2 p(X=x;\\mu_2,\\sigma_2)\\right)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\theta = (\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\theta^\\prime = (\\mu_1^\\prime,\\mu_2^\\prime,\\sigma_1^\\prime,\\sigma_2^\\prime,\\alpha_1^\\prime,\\alpha_2^\\prime).\n",
    "$$\n",
    "We would then have that $\\ell(\\theta)=\\sum_{n=1}^N f_{x_i}(\\theta)$ is majorized by $g_{x_i}(\\theta,\\theta^\\prime)$.\n",
    "\n",
    "We set\n",
    "$$\n",
    "q_i(\\theta) = \\alpha_i p(X=x;\\mu_i,\\sigma_i)\n",
    "$$\n",
    "for $i=1,2$. We claim that\n",
    "$$\n",
    "g_x(\\theta, \\theta^\\prime) = -\\frac{q_1(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(\\frac{q_1(\\theta)}{q_1(\\theta^\\prime)}(q_1(\\theta^\\prime)+q_2(\\theta^\\prime))\\right) -\\frac{q_2(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(\\frac{q_2(\\theta)}{q_2(\\theta^\\prime)}(q_1(\\theta^\\prime)+q_2(\\theta^\\prime))\\right)\n",
    "$$\n",
    "majorizes $f_x(\\theta)$. To simplify a little further, we note that\n",
    "$$\n",
    "\\frac{q_i(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\n",
    "$$\n",
    "is the conditional probility of that $x$ was drawn from component $i$, $p(C=i\\vert X=x;\\theta^\\prime)$. In particular, $p(C=1\\vert X=x;\\theta^\\prime)+p(C=2\\vert X=x;\\theta^\\prime)=1$. Thus,\n",
    "\n",
    "$$\n",
    "g_x(\\theta, \\theta^\\prime) = -p(C=1\\vert X=x;\\theta^\\prime)\\log\\left(\\frac{q_1(\\theta)}{p(C=1\\vert X=x;\\theta^\\prime)}\\right) -p(C=2\\vert X=x;\\theta^\\prime)\\log\\left(\\frac{q_2(\\theta)}{p(C=2\\vert X=x;\\theta^\\prime)}\\right)\n",
    "$$\n",
    "\n",
    "We first note that\n",
    "\n",
    "$$\n",
    "g_x(\\theta^\\prime,\\theta^\\prime)= \\frac{q_1(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(\\frac{q_1(\\theta^\\prime)}{q_1(\\theta^\\prime)}(q_1(\\theta^\\prime)+q_2(\\theta^\\prime))\\right) -\\frac{q_2(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(\\frac{q_2(\\theta^\\prime)}{q_2(\\theta^\\prime)}(q_1(\\theta^\\prime)+q_2(\\theta^\\prime))\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_x(\\theta^\\prime,\\theta^\\prime)=\\frac{q_1(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(q_1(\\theta^\\prime)+q_2(\\theta^\\prime)\\right) -\\frac{q_2(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(q_1(\\theta^\\prime)+q_2(\\theta^\\prime)\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_x(\\theta^\\prime,\\theta^\\prime)=-(p(C=1\\vert X=x;\\theta^\\prime)+p(C=2\\vert X=x;\\theta^\\prime))\\log\\left(q_1(\\theta^\\prime)+q_2(\\theta^\\prime)\\right)=-\\log\\left(q_1(\\theta^\\prime)+q_2(\\theta^\\prime)\\right)=f_x(\\theta^\\prime).\n",
    "$$\n",
    "\n",
    "To show majorization, we see that\n",
    "\n",
    "$$\n",
    "f_x(\\theta)=-\\log\\left(q_1(\\theta)+q_2(\\theta)\\right)=-\\log\\left(p(C=1\\vert X=x;\\theta^\\prime)\\frac{q_1(\\theta)}{p(C=1\\vert X=x;\\theta^\\prime)}+p(C=2\\vert X=x;\\theta^\\prime)\\frac{q_2(\\theta)}{p(C=2\\vert X=x;\\theta^\\prime)}\\right).\n",
    "$$\n",
    "\n",
    "Noting that $-\\log$ is a convex function, we have\n",
    "\n",
    "$$\n",
    "f_x(\\theta)\\leq -p(C=1\\vert X=x;\\theta^\\prime)\\log\\left(\\frac{q_1(\\theta)}{p(C=1\\vert X=x;\\theta^\\prime)}\\right) -p(C=2\\vert X=x;\\theta^\\prime)\\log\\left(\\frac{q_2(\\theta)}{p(C=2\\vert X=x;\\theta^\\prime)}\\right)=g_x(\\theta,\\theta^\\prime).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Expectation-Maximization \n",
    "\n",
    "While majorization-minimization indicates how to proceed to begin minimizing $\\ell(\\theta)$, we now explain why this algorithm is equivalent to the **expectation-maximization** procedure. First, we introduce a likelihood function where the latent variables $c_n$ are known. That is, suppose we have the data $\\mathcal{X}=\\{x_n\\}_{n=1}^N$ and associated latent variables $\\mathcal{C}=\\{i_n\\}_{n=1}^N$. Then the likelihood of the parameter $\\theta$ given this data is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta; \\mathcal{X}, \\mathcal{C}) = \\prod_{n=1}^N \\alpha_{i_n}p(X=x_n; \\mu_{i_n}, \\sigma_{i_n})\n",
    "$$\n",
    "\n",
    "and the negative log-likelihood is given by\n",
    "\n",
    "\n",
    "$$\n",
    "\\ell(\\theta; \\mathcal{X}, \\mathcal{C}) = -\\sum_{n=1}^N \\log\\left(\\alpha_{i_n}p(X=x_n; \\mu_{i_n}, \\sigma_{i_n})\\right)\n",
    "$$\n",
    "\n",
    "For our mixture of two components, the expectation-maximization algorithm has the following form:\n",
    "\n",
    "1. **E Step**: Compute $p(C=i|X=x_n;\\theta^{(k)})$ for $i=1,2$ and $n=1,\\ldots, N$ to form the conditional expectation\n",
    "\n",
    "$$\n",
    "Q(\\theta\\vert\\theta^{(k)})= \\mathbb{E}_{\\mathcal{C}\\vert \\mathcal{X};\\theta^{(k)}} \\log\\mathcal{L}(\\theta;\\mathcal{X}, \\mathcal{C}) = \\mathbb{E}_{\\mathcal{C}\\vert \\mathcal{X};\\theta^{(k)}}\\sum_{n=1}^N \\log\\left(\\alpha_{i_n}p(X=x_n; \\mu_{i_n}, \\sigma_{i_n})\\right) = \\sum_{n=1}^N \\mathbb{E}_{\\mathcal{C}\\vert \\mathcal{X};\\theta^{(k)}}\\log\\left(\\alpha_{i_n}p(X=x_n; \\mu_{i_n}, \\sigma_{i_n})\\right),\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "Q(\\theta\\vert\\theta^{(k)})= \\sum_{n=1}^N p(C=1\\vert X=x_n;\\theta^{(k)})\\log\\left(\\alpha_{1}p(X=x_n; \\mu_{1}, \\sigma_{1})\\right) + p(C=2\\vert X=x_n;\\theta^{(k)})\\log\\left(\\alpha_{2}p(X=x_n; \\mu_{2}, \\sigma_{2})\\right),\n",
    "$$\n",
    "\n",
    "which becomes\n",
    "\n",
    "$$\n",
    "Q(\\theta\\vert\\theta^{(k)})= \\sum_{n=1}^N p(C=1\\vert X=x_n;\\theta^{(k)})\\log\\left(\\alpha_{1}p(X=x_n; \\mu_{1}, \\sigma_{1})\\right) + p(C=2\\vert X=x_n;\\theta^{(k)})\\log\\left(\\alpha_{2}p(X=x_n; \\mu_{2}, \\sigma_{2})\\right),\n",
    "$$\n",
    "\n",
    "2. **M step**: Solve $\\theta^{(k+1)} = \\arg\\max_{\\theta} Q(\\theta\\vert \\theta^{(k)})$. Note that this program is equivalent to the program\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\sum_{n=1}^N g_{x_n}(\\theta, \\theta^{(k)})\n",
    "$$\n",
    "\n",
    "since\n",
    "\n",
    "$$\n",
    "g_{x}(\\theta,\\theta^{(k)})=-p(C=1\\vert X=x;\\theta^{(k)})\\log\\left(\\alpha_{1}p(X=x; \\mu_{1}, \\sigma_{1})\\right) - p(C=2\\vert X=x;\\theta^{(k)})\\log\\left(\\alpha_{2}p(X=x; \\mu_{2}, \\sigma_{2})\\right)\\\\ + p(C=1\\vert X=x;\\theta^{(k)})\\log\\left(p(C=1\\vert X=x;\\theta^{(k)})\\right) + p(C=2\\vert X=x;\\theta^{(k)})\\log\\left(p(C=2\\vert X=x;\\theta^{(k)})\\right),\n",
    "$$\n",
    "\n",
    "and hence\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^N g_{x_n}(\\theta, \\theta^{(k)}) = -Q(\\theta\\vert\\theta^{(k)})\n",
    "+\\sum_{n=1}^N p(C=1\\vert X=x;\\theta^{(k)})\\log\\left(p(C=1\\vert X=x;\\theta^{(k)})\\right) + p(C=2\\vert X=x;\\theta^{(k)})\\log\\left(p(C=2\\vert X=x;\\theta^{(k)})\\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "First, it is relatively simple to compute the quantities\n",
    "\n",
    "$$\n",
    "q_{i,n}=p(C=i\\vert X=x_n;\\theta^{(k)}) = \\frac{\\alpha_i p(X=x_n;\\mu_i^{(k)},\\sigma_i^{(k)})}{\\alpha_1 p(X=x_n;\\mu_1^{(k)},\\sigma_1^{(k)})+\\alpha_2 p(X=x_n;\\mu_2^{(k)},\\sigma_2^{(k)})}\n",
    "$$\n",
    "\n",
    "Next, we observe that\n",
    "\n",
    "$$\n",
    "\\log\\left(\\alpha_i p(X=x_n; \\mu_i,\\sigma_i)\\right)=\\log\\left(\\alpha_i \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}e^{-\\frac{(x_n-\\mu_i)^2}{2\\sigma_i^2}}\\right)=\\log(\\alpha_i)-\\frac{1}{2}\\log(2\\pi)-\\log(\\sigma_i) -\\frac{(x_n-\\mu_i)^2}{2\\sigma_i^2},\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^N g_{x_n}(\\theta,\\theta^{(k)}) = \\sum_{n=1}^N q_{1, n}\\log(\\alpha_1)+\\sum_{n=1}^N q_{2, n}\\log(\\alpha_2)-\\sum_{n=1}^N q_{1, n}\\frac{1}{2}\\log(2\\pi)-\\sum_{n=1}^N q_{2, n}\\frac{1}{2}\\log(2\\pi)-\\sum_{n=1}^N q_{1, n}\\left(\\log(\\sigma_1) +\\frac{(x_n-\\mu_1)^2}{2\\sigma_1^2}\\right)-\\sum_{n=1}^N q_{2, n}\\left(\\log(\\sigma_2) +\\frac{(x_n-\\mu_2)^2}{2\\sigma_2^2}\\right).\n",
    "$$\n",
    "\n",
    "Minimizing subject to the constraint $\\alpha_1+\\alpha_2$ gives us a Lagrange multiplier such that\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{\\alpha_1}\\sum_{n=1}^N q_{1, n}\\\\\n",
    "\\frac{1}{\\alpha_2}\\sum_{n=1}^N q_{2, n}\n",
    "\\end{pmatrix}=\\lambda\\begin{pmatrix} 1\\\\ 1\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and therefore $\\alpha_1^{(k+1)} = \\frac{1}{N} \\sum_{n=1}^N q_{1, n}$ and $\\alpha_2^{(k+1)} = \\frac{1}{N}\\sum_{n=1}^N q_{2, n}$.\n",
    "\n",
    "Taking the gradient with respect to the unconstrained variables $\\mu_1$ and $\\mu_2$, we obtain the necessary conditions\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma_1^2}\\sum_{n=1}^N q_{1, n} (x_n - \\mu_1)=0\\text{ and } \\frac{1}{\\sigma_2^2}\\sum_{n=1}^N q_{2, n} (x_n - \\mu_2)=0\n",
    "$$\n",
    "\n",
    "so that \n",
    "\n",
    "$$\n",
    "\\mu_1^{(k+1)}=\\frac{1}{\\sum_{n=1}^N q_{1, n}}\\sum_{n=1}^N q_{1, n} x_n\\text{ and } \\mu_2^{(k+1)}=\\frac{1}{\\sum_{n=1}^N q_{2, n}}\\sum_{n=1}^N q_{2, n} x_n.\n",
    "$$\n",
    "\n",
    "Finally, taking the gradient with respect to $\\sigma_1$ and $\\sigma_2$ give the conditions\n",
    "\n",
    "$$\n",
    "-\\sum_{n=1}^N q_{i, n}\\left(\\frac{1}{\\sigma_i} -\\frac{(x_n-\\mu_i^{(k+1)})^2}{\\sigma_i^3}\\right)=0\\text{ for }i=1,2\n",
    "$$\n",
    "\n",
    "which reduces to\n",
    "\n",
    "$$\n",
    "\\sigma_i^2 = \\left(\\frac{1}{\\sum_{n=1}^N q_{i, n}}\\sum_{n=1}^N q_{i, n} x_n^2\\right) - \\left(\\mu_i^{(k+1)}\\right)^2\\text{ for }i=1,2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCOFiJPELIYSL+X+f9nFb49FhagAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a533198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gauss_1D_pdf(x, mu,sig):\n",
    "    return np.exp(-(x-mu)**2/(2*sig**2))/np.sqrt(2*np.pi*sig**2)\n",
    "\n",
    "def two_gauss_mix_pdf(th):\n",
    "    def f(x):\n",
    "        return th[0] * gauss_1D_pdf(x, th[1], th[2]) + th[3] * gauss_1D_pdf(x, th[4], th[5])\n",
    "    return f\n",
    "    \n",
    "def rand_two_gauss_mix(n, th):\n",
    "    latent = rd.choice(2, n, p=th[[0,3]])\n",
    "    x = rd.randn(n)\n",
    "    return (1-latent)*(th[2]*x + th[1]) + latent*(th[5]*x + th[4])\n",
    "    \n",
    "def estep(x, th):\n",
    "    comp_1_likelihoods = th[0] * gauss_1D_pdf(x, th[1], th[2])\n",
    "    comp_2_likelihoods = th[3] * gauss_1D_pdf(x, th[4], th[5])\n",
    "    normalizations = comp_1_likelihoods + comp_2_likelihoods\n",
    "    cond_probs_1 = comp_1_likelihoods / normalizations\n",
    "    cond_probs_2 = comp_2_likelihoods / normalizations\n",
    "    return np.vstack([cond_probs_1, cond_probs_2])\n",
    "\n",
    "def mstep(th, x, cond_probs):\n",
    "    th_new = np.zeros(6)\n",
    "    sum_cond_probs = np.sum(cond_probs, axis = 1)\n",
    "    th_new[[0, 3]] = sum_cond_probs / x.size\n",
    "    th_new[[1, 4]] = np.sum(cond_probs * x, axis = 1) / sum_cond_probs\n",
    "    th_new[[2, 5]] = np.sqrt((np.sum(cond_probs * x**2, axis=1) / sum_cond_probs) - th_new[[1, 4]]**2)\n",
    "    return th_new\n",
    "\n",
    "rd.seed(1234)\n",
    "\n",
    "theta_true = np.array([0.25, 0, 1, 0.75, 4, 1])\n",
    "\n",
    "num_samples = 100\n",
    "\n",
    "theta_init = [0.5, -1, 2, 0.5, 1, 2]\n",
    "theta = theta_init\n",
    "x = rand_two_gauss_mix(num_samples, theta_true)\n",
    "\n",
    "plt.scatter(x, np.zeros(num_samples))\n",
    "t = np.linspace(np.min(x), np.max(x), 100)\n",
    "f = two_gauss_mix_pdf(theta_true)\n",
    "plt.plot(t, f(t))\n",
    "\n",
    "iters = 400\n",
    "for it in range(iters):\n",
    "    f_est = two_gauss_mix_pdf(theta)\n",
    "    \n",
    "    cond_probs = estep(x, theta)\n",
    "    #print(cond_probs)\n",
    "    theta = mstep(theta, x, cond_probs)\n",
    "    #print(theta)\n",
    "\n",
    "plt.plot(t, f_est(t))\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Group Problems\n",
    "\n",
    "1. Groups 1 and 2: Derive the EM steps for a mixture of thee Gaussians\n",
    "2. Groups 3 and 4: Derive the EM steps for a mixture of two Gaussians in 2D\n",
    "3. Groups 5 and 6: Derive the EM steps for a mixture of two Cauchy random variables: \n",
    "$$\n",
    "p(X=x;\\mu,\\gamma) = \\frac{1}{\\pi\\gamma\\left(1+\\left(\\frac{(x-\\mu)}{\\gamma}\\right)^2\\right)}\n",
    "$$\n",
    "4. Groups 7 and 8: Derive the EM steps for approximating the joint probability table $P=(p_{i, j})\\in M_{m, n}$ with $p_{i, j}\\geq 0$ and $\\sum_i\\sum_j p_{i, j}=0$ using a latent mixture of two independent components: $\\alpha_1 {\\bf p}_1{\\bf q}_1^T + \\alpha_2 {\\bf p}_2{\\bf q}_2^T$ where $\\alpha_i\\in\\mathbb{R}$, and ${\\bf p}_i\\in\\mathbb{R}^m$, ${\\bf q}_i\\in\\mathbb{R}^n$ for $i=1, 2$ are **probability vectors**. That is, they have non-negative entries and their entries sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
