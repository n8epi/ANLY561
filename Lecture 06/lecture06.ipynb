{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lecture 06: Majorization-Minimization and Expectation Maximization\n",
    "\n",
    "## Majorization-Minimization\n",
    "\n",
    "We say that a function $g:\\mathbb{R}^d\\times\\mathbb{R}^d\\rightarrow\\mathbb{R}$ **majorizes** a function $f:\\mathbb{R}^d\\rightarrow\\mathbb{R}$ if\n",
    "\n",
    "1. $g({\\bf x}, {\\bf x}^\\prime)\\geq f({\\bf x})$ for all ${\\bf x}, {\\bf x}^\\prime\\in \\mathbb{R}^d$, and\n",
    "2. $g({\\bf x}^\\prime, {\\bf x}^\\prime)=f({\\bf x}^\\prime)$ for all ${\\bf x}^\\prime\\in \\mathbb{R}^d$.\n",
    "\n",
    "Whenever we have a majorization relationship, we can define a **majorization-minimization** optimization procedure by intializing ${\\bf x}^{(0)}$ and computing the sequence\n",
    "\n",
    "$$\n",
    "{\\bf x}^{(k+1)} = \\arg\\min_{\\bf x} g({\\bf x}, {\\bf x}^{(k)}).\n",
    "$$\n",
    "\n",
    "We then have that\n",
    "\n",
    "$$\n",
    "f({\\bf x}^{(k+1)})\\leq g({\\bf x}^{(k+1)}, {\\bf x}^{(k)})\\leq g({\\bf x}^{(k)}, {\\bf x}^{(k)})=f({\\bf x}^{(k)})\n",
    "$$\n",
    "\n",
    "and it follows from the transitive property that $f({\\bf x}^{(k)})$ is a monotonically decreasing sequence of values.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Let $f(x)=x\\arctan(x)-\\log(1+x^2)/2$. Then\n",
    "\n",
    "$$\n",
    "g(x,x^\\prime) = f(x^\\prime) + \\arctan(x^\\prime)(x-x^\\prime) + \\frac{1}{2}(x-x^\\prime)^2\n",
    "$$\n",
    "\n",
    "majorizes $f$. It is easy to see that $g(x^\\prime,x^\\prime)=f(x^\\prime)$ for all $x^\\prime$. On the other hand, Taylor's theorem gives a $\\xi$ between $x$ and $x^\\prime$ such that\n",
    "\n",
    "$$\n",
    "f(x) = f(x^\\prime) + f^\\prime(x^\\prime)(x-x^\\prime) + \\frac{1}{2} f^{\\prime\\prime}(\\xi)(x-x^\\prime)^2=f(x^\\prime) + \\arctan(x^\\prime)(x-x^\\prime) + \\frac{1}{2} \\frac{1}{1+\\xi^2}(x-x^\\prime)^2\\leq f(x^\\prime) + \\arctan(x^\\prime)(x-x^\\prime) + \\frac{1}{2} (x-x^\\prime)^2 = g(x,x^\\prime).\n",
    "$$\n",
    "\n",
    "Noting that $g(x,x^\\prime)$ is convex as a function of $x$, we have that $0=\\frac{d}{dx} g(x, x^\\prime) = \\arctan(x^\\prime) + (x-x^\\prime)$ is necessary and sufficient for optimality. Thus, $x = x^\\prime - \\arctan(x^\\prime)$ is the only minimizer of $g(x, x^\\prime)$, so the iterates in majorization-minimization will be\n",
    "\n",
    "$$\n",
    "x^{(k+1)} = x^{(k)} - \\arctan(x^{(k)}).\n",
    "$$\n",
    "\n",
    "## A Large Class of Examples\n",
    "\n",
    "\n",
    "Based on the reasoning in this example, if $f\\in C^2(\\mathbb{R})$ with $\\vert f^{\\prime\\prime}(x)\\vert\\leq C$ for all $x\\in\\mathbb{R}$, we have that\n",
    "\n",
    "$$\n",
    "g(x,x^\\prime) = f(x^\\prime) + f^\\prime(x^\\prime)(x-x^\\prime) + \\frac{C}{2}(x-x^\\prime)^2.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The generalization to higher dimensions requires us to define the **operator** or **spectral** norm of a square matrix:\n",
    "\n",
    "$$\n",
    "\\Vert A\\Vert_{\\text{op}} = \\min_{\\Vert {\\bf u}\\Vert=1} {\\bf u}^T A {\\bf u}.\n",
    "$$\n",
    "\n",
    "We then get that, if $f\\in C^2(\\mathbb{R}^d)$ and $\\Vert \\nabla^2 f({\\bf x})\\Vert_{\\text{op}}\\leq C$ for all ${\\bf x}\\in\\mathbb{R}^d$, then \n",
    "\n",
    "$$\n",
    "g({\\bf x}, {\\bf x}^\\prime)= f({\\bf x}^\\prime) + \\nabla f({\\bf x}^\\prime)^T({\\bf x}-{\\bf x}^\\prime) + \\frac{C}{2}\\Vert {\\bf x}-{\\bf x}^\\prime\\Vert^2.\n",
    "$$\n",
    "\n",
    "The reason we get this generalization is that Taylor's theorem extends to higher dimensions. In particular, for $f\\in C^2(\\mathbb{R}^d)$, there is a $\\xi$ on the *line segment connecting* ${\\bf x}$ and ${\\bf x}^\\prime$ with\n",
    "\n",
    "$$\n",
    "f({\\bf x}) = f({\\bf x}^\\prime) + \\nabla f({\\bf x}^\\prime)^T({\\bf x}-{\\bf x}^\\prime) + \\frac{1}{2}({\\bf x}-{\\bf x}^\\prime)^T\\nabla^2 f(\\xi)({\\bf x}-{\\bf x}^\\prime).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation-Maximization\n",
    "\n",
    "Expectation-Maximization is a technique for fitting **latent mixture models** using (essentially) majorization minimization.\n",
    "\n",
    "### Latent mixture models\n",
    "\n",
    "A latent mixture model is a random variable $X$ whose probability density function has the form\n",
    "\n",
    "$$\n",
    "p(X=x) = \\sum_{i=1}^k p(X=x, C=i).\n",
    "$$\n",
    "\n",
    "That is, the density function is a marginal of a joint probability density over random variables $X$ and $C$, where $C$ takes values in the finite set $\\{1,\\ldots, k\\}$. We call $C$ the **latent variable** because it is generally never observed. Using the multiplicative law of probability, we have\n",
    "\n",
    "$$\n",
    "p(X=x) = \\sum_{i=1}^k p(X=x\\vert C=i) p(C=i),\n",
    "$$\n",
    "\n",
    "which tells us that $p(X=x)$ is a weighted mixture of the conditional densities $p(X=x\\vert C=i)$, with weights given by $p(C=i)$. It therefore makes sense to call this a *latent mixture model*.\n",
    "\n",
    "This representation of $p(X)$ also indicates that $X$ can be generated in a *hierarchical manner*:\n",
    "\n",
    "1. Draw $c$ according to the density $p(C)$\n",
    "2. Draw $X$ according to the conditional density $p(X\\vert C=c)$\n",
    "\n",
    "### Fitting parametric latent mixture models\n",
    "\n",
    "**Density estimation** seeks to estimate the true density of a random variable $X$. That is, we are given data $\\{ X_n\\}_{n=1}^N$ and we use this to estimate $\\widehat{p}(X)$ which is suitably close to the true density $p(X)$. One way to do this is by setting up a **parameteric latent mixture model**. To do this we restrict our estimate to densities of the form\n",
    "\n",
    "$$\n",
    "p(X=x; \\theta_1,\\ldots, \\theta_k) = \\sum_{i=1}^k p(X=x;\\theta_i)p(C=i)\n",
    "$$\n",
    "\n",
    "where $p(X=x;\\theta)$ is a density that depends on the parameter $\\theta$, and $k$ is a fixed hyperparameter. For example, we could use the Gaussian parametric family\n",
    "\n",
    "$$\n",
    "p(X=x; \\theta) = \\frac{1}{\\sqrt{2\\pi}} e^{-(x-\\theta)^2/2}\\text{ for }\\theta\\in\\mathbb{R}\n",
    "$$\n",
    "\n",
    "and then\n",
    "\n",
    "$$\n",
    "p(X=x; \\theta_1,\\ldots, \\theta_k) = \\sum_{i=1}^k \\frac{1}{\\sqrt{2\\pi}} e^{-(x-\\theta_i)^2/2}p(C=i).\n",
    "$$\n",
    "\n",
    "Now, we also don't know $\\alpha_i=p(C=i)$, so this must be estimated as well. Thus, a simple **Gaussian mixture model** would look like\n",
    "\n",
    "$$\n",
    "p(X=x; \\theta_1,\\ldots, \\theta_k,\\alpha_1,\\ldots,\\alpha_k) = \\sum_{i=1}^k \\frac{\\alpha_i}{\\sqrt{2\\pi}} e^{-(x-\\theta_i)^2/2}.\n",
    "$$\n",
    "\n",
    "where $\\theta_i,\\alpha_i\\in \\mathbb{R}$, $\\alpha_i\\geq0$, and $\\sum_{i=1}^k\\alpha_i=1$. \n",
    "\n",
    "A more flexible Gaussian mixture model would allow us to also fit the variance inside of the *mixture components*. Thus, the general Gaussian mixture model for a 1D random variable is given by\n",
    "\n",
    "$$\n",
    "p(X=x; \\mu_1,\\ldots, \\mu_k,\\sigma_1,\\ldots,\\sigma_k,\\alpha_1,\\ldots,\\alpha_k) = \\sum_{i=1}^k \\frac{\\alpha_i}{\\sqrt{2\\pi\\sigma_i^2}} e^{-\\frac{1}{2\\sigma_i^2}(x-\\mu_i)^2}.\n",
    "$$\n",
    "\n",
    "where $\\mu_i,\\sigma_i,\\alpha_i\\in\\mathbb{R}$, $\\sigma_i>0$, $\\alpha_i\\geq 0$, and $\\sum_{i=1}^k\\alpha_i=1$.\n",
    "\n",
    "In particular, if we wanted to try to approximate a *bimodal distribution*, we might try to fit\n",
    "\n",
    "$$\n",
    "p(X=x;\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2)= \\alpha_1 p(X=x;\\mu_1,\\sigma_1) + \\alpha_2 p(X-x;\\mu_2,\\sigma_2)=\\frac{\\alpha_1}{\\sqrt{2\\pi\\sigma_1^2}}e^{-\\frac{1}{2\\sigma_1^2}(x-\\mu_1)^2} + \\frac{\\alpha_2}{\\sqrt{2\\pi\\sigma_2^2}}e^{-\\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2}\n",
    "$$\n",
    "\n",
    "to a dataset $\\{x_n\\}_{n=1}^N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood for parameter estimation in the two-component Gaussian mixture model\n",
    "\n",
    "The most straightforward way to fit this parametric model to data is to form a likelihood to maximize:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2) = \\prod_{n=1}^N p(X=x_n;\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2).\n",
    "$$\n",
    "\n",
    "This is converted to a negative log-likelihood \n",
    "\n",
    "$$\n",
    "\\ell(\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2) = -\\sum_{n=1}^N\\log p(X=x_n;\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2)= -\\sum_{n=1}^N\\log \\left(\\alpha_1p(X=x_n;\\mu_1,\\sigma_1) + \\alpha_2 p(X=x_n;\\mu_2,\\sigma_2)\\right)\n",
    "$$\n",
    "\n",
    "To find a function which majorizes this sum, we first find a function $g_x(\\theta,\\theta^\\prime)$ which majorizes\n",
    "$$\n",
    "f_x(\\theta)=-\\log\\left(\\alpha_1p(X=x;\\mu_1,\\sigma_1) + \\alpha_2 p(X=x;\\mu_2,\\sigma_2)\\right)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\theta = (\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\theta^\\prime = (\\mu_1^\\prime,\\mu_2^\\prime,\\sigma_1^\\prime,\\sigma_2^\\prime,\\alpha_1^\\prime,\\alpha_2^\\prime).\n",
    "$$\n",
    "We would then have that $\\ell(\\theta)=\\sum_{n=1}^N f_{x_i}(\\theta)$ is majorized by $g_{x_i}(\\theta,\\theta^\\prime)$.\n",
    "\n",
    "We set\n",
    "$$\n",
    "q_i(\\theta) = \\alpha_i p(X=x;\\mu_i,\\sigma_i)\n",
    "$$\n",
    "for $i=1,2$. We claim that\n",
    "$$\n",
    "g_x(\\theta, \\theta^\\prime) = -\\frac{q_1(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(\\frac{q_1(\\theta)}{q_1(\\theta^\\prime)}(q_1(\\theta^\\prime)+q_2(\\theta^\\prime))\\right) -\\frac{q_2(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(\\frac{q_2(\\theta)}{q_2(\\theta^\\prime)}(q_1(\\theta^\\prime)+q_2(\\theta^\\prime))\\right)\n",
    "$$\n",
    "majorizes $f_x(\\theta)$. To simplify a little further, we note that\n",
    "$$\n",
    "\\frac{q_i(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\n",
    "$$\n",
    "is the conditional probility of that $x$ was drawn from component $i$, $p(C=i\\vert X=x;\\theta^\\prime)$. In particular, $p(C=1\\vert X=x;\\theta^\\prime)+p(C=2\\vert X=x;\\theta^\\prime)=1$. Thus,\n",
    "\n",
    "$$\n",
    "g_x(\\theta, \\theta^\\prime) = -p(C=1\\vert X=x;\\theta^\\prime)\\log\\left(\\frac{q_1(\\theta)}{p(C=1\\vert X=x;\\theta^\\prime)}\\right) -p(C=2\\vert X=x;\\theta^\\prime)\\log\\left(\\frac{q_2(\\theta)}{p(C=2\\vert X=x;\\theta^\\prime)}\\right)\n",
    "$$\n",
    "\n",
    "We first note that\n",
    "\n",
    "$$\n",
    "g_x(\\theta^\\prime,\\theta^\\prime)= \\frac{q_1(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(\\frac{q_1(\\theta^\\prime)}{q_1(\\theta^\\prime)}(q_1(\\theta^\\prime)+q_2(\\theta^\\prime))\\right) -\\frac{q_2(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(\\frac{q_2(\\theta^\\prime)}{q_2(\\theta^\\prime)}(q_1(\\theta^\\prime)+q_2(\\theta^\\prime))\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_x(\\theta^\\prime,\\theta^\\prime)=\\frac{q_1(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(q_1(\\theta^\\prime)+q_2(\\theta^\\prime)\\right) -\\frac{q_2(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(q_1(\\theta^\\prime)+q_2(\\theta^\\prime)\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_x(\\theta^\\prime,\\theta^\\prime)=-(p(C=1\\vert X=x;\\theta^\\prime)+p(C=2\\vert X=x;\\theta^\\prime))\\log\\left(q_1(\\theta^\\prime)+q_2(\\theta^\\prime)\\right)=-\\log\\left(q_1(\\theta^\\prime)+q_2(\\theta^\\prime)\\right)=f_x(\\theta^\\prime).\n",
    "$$\n",
    "\n",
    "To show majorization, we see that\n",
    "\n",
    "$$\n",
    "f_x(\\theta)=-\\log\\left(q_1(\\theta)+q_2(\\theta)\\right)=-\\log\\left(p(C=1\\vert X=x;\\theta^\\prime)\\frac{q_1(\\theta)}{p(C=1\\vert X=x;\\theta^\\prime)}+p(C=2\\vert X=x;\\theta^\\prime)\\frac{q_2(\\theta)}{p(C=2\\vert X=x;\\theta^\\prime)}\\right).\n",
    "$$\n",
    "\n",
    "Noting that $-\\log$ is a convex function, we have\n",
    "\n",
    "$$\n",
    "f_x(\\theta)\\leq -p(C=1\\vert X=x;\\theta^\\prime)\\log\\left(\\frac{q_1(\\theta)}{p(C=1\\vert X=x;\\theta^\\prime)}\\right) -p(C=2\\vert X=x;\\theta^\\prime)\\log\\left(\\frac{q_2(\\theta)}{p(C=2\\vert X=x;\\theta^\\prime)}\\right)=g_x(\\theta,\\theta^\\prime).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Expectation-Maximization \n",
    "\n",
    "While majorization-minimization indicates how to proceed to begin minimizing $\\ell(\\theta)$, we now explain why this algorithm is equivalent to the **expectation-maximization** procedure. First, we introduce a likelihood function where the latent variables $c_n$ are known. That is, suppose we have the data $\\mathcal{X}=\\{x_n\\}_{n=1}^N$ and associated latent variables $\\mathcal{C}=\\{i_n\\}_{n=1}^N$. Then the likelihood of the parameter $\\theta$ given this data is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta; \\mathcal{X}, \\mathcal{C}) = \\prod_{n=1}^N \\alpha_{i_n}p(X=x_n; \\mu_{i_n}, \\sigma_{i_n})\n",
    "$$\n",
    "\n",
    "and the negative log-likelihood is given by\n",
    "\n",
    "\n",
    "$$\n",
    "\\ell(\\theta; \\mathcal{X}, \\mathcal{C}) = -\\sum_{n=1}^N \\log\\left(\\alpha_{i_n}p(X=x_n; \\mu_{i_n}, \\sigma_{i_n})\\right)\n",
    "$$\n",
    "\n",
    "For our mixture of two components, the expectation-maximization algorithm has the following form:\n",
    "\n",
    "1. **E Step**: Compute $p(C=i|X=x_n;\\theta^{(k)})$ for $i=1,2$ and $n=1,\\ldots, N$ to form the conditional expectation\n",
    "\n",
    "$$\n",
    "Q(\\theta\\vert\\theta^{(k)})= \\mathbb{E}_{\\mathcal{C}\\vert \\mathcal{X};\\theta^{(k)}} \\log\\mathcal{L}(\\theta;\\mathcal{X}, \\mathcal{C}) = \\mathbb{E}_{\\mathcal{C}\\vert \\mathcal{X};\\theta^{(k)}}\\sum_{n=1}^N \\log\\left(\\alpha_{i_n}p(X=x_n; \\mu_{i_n}, \\sigma_{i_n})\\right) = \\sum_{n=1}^N \\mathbb{E}_{\\mathcal{C}\\vert \\mathcal{X};\\theta^{(k)}}\\log\\left(\\alpha_{i_n}p(X=x_n; \\mu_{i_n}, \\sigma_{i_n})\\right),\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "Q(\\theta\\vert\\theta^{(k)})= \\sum_{n=1}^N p(C=1\\vert X=x_n;\\theta^{(k)})\\log\\left(\\alpha_{1}p(X=x_n; \\mu_{1}, \\sigma_{1})\\right) + p(C=2\\vert X=x_n;\\theta^{(k)})\\log\\left(\\alpha_{2}p(X=x_n; \\mu_{2}, \\sigma_{2})\\right),\n",
    "$$\n",
    "\n",
    "which becomes\n",
    "\n",
    "$$\n",
    "Q(\\theta\\vert\\theta^{(k)})= \\sum_{n=1}^N p(C=1\\vert X=x_n;\\theta^{(k)})\\log\\left(\\alpha_{1}p(X=x_n; \\mu_{1}, \\sigma_{1})\\right) + p(C=2\\vert X=x_n;\\theta^{(k)})\\log\\left(\\alpha_{2}p(X=x_n; \\mu_{2}, \\sigma_{2})\\right),\n",
    "$$\n",
    "\n",
    "2. **M step**: Solve $\\theta^{(k+1)} = \\arg\\max_{\\theta} Q(\\theta\\vert \\theta^{(k)})$. Note that this program is equivalent to the program\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\sum_{n=1}^N g_{x_n}(\\theta, \\theta^{(k)})\n",
    "$$\n",
    "\n",
    "since\n",
    "\n",
    "$$\n",
    "g_{x}(\\theta,\\theta^{(k)})=-p(C=1\\vert X=x;\\theta^{(k)})\\log\\left(\\alpha_{1}p(X=x; \\mu_{1}, \\sigma_{1})\\right) - p(C=2\\vert X=x;\\theta^{(k)})\\log\\left(\\alpha_{2}p(X=x; \\mu_{2}, \\sigma_{2})\\right)\\\\ + p(C=1\\vert X=x;\\theta^{(k)})\\log\\left(p(C=1\\vert X=x;\\theta^{(k)})\\right) + p(C=2\\vert X=x;\\theta^{(k)})\\log\\left(p(C=2\\vert X=x;\\theta^{(k)})\\right),\n",
    "$$\n",
    "\n",
    "and hence\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^N g_{x_n}(\\theta, \\theta^{(k)}) = -Q(\\theta\\vert\\theta^{(k)})\n",
    "+\\sum_{n=1}^N p(C=1\\vert X=x;\\theta^{(k)})\\log\\left(p(C=1\\vert X=x;\\theta^{(k)})\\right) + p(C=2\\vert X=x;\\theta^{(k)})\\log\\left(p(C=2\\vert X=x;\\theta^{(k)})\\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "First, it is relatively simple to compute the quantities\n",
    "\n",
    "$$\n",
    "q_{i,n}=p(C=i\\vert X=x_n;\\theta^{(k)}) = \\frac{\\alpha_i p(X=x_n;\\mu_i^{(k)},\\sigma_i^{(k)})}{\\alpha_1 p(X=x_n;\\mu_1^{(k)},\\sigma_1^{(k)})+\\alpha_2 p(X=x_n;\\mu_2^{(k)},\\sigma_2^{(k)})}\n",
    "$$\n",
    "\n",
    "Next, we observe that\n",
    "\n",
    "$$\n",
    "\\log\\left(\\alpha_i p(X=x_n; \\mu_i,\\sigma_i)\\right)=\\log\\left(\\alpha_i \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}e^{-\\frac{(x_n-\\mu_i)^2}{2\\sigma_i^2}}\\right)=\\log(\\alpha_i)-\\frac{1}{2}\\log(2\\pi)-\\log(\\sigma_i) -\\frac{(x_n-\\mu_i)^2}{2\\sigma_i^2},\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^N g_{x_n}(\\theta,\\theta^{(k)}) = \\sum_{n=1}^N q_{1, n}\\log(\\alpha_1)+\\sum_{n=1}^N q_{2, n}\\log(\\alpha_2)-\\sum_{n=1}^N q_{1, n}\\frac{1}{2}\\log(2\\pi)-\\sum_{n=1}^N q_{2, n}\\frac{1}{2}\\log(2\\pi)-\\sum_{n=1}^N q_{1, n}\\left(\\log(\\sigma_1) +\\frac{(x_n-\\mu_1)^2}{2\\sigma_1^2}\\right)-\\sum_{n=1}^N q_{2, n}\\left(\\log(\\sigma_2) +\\frac{(x_n-\\mu_2)^2}{2\\sigma_2^2}\\right).\n",
    "$$\n",
    "\n",
    "Minimizing subject to the constraint $\\alpha_1+\\alpha_2$ gives us a Lagrange multiplier such that\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{\\alpha_1}\\sum_{n=1}^N q_{1, n}\\\\\n",
    "\\frac{1}{\\alpha_2}\\sum_{n=1}^N q_{2, n}\n",
    "\\end{pmatrix}=\\lambda\\begin{pmatrix} 1\\\\ 1\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and therefore $\\alpha_1^{(k+1)} = \\frac{1}{N} \\sum_{n=1}^N q_{1, n}$ and $\\alpha_2^{(k+1)} = \\frac{1}{N}\\sum_{n=1}^N q_{2, n}$.\n",
    "\n",
    "Taking the gradient with respect to the unconstrained variables $\\mu_1$ and $\\mu_2$, we obtain the necessary conditions\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma_1^2}\\sum_{n=1}^N q_{1, n} (x_n - \\mu_1)=0\\text{ and } \\frac{1}{\\sigma_2^2}\\sum_{n=1}^N q_{2, n} (x_n - \\mu_2)=0\n",
    "$$\n",
    "\n",
    "so that \n",
    "\n",
    "$$\n",
    "\\mu_1^{(k+1)}=\\frac{1}{\\sum_{n=1}^N q_{1, n}}\\sum_{n=1}^N q_{1, n} x_n\\text{ and } \\mu_2^{(k+1)}=\\frac{1}{\\sum_{n=1}^N q_{2, n}}\\sum_{n=1}^N q_{2, n} x_n.\n",
    "$$\n",
    "\n",
    "Finally, taking the gradient with respect to $\\sigma_1$ and $\\sigma_2$ give the conditions\n",
    "\n",
    "$$\n",
    "-\\sum_{n=1}^N q_{i, n}\\left(\\frac{1}{\\sigma_i} -\\frac{(x_n-\\mu_i^{(k+1)})^2}{\\sigma_i^3}\\right)=0\\text{ for }i=1,2\n",
    "$$\n",
    "\n",
    "which reduces to\n",
    "\n",
    "$$\n",
    "\\sigma_i^2 = \\left(\\frac{1}{\\sum_{n=1}^N q_{i, n}}\\sum_{n=1}^N q_{i, n} x_n^2\\right) - \\left(\\mu_i^{(k+1)}\\right)^2\\text{ for }i=1,2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHub/A17w7pcPFJyMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a554400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gauss_1D_pdf(x, mu,sig):\n",
    "    return np.exp(-(x-mu)**2/(2*sig**2))/np.sqrt(2*np.pi*sig**2)\n",
    "\n",
    "def two_gauss_mix_pdf(th):\n",
    "    def f(x):\n",
    "        return th[0] * gauss_1D_pdf(x, th[1], th[2]) + th[3] * gauss_1D_pdf(x, th[4], th[5])\n",
    "    return f\n",
    "    \n",
    "def rand_two_gauss_mix(n, th):\n",
    "    latent = rd.choice(2, n, p=th[[0,3]])\n",
    "    x = rd.randn(n)\n",
    "    return (1-latent)*(th[2]*x + th[1]) + latent*(th[5]*x + th[4])\n",
    "    \n",
    "def estep(x, th):\n",
    "    comp_1_likelihoods = th[0] * gauss_1D_pdf(x, th[1], th[2])\n",
    "    comp_2_likelihoods = th[3] * gauss_1D_pdf(x, th[4], th[5])\n",
    "    normalizations = comp_1_likelihoods + comp_2_likelihoods\n",
    "    cond_probs_1 = comp_1_likelihoods / normalizations\n",
    "    cond_probs_2 = comp_2_likelihoods / normalizations\n",
    "    return np.vstack([cond_probs_1, cond_probs_2])\n",
    "\n",
    "def mstep(th, x, cond_probs):\n",
    "    th_new = np.zeros(6)\n",
    "    sum_cond_probs = np.sum(cond_probs, axis = 1)\n",
    "    th_new[[0, 3]] = sum_cond_probs / x.size\n",
    "    th_new[[1, 4]] = np.sum(cond_probs * x, axis = 1) / sum_cond_probs\n",
    "    th_new[[2, 5]] = np.sqrt((np.sum(cond_probs * x**2, axis=1) / sum_cond_probs) - th_new[[1, 4]]**2)\n",
    "    return th_new\n",
    "\n",
    "rd.seed(1234)\n",
    "\n",
    "theta_true = np.array([0.25, 0, 1, 0.75, 4, 1])\n",
    "\n",
    "num_samples = 100\n",
    "\n",
    "theta_init = [0.5, -1, 2, 0.5, 1, 2]\n",
    "theta = theta_init\n",
    "x = rand_two_gauss_mix(num_samples, theta_true)\n",
    "\n",
    "plt.scatter(x, np.zeros(num_samples))\n",
    "t = np.linspace(np.min(x), np.max(x), 100)\n",
    "f = two_gauss_mix_pdf(theta_true)\n",
    "plt.plot(t, f(t))\n",
    "\n",
    "iters = 1000\n",
    "for it in range(iters):\n",
    "    f_est = two_gauss_mix_pdf(theta)\n",
    "    \n",
    "    cond_probs = estep(x, theta)\n",
    "    #print(cond_probs)\n",
    "    theta = mstep(theta, x, cond_probs)\n",
    "    #print(theta)\n",
    "\n",
    "plt.plot(t, f_est(t))\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Group Problems\n",
    "\n",
    "1. Groups 1 and 2: Derive the EM steps for a mixture of thee Gaussians\n",
    "2. Groups 3 and 4: Derive the EM steps for a mixture of two Gaussians in 2D\n",
    "3. Groups 5 and 6: Derive the EM steps for a mixture of two Cauchy random variables: \n",
    "$$\n",
    "p(X=x;\\mu,\\gamma) = \\frac{1}{\\pi\\gamma\\left(1+\\left(\\frac{(x-\\mu)}{\\gamma}\\right)^2\\right)}\n",
    "$$\n",
    "4. Groups 7 and 8: Derive the EM steps for approximating the joint probability table $P=(p_{i, j})\\in M_{m, n}$ with $p_{i, j}\\geq 0$ and $\\sum_i\\sum_j p_{i, j}=1$ using a latent mixture of two independent components: $\\alpha_1 {\\bf p}_1{\\bf q}_1^T + \\alpha_2 {\\bf p}_2{\\bf q}_2^T$ where $\\alpha_i\\in\\mathbb{R}$, and ${\\bf p}_i\\in\\mathbb{R}^m$, ${\\bf q}_i\\in\\mathbb{R}^n$ for $i=1, 2$ are **probability vectors**. That is, they have non-negative entries and their entries sum to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reasons for using EM\n",
    "\n",
    "1. Perform density estimation for detection/classification using Neymann-Pearson\n",
    "2. Constructs a generative model that is useful for simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
