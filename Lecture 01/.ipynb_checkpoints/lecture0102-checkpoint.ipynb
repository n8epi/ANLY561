{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1 Section 2: *Univariate Solvers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Exact Minimization\n",
    "\n",
    "\n",
    "## Brute force search\n",
    "Theoretically, one could simply employ **brute force search** to minimize a function $f$ by evaluating $f(x)$ for each point in the feasible region. However, our feasible regions have infinitely many points to consider, so this approach is impractical.\n",
    "\n",
    "Recall that a function $f:X\\rightarrow \\mathbb{R}$ is **differentiable** at $x^{(0)}\\in X$ if the limit\n",
    "$$\n",
    "f^\\prime(x^{(0)})=\\lim_{A\\ni x\\rightarrow x^{(0)}} \\frac{f(x)-f(x^{(0)})}{x-x^{(0)}}\n",
    "$$\n",
    "exists. If $f$ is differentiable at $x$ for all $x$ in $X$, then we say that $f$ is **differentiable on** $X$. If $f$ is differentiable on $X$ and the derivative function $f^\\prime$ is continuous on $X$, we say that $f$ is **continuously differentiable** and write $f\\in C^1(X)$. Similarly, if $f^\\prime\\in C^1(X)$, we say that $f\\in C^2(X)$, and if $f^\\prime\\in C^2(X)$ then $f\\in C^3(X)$.\n",
    "\n",
    "The **interior** of a set $X\\subset\\mathbb{R}$, denoted $\\text{int}(X)$ is the largest open subset of $\\mathbb{R}$ in $X$. In particular, $x\\in \\text{int}(X)$ if and only if there is an $\\varepsilon>0$ such that $(x-\\varepsilon, x+\\varepsilon)\\subset X$.\n",
    "\n",
    "### Theorem (Necessary Conditions for Optimality): If $f:X\\rightarrow\\mathbb{R}$ is differentiable on $X$ and $x^\\ast\\in\\text{int}(X)$ is a minimizer of $f$, then $f^\\prime(x^\\ast)=0$.\n",
    "\n",
    "The necessary conditions for optimality are most useful when\n",
    "\n",
    "1. enough is known about $f$ to analytically find all solutions to $f^\\prime(x)=0$, and\n",
    "2. the solution set of $f^\\prime(x)=0$ is either small or easy to characterize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "Example 02: f(x)=(x-1)(x-2)(x-4)(x-8) = x^4 -15x^3  on [0, 6]\n",
    "'''\n",
    "\n",
    "def poly_deriv_coeffs(a):\n",
    "    '''\n",
    "    Computes the coefficients of the derivative of a poly from the coeffs of a poly\n",
    "    :param a: coefficients of a poly in decreasing order of associated degree\n",
    "    :return: coeffs of deriv in decreasing order of the degree\n",
    "    '''\n",
    "    b=[]\n",
    "    for i in range(len(a)-1):\n",
    "        b.append((len(a)-1-i)*a[i])\n",
    "    return b\n",
    "\n",
    "p = lambda x: (x-1) * (x-2) * (x-4) * (x-8)\n",
    "\n",
    "coeffs = np.poly([1, 2, 4, 8])\n",
    "dcoeffs = poly_deriv_coeffs(coeffs)\n",
    "roots = np.roots(dcoeffs) # numpy computes the zeros of the poly's derivative from the coeffs of the deriv\n",
    "\n",
    "feasible_solutions = roots[roots <= 6] # remove all roots above 6\n",
    "feasible_solutions = feasible_solutions[0 <= feasible_solutions] # remove all roots below 0\n",
    "feasible_solutions = np.append(feasible_solutions, [0, 6]) # add endpoints to check\n",
    "\n",
    "feas_vals = [p(t) for t in feasible_solutions]\n",
    "k = np.argmin(feas_vals)\n",
    "\n",
    "x_ast = feasible_solutions[k]\n",
    "p_min = feas_vals[k]\n",
    "\n",
    "print('p attains the minimum value %f at %f on [0, 6]' % (p_min, x_ast))\n",
    "\n",
    "s = np.linspace(0, 6, 100)\n",
    "p_graph = p(s)\n",
    "\n",
    "plt.plot([0, 6], [0, 0], 'k--')\n",
    "plt.plot(s, p_graph)\n",
    "plt.scatter([x_ast], [0])\n",
    "plt.scatter([x_ast], [p_min])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Brute force using necessary conditions')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the points for which $f^\\prime(x)=0$ are called **critical points**. There are several important types of critical points.\n",
    "\n",
    "1. $x$ is a **local minimizer** of $f:X\\rightarrow\\mathbb{R}$ if there is an $\\varepsilon>0$ such that $f(x)\\leq f(y)$ for all $y\\in(x-\\varepsilon, x+\\varepsilon)\\cap X$ (the **intersection** of the $\\varepsilon$-neighborhood around $x$ and $X$).\n",
    "2. $x$ is a **local maximizer** of $f:X\\rightarrow\\mathbb{R}$ if there is an $\\varepsilon>0$ such that $f(x)\\geq f(y)$ for all $y\\in(x-\\varepsilon, x+\\varepsilon)\\cap X$\n",
    "3. $x$ is a **global minimizer** of $f:X\\rightarrow\\mathbb{R}$ if $f(x)\\leq f(y)$ for all $y\\in X$. \n",
    "4. $x$ is a **global maximizer** of $f:X\\rightarrow\\mathbb{R}$ if $f(x)\\geq f(y)$ for all $y\\in X$. \n",
    "\n",
    "Note that a global minimizer in the interior of a set must also be a local minimizer. Thus, the second derivative test allows us to exclude all local maximizers from our search.\n",
    "\n",
    "### Theorem (Second Derivative Test): Let $f\\in C^2(X)$. If $x^\\ast\\in \\text{int}(X)$ is a global minimizer, then $f^{\\prime\\prime}(x^\\ast)\\geq0$.\n",
    "\n",
    "With a one more conditiodn on $f$, the necessary conditions also become sufficient conditions.\n",
    "\n",
    "### Theorem (Sufficient Conditions for Optimality): If $X\\subset\\mathbb{R}$ is a convex set, $f:X\\rightarrow\\mathbb{R}$ is convex and differentiable on $X$, and $x^\\ast\\in X$ satisfies $f^\\prime(x^\\ast)=0$, then $x^\\ast$ is a minimizer of $f$ on $X$.\n",
    "\n",
    "This gives us nice theoretical tools, and illustrates why convexity is so useful. On the other hand, solving $f^\\prime(x)=0$ may be exceedingly difficult, making brute force search on the critical points (and end points!) untenable for many applications.\n",
    "\n",
    "## Team Questions\n",
    "\n",
    "1. Find the minimizer of the function $f(x)=x^3-3x+1$ on the interval $[0, 2]$.\n",
    "2. Find the minimizer of the function $f(x)=\\frac{1}{x}e^x$ on the interval $(0,\\infty)$.\n",
    "3. Find the minimizer of the function $f(x)=\\log(1+e^{-x})+\\log(1+e^{x})$\n",
    "4. Find the minimizer of the function $f(x)=\\vert x-1\\vert + \\vert x-2\\vert + \\vert x-4\\vert$\n",
    "5. Find the minimizer of the function $f(x)=(x-1)^2+(x-2)^2+(x-4)^2$\n",
    "6. Find the minimizer of the function $f(x)=\\max(\\vert x-1\\vert, \\vert x-2\\vert, \\vert x-4\\vert)$\n",
    "7. Find the minimizer of the function $f(x)=x^4-4x+1$ on $[-3, -2]\\cup[3, 4]$\n",
    "8. Which of the above programs are convex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t1 = np.linspace(0, 2)\n",
    "f1 = lambda x: x**3 - 3*x +1\n",
    "\n",
    "t2 = np.linspace(0.1, 2)\n",
    "f2 = lambda x: np.exp(x)/x\n",
    "\n",
    "plt.figure('Graphs')\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(t1, f1(t1))\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(t2, f2(t2))\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Backtracking for Numerical Solutions\n",
    "\n",
    "For problems that do not yield to brute force, we can often implement **iterative** methods that produce a sequence of points with smaller and smaller function values. Such methods generally only rely on local information (i.e. evaluations of the function and its derivative). We now construct a practical algorithm with the following properties:\n",
    "\n",
    "1. The algorithm can be used for any $f$ which is differentiable on $\\mathbb{R}$.\n",
    "2. The algorithm can be initialized with any point $x^{(0)}\\in\\mathbb{R}$ and produces a sequence of **iterates** $x^{(1)}, x^{(2)}, x^{(3)},\\ldots$ where the $(k+1)$th iterate $x^{(k+1)}$ only depends on $x^{(k)}$, $f(x^{(k)})$, $f^\\prime(x^{(k)})$, and two user-defined parameters $\\alpha, \\beta\\in (0,1)$\n",
    "3. The sequence of iterates satisfies $f(x^{(0)}) > f(x^{(1)}) > f(x^{(2)}) >\\cdots$\n",
    "\n",
    "To motivate the algorithm, assume that $f$ is differentiable on $\\mathbb{R}$, let $\\alpha, \\beta \\in (0, 1)$, $x\\in\\mathbb{R}$ with $f^\\prime(x)\\not=0$, and suppose that $\\Delta x$ has the opposite sign as $f^\\prime(x)$. If $f^\\prime(x)$ is positive, then note that $\\lim_{n\\rightarrow\\infty}\\beta^n=0$ and hence\n",
    "$$\n",
    "\\lim_{n\\rightarrow\\infty} \\frac{f(x + \\beta^n \\Delta x) - f(x)}{\\beta^n\\Delta x}=f^\\prime(x) > \\alpha f^\\prime(x).\n",
    "$$\n",
    "Consequently, there is an $N$ such that $n\\geq N$ implies\n",
    "$$\n",
    "\\frac{f(x + \\beta^n \\Delta x) - f(x)}{\\beta^n\\Delta x} > \\alpha f^\\prime(x).\n",
    "$$\n",
    "On the other hand, if $f^\\prime(x)<0$, then there is an $N$ such that $n\\geq N$ such that \n",
    "$$\n",
    "\\frac{f(x + \\beta^n \\Delta x) - f(x)}{\\beta^n\\Delta x} < \\alpha f^\\prime(x).\n",
    "$$\n",
    "In either case, it follows that there is an $n$ such that\n",
    "$$\n",
    "f(x + \\beta^n \\Delta x) - f(x) < \\alpha \\beta^n\\Delta x f^\\prime(x),\n",
    "$$\n",
    "and hence\n",
    "$$\n",
    "f(x + \\beta^n\\Delta x) < f(x) +\\alpha\\beta^n\\Delta x f^\\prime(x) < f(x).\n",
    "$$\n",
    "Thus, we simply need increase $n$ until this condition is satisfied to decrease the function.\n",
    "\n",
    "#### Theorem (Monotonicity of Backtracking): Suppose $f$ is differentiable on $\\mathbb{R}$ and $x^{(0)}\\in\\mathbb{R}$. Define the sequence $x^{(0)}, x^{(1)}, x^{(2)}, x^{(3)}, \\ldots$ by successively setting $x^{(k+1)}=x^{(k)} + \\beta^n\\Delta x^{(k)}$ where $n$ is the first $n\\geq 0$ such that $f(x^{(k)} + \\beta^n\\Delta x^{(k)})\\leq f(x^{(k)}) + \\alpha\\beta^n \\Delta x^{(k)} f^\\prime(x^{(k)})$, and where the increments $\\Delta x^{(1)}, \\Delta x^{(2)}, \\Delta x^{(3)},\\ldots$ which have the opposite signs as $f^\\prime(x^{(0)}),  f^\\prime(x^{(1)}), f^\\prime(x^{(2)}),\\ldots$. Then $f(x^{(0)}) > f(x^{(1)}) > f(x^{(2)}) > f(x^{(3)}) > \\cdots$. \n",
    "\n",
    "Our previous analysis gives us this nice theoretical guarantee, but it also offers quantitative estimates for the decrease from each $f(x^{(k)})$ to $f(x^{(k+1)})$. \n",
    "\n",
    "One immediate question presents itself: how do we choose $\\Delta x^{(k)}$, $\\alpha$, and $\\beta$? Generally we let $\\Delta x^{(k)}=-f^\\prime(x^{(k)})$, which is called **steepest descent**. The choices of $\\alpha$ and $\\beta$ are best explained by example.\n",
    "\n",
    "We now provide code for the backtracking algorithm, and we illustrate how it behaves in a simple scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numerical python and pyplot\n",
    "import numpy as np # Namespace is np\n",
    "import matplotlib.pyplot as plt # Namespace is plt\n",
    "\n",
    "\n",
    "def backtracking1D(x0, dx, f, df0, alpha=0.2, beta=0.8, verbose=False):\n",
    "    '''\n",
    "    Backtracking for 1D functions with illustrations\n",
    "    :param x0: Previous point from backtracking $x^{(k)}$, or initial guess $x^{(0)}$\n",
    "    :param dx: Incremental factor for updating x0; $\\Delta x$\n",
    "    :param f: Objective function\n",
    "    :param df0: Derivative of f at x0 or $f^\\prime(x^{(0)})$\n",
    "    :param alpha: Sloping factor of stopping criterion\n",
    "    :param beta: \"Agressiveness\" parameter for backtracking steps\n",
    "    :param verbose: Boolean for providing plots to illustrate\n",
    "    :return: x1, the next iterate in backtracking, or $x^{(k+1)}$\n",
    "    '''\n",
    "    \n",
    "    print('In backtracking...')\n",
    "    \n",
    "    if verbose:\n",
    "        n=0\n",
    "        xs = [x0 + dx] * 3\n",
    "    \n",
    "    ######################################\n",
    "    # The core of the algorithm\n",
    "    ######################################\n",
    "    delta = alpha * dx * df0 # Just precomputing the alpha times increment times derivative factor\n",
    "    t = 1 # Initialize t=beta**0; beta**n in the loop\n",
    "    f0 = f(x0) # Evaluate for future use\n",
    "    x = x0 + dx # Initialize x_{0, inner}, $x = x^{(0)}+\\beta^0\\Delta x$\n",
    "    fx = f(x)\n",
    "    print(fx)\n",
    "    while (not np.isfinite(fx)) or fx > f0 + delta * t:\n",
    "        print(fx)\n",
    "        t = beta * t\n",
    "        x = x0 + t * dx\n",
    "        fx = f(x)\n",
    "    ###################################### \n",
    "    \n",
    "        if verbose:\n",
    "            n += 1\n",
    "            xs.append(x)\n",
    "            xs.pop(0)\n",
    "            \n",
    "    if verbose: \n",
    "        u = 1.1 * np.abs(xs[0] - x0)\n",
    "        l = 0.1 * np.abs(xs[0] - x0)\n",
    "        if dx < 0:\n",
    "            s = np.linspace(x0 - u, x0 + l, 100)\n",
    "            xi = [x0-u, x0]\n",
    "            fxi = [f(x0) - alpha*u*df0, f(x0)]\n",
    "        else:\n",
    "            s = np.linspace(x0 - l, x0 + u, 100)\n",
    "            xi = [x0, x0 + u]\n",
    "            fxi = [f(x0), f(x0) + alpha*u*df0]\n",
    "            \n",
    "        y = np.zeros(len(s))\n",
    "        for i in range(len(s)):\n",
    "            y[i] = f(s[i]) # Slow for vectorized functions\n",
    "            \n",
    "        plt.figure('Backtracking illustration')\n",
    "        arm, =plt.plot(xi, fxi, '--', label='Armijo Criterion')\n",
    "        fcn, =plt.plot(s, y, label='Objective Function')\n",
    "        plt.plot([s[0], s[-1]], [0, 0], 'k--')\n",
    "        pts =plt.scatter(xs, [0 for p in xs], label='Backtracking points for n=%d, %d, %d' % (n, n+1, n+2))\n",
    "        plt.scatter(xs, [f(p) for p in xs], label='Backtracking points for n=%d, %d, %d' % (n, n+1, n+2))\n",
    "        init =plt.scatter([x0, x0], [0, f(x0)], color='black', label='Initial point')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('f(x)')\n",
    "        plt.legend(handles=[arm, fcn, pts, init])\n",
    "        plt.show()\n",
    "    \n",
    "    return x\n",
    "\n",
    "# We illustrate a few backtracking steps for  simple quadratic\n",
    "\n",
    "fun = lambda x: x**2\n",
    "dfun = lambda x: 2*x\n",
    "\n",
    "x0 = -3 # $x^{(0)}$\n",
    "dx = -dfun(x0) # $\\Delta x^{(0)} = -f^\\prime(x^{(0)})$\n",
    "\n",
    "alpha = 0.5\n",
    "beta = 0.8\n",
    "\n",
    "# First backtracking step\n",
    "#x1 = backtracking1D(x0, dx, fun, dfun(x0), alpha=alpha, beta=beta, verbose=True) # get $x^{(1)}$ from $x^{(0)}$\n",
    "\n",
    "x1 = backtracking1D(x0, -dfun(x0), fun, dfun(x0), alpha=alpha, beta=beta, verbose=True) # get $x^{(1)}$ from $x^{(0)}$\n",
    "\n",
    "# Second backtracking step\n",
    "x2 = backtracking1D(x1, -dfun(x1), fun, dfun(x1), alpha=alpha, beta=beta, verbose=True) # get $x^{(2)}$ from $x^{(1)}$\n",
    "\n",
    "# Third backtracking step\n",
    "x3 = backtracking1D(x2, -dfun(x2), fun, dfun(x2), alpha=alpha, beta=beta, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Value Minimizing Theory and Accelerated Backtracking\n",
    "\n",
    "In the last example, we see that the backtracking iterates $x^{(k)}$ rapidly tend towards $0$, and the **iterate values** $f(x^{(k)})$ also tend toward $f(0)=0$ very quickly. The speed of this convergence is often called the **convergence rate**. Now that we have an example algorithm, we can ask the following questions:\n",
    "\n",
    "1. Is there an algorithm which has a better convergence rate?\n",
    "2. Is there an algorithm which uses fewer evaluations of $f$ and $f^\\prime$?\n",
    "\n",
    "These questions seem very hard to answer, but it is possible to prove the following rate of convergence for the iterate values produced by backtracking.\n",
    "\n",
    "### Theorem: Suppose $f\\in C^2(\\mathbb{R})$ is convex, $x^\\ast$ minimizes $f$ on $\\mathbb{R}$, and there is an $M>0$ such that $\\vert f^{\\prime\\prime}(x)\\vert\\leq M$ for all $x\\in\\mathbb{R}$. If $x^{(0)}\\in\\mathbb{R}$ and $x^{(k)}$ are the iterates obtained from successively applying backtracking with steepest descent increment, then there is a constant $C>0$ such that\n",
    "$$\n",
    "f(x^{(k)})-f(x^\\ast) \\leq \\frac{C}{k}.\n",
    "$$\n",
    "\n",
    "So, we will say that backtracking has an $\\mathcal{O}(1/k)$ convergence rate. Now, is this rate optimal? Nesterov showed that a **first order method** (a method which only relies on the function and its derivative) cannot have a rate better than $C/k^2$, or $\\mathcal{O}(1/k^2)$. In many cases, the convergence rate for backtracking is nearly $1/k^2$, but it will do worse when a function has a very small derivative near the true solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrating the slow rate of convergence for backtracking\n",
    "\n",
    "fun = lambda x: x**10\n",
    "dfun = lambda x: 10*(x**9)\n",
    "\n",
    "x = -0.5 # initial guess\n",
    "\n",
    "iterations = 100000\n",
    "alpha = 0.4\n",
    "beta = 0.8\n",
    "\n",
    "iterates = np.zeros(iterations+1)\n",
    "iterate_values = np.zeros(iterations+1)\n",
    "\n",
    "iterates[0] = x\n",
    "iterate_values[0] = fun(x)\n",
    "\n",
    "for i in range(iterations):\n",
    "    x = backtracking1D(x, -dfun(x), fun, dfun(x), alpha=alpha, beta=beta)\n",
    "    iterates[i+1] = x\n",
    "    iterate_values[i+1] = fun(x)\n",
    "    \n",
    "idx = np.array(list(range(iterations)))+1\n",
    "p1, = plt.semilogy(iterate_values, label='Function values')\n",
    "p2, = plt.semilogy(0.001/idx, label = '$1/k$ rate')\n",
    "p3, = plt.semilogy(1/(idx**2), label = '$1/k^2$ rate')\n",
    "plt.title('Convergence rate comparison')\n",
    "plt.legend(handles=[p1, p2, p3])\n",
    "plt.ylabel('Error = $| f(x^{(k)}) - 0 |$')\n",
    "plt.xlabel('Iterate index $k$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gap leads us to consider **accelerated** methods that may converge more rapidly. In particular, if we use data from two previous iterates, we can attain the optimal theoretical rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accelerated_backtracking1D(k, tk, x0, x1, dx, f, df, beta=0.8, verbose=False):\n",
    "    '''\n",
    "    Accelerated backtracking for 1D functions with illustrations\n",
    "    :param k: Index of the current accelerated backtracking iteration; k=1 for the first\n",
    "    :param tk: The t from the previous accelerated backtracking iteration; tk=1 for the first\n",
    "    :param x0: Next most recent point from accelerated backtracking\n",
    "    :param x1: Most recent point from accelerated backtracking; x1=x0 for the first iteration\n",
    "    :param dx: Incremental factor for updating x1\n",
    "    :param f: Objective function\n",
    "    :param df: Derivative function of f\n",
    "    :param beta: \"Agressiveness\" parameter for backtracking steps\n",
    "    :param verbose: Boolean for providing plots to illustrate\n",
    "    :return: x, t the next iterate and initial t in accelerated backtracking\n",
    "    '''\n",
    "    \n",
    "    y = x1 + (k-1)*(x1 - x0)/(k+2) # Base point for accelerated backtracking\n",
    "    \n",
    "    if verbose:\n",
    "        n=0\n",
    "        xs = [y + tk*dx] * 3\n",
    "    \n",
    "    t = tk # Initialize t from the last iteration; t_0=1\n",
    "    x = y + t*dx\n",
    "    fx = f(x)\n",
    "    fy = f(y)\n",
    "    dfy = df(y)\n",
    "    delta = dfy * dx\n",
    "\n",
    "    \n",
    "    while (not np.isfinite(fx)) or fy + delta*t + t*dx**2/2 < fx:\n",
    "        t = beta * t\n",
    "        x = y + t*dx\n",
    "        fx = f(x)\n",
    "    \n",
    "        if verbose:\n",
    "            n += 1\n",
    "            xs.append(x)\n",
    "            xs.pop(0)\n",
    "            \n",
    "    if verbose: \n",
    "        u = 1.1 * np.abs(xs[0] - y)\n",
    "        l = 0.1 * np.abs(xs[0] - y)\n",
    "        if dx < 0:\n",
    "            s = np.linspace(y - u, y + l, 100)\n",
    "            xi = np.linspace(y-u, y, 100)\n",
    "        else:\n",
    "            s = np.linspace(y - l, y + u, 100)\n",
    "            xi = np.linspace(y, y + u, 100)\n",
    "        dxi = xi-y\n",
    "        fxi = fy + dfy*dxi + dxi*dx/2\n",
    "            \n",
    "        z = np.zeros(len(s))\n",
    "        for i in range(len(s)):\n",
    "            z[i] = f(s[i]) # Slow for vectorized functions\n",
    "            \n",
    "        plt.figure('Accelerated Backtracking illustration')\n",
    "        plt.plot([s[0], s[-1]], [0, 0], 'k--')\n",
    "        arm, =plt.plot(xi, fxi, '--', label='Stopping Criterion')\n",
    "        fcn, =plt.plot(s, z, label='Objective Function')\n",
    "        pts =plt.scatter(xs, [0 for p in xs], label='Backtracking points for n=%d, %d, %d' % (n, n+1, n+2))\n",
    "        plt.scatter(xs, [f(p) for p in xs], label='Backtracking points for n=%d, %d, %d' % (n, n+1, n+2))\n",
    "        init =plt.scatter([y], [fy], label='Initial point', color='black')\n",
    "        plt.legend(handles=[arm, fcn, pts, init])\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('f(x)')\n",
    "        plt.show()\n",
    "    \n",
    "    return x, t\n",
    "\n",
    "fun = lambda x: x**2\n",
    "dfun = lambda x: 2*x\n",
    "\n",
    "x0 = -3\n",
    "x1 = backtracking1D(x0, -dfun(x0), fun, dfun(x0))\n",
    "dx = -dfun(x1)\n",
    "t0 = 1\n",
    "beta = 0.8\n",
    "\n",
    "x2, t1 = accelerated_backtracking1D(1, t0, x0, x1, dx, fun, dfun, beta=beta, verbose=True)\n",
    "x3, t2 = accelerated_backtracking1D(2, t1, x1, x2, -dfun(x2), fun, dfun, beta=beta, verbose=True)\n",
    "x4, t3 = accelerated_backtracking1D(3, t2, x2, x3, -dfun(x3), fun, dfun, beta=beta, verbose=True)\n",
    "x5, t4 = accelerated_backtracking1D(4, t3, x3, x4, -dfun(x3), fun, dfun, beta=beta, verbose=True)\n",
    "x6, t5 = accelerated_backtracking1D(5, t4, x4, x5, -dfun(x3), fun, dfun, beta=beta, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrating the slow rate of convergence for backtracking\n",
    "\n",
    "fun = lambda x: x**10\n",
    "dfun = lambda x: 10*(x**9)\n",
    "\n",
    "x = -0.5 # initial guess\n",
    "\n",
    "iterations = 100000\n",
    "alpha = 0.4\n",
    "beta = 0.8\n",
    "\n",
    "iterates = np.zeros(iterations)\n",
    "iterate_values = np.zeros(iterations)\n",
    "\n",
    "iterates[0] = x\n",
    "iterates[1] = x\n",
    "iterate_values[0] = fun(x)\n",
    "iterate_values[1] = fun(x)\n",
    "\n",
    "t = 1\n",
    "\n",
    "for i in range(2, iterations):\n",
    "    x, t = accelerated_backtracking1D(i+1, t, iterates[i-2], iterates[i-1], -dfun(iterates[i-1]), fun, dfun, beta=beta)\n",
    "    iterates[i] = x\n",
    "    iterate_values[i] = fun(x)\n",
    "    \n",
    "idx = np.array(list(range(iterations)))+1\n",
    "p1, = plt.semilogy(iterate_values, label='Function values')\n",
    "p2, = plt.semilogy(0.0000001/idx, label = '$1/k$ rate')\n",
    "p3, = plt.semilogy(0.0001/(idx**2), label = '$1/k^2$ rate')\n",
    "plt.title('Accelerated convergence rate comparison')\n",
    "plt.legend(handles=[p1, p2, p3])\n",
    "plt.ylabel('Error = $| f(x^{(k)}) - 0 |$')\n",
    "plt.xlabel('Iterate index $k$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that acceleration manages to compensate for small derivatives. In particular, we see that accelerated steepest descent has a rate better than $\\mathcal{O}(1/k^2)$ *in this particular case*. The theory also shows that accelerated backtracking achieves the optimal rate.\n",
    "\n",
    "### Theorem: Suppose $f\\in C^2(\\mathbb{R})$ is convex, $x^\\ast$ minimizes $f$ over $\\mathbb{R}$, and that there is an $M>0$ such that $\\vert f^{\\prime\\prime}(x)\\vert\\leq M$ for all $x\\in\\mathbb{R}$. Then, for the sequence $x^{(1)}, x^{(2)}, x^{(3)},\\ldots$ produced by accelerated backtracking, there is a constant $C>0$ such that\n",
    "$$\n",
    "f(x^{(k)})-f(x^\\ast) \\leq \\frac{C}{k^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV: Approximate Solution Theory\n",
    "\n",
    "What if the value of the function is less important than convergence to a minimizer? In this case, methods like **Fibonnaci search** and **golden section search** are able to provide nearly optimal approximations to solutions given a fixed number of function evaluations. We will not consider these algorithms, and instead we will focus on techniques that leverage backtracking.\n",
    "\n",
    "## Steepest descent with constant step size\n",
    "\n",
    "Instead of employing backtracking, we can simply choose a parameter $\\tau\\in (0,1)$ to produce iterates of the form\n",
    "$$\n",
    "x^{(k+1)}=x^{(k)} - \\tau f^\\prime(x^{(k)}).\n",
    "$$\n",
    "This is called **damped steepest descent**, or steepest descent with constant step size. While it may not be the most efficient procedure, it can be analyzed a little more easily.\n",
    "\n",
    "### Theorem: Suppose $f\\in C^2(\\mathbb{R})$ and there is an $M>0$ and a $c>0$ such that $c<f^{\\prime\\prime}(x)<M$ for all $x\\in\\mathbb{R}$. Then there is a $\\gamma\\in(0,1)$ such that the sequence $x^{(k+1)} = x^{(k)} - \\frac{1}{M}f^\\prime(x^{(k)})$ satisfies $\\vert x^{(k+1)}-x^\\ast\\vert \\leq \\gamma\\vert x^{(k)}-x^\\ast\\vert$ for all $k$ given any initial $x^{(0)}$.\n",
    "\n",
    "The proof uses the Mean Value Theorem:\n",
    "\n",
    "$$\n",
    "x^\\ast - x^{(k+1)} = x^\\ast - x^{(k)} + \\frac{1}{M} f^\\prime(x^{(k)})=x^\\ast - x^{(k)} + \\frac{1}{M} f^{\\prime\\prime}(\\xi)(x^{(k)}-x^\\ast) = \\left(1 - \\frac{1}{M} f^{\\prime\\prime}(\\xi)\\right)(x^\\ast - x^{(k)})\n",
    "$$\n",
    "for some $\\xi$ between $x^{(k)}$ and $x^\\ast$, and hence\n",
    "$$\n",
    "\\vert x^\\ast - x^{(k+1)}\\vert = \\left\\vert 1-\\frac{1}{M} f^{\\prime\\prime}(\\xi)\\right\\vert \\vert x^\\ast - x^{(k)}\\vert.\n",
    "$$\n",
    "The inequality $c< f^{\\prime\\prime}(\\xi) < M$ implies $\\frac{c}{M}< \\frac{f^{\\prime\\prime}(\\xi)}{M} < 1$, and hence $0<1-\\frac{f^{\\prime\\prime}(\\xi)}{M}< 1- \\frac{c}{M}<1$. We conclude that the theorem holds with $\\gamma=1-\\frac{c}{M}$.\n",
    "\n",
    "We call the quantity $\\vert x^{(k)}-x^\\ast\\vert$ the **error** of the approximation $x^{(k)}$ to $x^\\ast$. The type of convergence discussed in the previous theorem is called **linear convergence** of the error, though it is also sometimes called **exponential convergence** since it implies that the error decays exponentially:\n",
    "$$\n",
    "\\vert x^{(k)}-x^\\ast\\vert \\leq \\gamma^k\\vert x^{(0)}-x^\\ast\\vert.\n",
    "$$\n",
    "\n",
    "\n",
    "## Newton's method\n",
    "\n",
    "Newton's method is a way to get even faster convergence to solutions, but it requires evaluation of $f^{\\prime\\prime}$ at each step. The Newton iterates are defined by\n",
    "$$\n",
    "x^{(k+1)} = x^{(k)} - \\frac{f^\\prime(x^{(k)})}{f^{\\prime\\prime}(x^{(k)})}\n",
    "$$\n",
    "We let $C^3(\\mathbb{R})$ denote the space of functions over $\\mathbb{R}$ with three continuous derivatives.\n",
    "\n",
    "### Theorem (Quadratic Convergence of Newton's Method): Suppose $f\\in C^3(\\mathbb{R})$, $f^\\prime(x^\\ast)=0$, there is a constant $c>0$ such that $f^{\\prime\\prime}(x^\\ast)\\geq c$, and there is a constant $K>0$ such that $\\vert f^{\\prime\\prime\\prime}(x)\\vert \\leq K$ for all $x\\in\\mathbb{R}$. If $x^{(0)}$ satisfies $\\vert x^{(0)}-x^\\ast\\vert\\leq \\frac{2c}{3K}$, then the Newton iterates initialized with $x^{(0)}$ satisfy $\\vert x^{(k)}-x^\\ast\\vert\\leq \\frac{2c}{3K}$ and $\\vert x^{(k+1)} - x^\\ast\\vert \\leq \\frac{3K}{2c} \\vert x^{(k)}-x^\\ast\\vert^2$ for all $k\\geq 0$. \n",
    "\n",
    "The **quadratic convergence** here is also known as **doubly exponential convergence** since it implies \n",
    "$$\n",
    "\\vert x^{(k)}-x^\\ast\\vert \\leq \\gamma^{2^k-1}\\vert x^{(0)}-x^\\ast\\vert\n",
    "$$\n",
    "for some $\\gamma\\in(0,1)$. However, convergence in this theorem is contingent on the fact that $\\vert x^{(0)}-x^\\ast\\vert < \\frac{2c}{3K}$. It turns out that Newton's method may produce an unbounded sequence of iterates. On the other hand, we can always feed the $\\Delta x$ from Newton's method to backtracking to stabilize Newton's method. \n",
    "\n",
    "The following plots illustrate convergence rates for iterates and function values for these procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.log(1 - 9*np.log(10)/np.log(9/10))/np.log(2)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Example 03: f(x) = x*arctan(x) - log(1+x^2)/2, f'(x)=arctan(x), f''(x)=1/(1+x^2)\n",
    "'''\n",
    "\n",
    "f = lambda x: x * np.arctan(x) - np.log(1+x**2)/2 # minimum value is 0 at x=0\n",
    "df = lambda x: np.arctan(x)\n",
    "d2f = lambda x: 1/(1+x**2) # Note that M=1, but strong convexity does not hold\n",
    "\n",
    "iter = 30 # 30 iterations of each\n",
    "x0 = 10\n",
    "\n",
    "x_sd = [x0]\n",
    "f_sd = [f(x0)]\n",
    "x = x0\n",
    "for i in range(iter):\n",
    "    x = x - df(x)/2 # Using the constant stepsize 2 > 1\n",
    "    x_sd.append(x)\n",
    "    f_sd.append(f(x))\n",
    "    \n",
    "x_nm = [x0]\n",
    "f_nm = [f(x0)]\n",
    "x = x0\n",
    "for i in range(iter):\n",
    "    x = x - df(x)/d2f(x) # Using the constant stepsize 2 > 1\n",
    "    x_nm.append(x)\n",
    "    f_nm.append(f(x))\n",
    "    \n",
    "# Compare convergence of function values with semilog plot\n",
    "sd, =plt.semilogy(f_sd, label='Steepest descent')\n",
    "nm, =plt.semilogy(f_nm, label='Newton\\'s method')\n",
    "plt.xlabel('Iteration Index')\n",
    "plt.ylabel('Iterate Value Error')\n",
    "plt.legend(handles=[sd, nm])\n",
    "plt.title('Semilog plot of function values')\n",
    "plt.show()\n",
    "\n",
    "# Compare convergece of iterates to the minimizer\n",
    "sd, =plt.semilogy(np.abs(x_sd), label='Steepest descent')\n",
    "nm, =plt.semilogy(np.abs(x_nm), label='Newton\\'s method')\n",
    "plt.ylabel('Iterate Error')\n",
    "plt.xlabel('Iteration Index')\n",
    "plt.legend(handles=[sd, nm])\n",
    "plt.title('Semilog plot of iterate error')\n",
    "plt.show()\n",
    "\n",
    "# Let's finish with a comparison with backtracking and accelerated backtracking\n",
    "\n",
    "x_sd_bt = [x0]\n",
    "f_sd_bt = [f(x0)]\n",
    "x = x0\n",
    "for i in range(iter):\n",
    "    x = backtracking1D(x, -df(x), f, df(x))\n",
    "    x_sd_bt.append(x)\n",
    "    f_sd_bt.append(f(x))\n",
    "\n",
    "x_nm_bt = [x0]\n",
    "f_nm_bt = [f(x0)]\n",
    "x = x0\n",
    "for i in range(iter):\n",
    "    x = backtracking1D(x, -df(x)/d2f(x), f, df(x))\n",
    "    x_nm_bt.append(x)\n",
    "    f_nm_bt.append(f(x))\n",
    "    \n",
    "x_sd_abt = [x0]\n",
    "f_sd_abt = [f(x0)]\n",
    "x = backtracking1D(x0, -df(x0), f, df(x0))\n",
    "x_sd_abt.append(x)\n",
    "f_sd_abt.append(f(x))\n",
    "t = 1\n",
    "for i in range(2,iter):\n",
    "    x, t = accelerated_backtracking1D(i+1, t, x_sd_abt[i-2], x_sd_abt[i-1], -df(x), f, df)\n",
    "    x_sd_abt.append(x)\n",
    "    f_sd_abt.append(f(x))\n",
    "\n",
    "x_nm_abt = [x0]\n",
    "f_nm_abt = [f(x0)]\n",
    "x = backtracking1D(x0, -df(x0)/d2f(x0), f, df(x0))\n",
    "x_nm_abt.append(x)\n",
    "f_nm_abt.append(f(x))\n",
    "t = 1\n",
    "for i in range(2, iter):\n",
    "    x, t = accelerated_backtracking1D(i+1, t, x_nm_abt[i-2], x_nm_abt[i-1], -df(x)/d2f(x), f, df)\n",
    "    x_nm_abt.append(x)\n",
    "    f_nm_abt.append(f(x))\n",
    "\n",
    "# Compare convergence of function values with semilog plot\n",
    "sd, =plt.semilogy(f_sd, label='Steepest descent')\n",
    "nm, =plt.semilogy(f_nm, label='Newton\\'s method')\n",
    "sd_bt, = plt.semilogy(f_sd_bt, label='SD Backtracking')\n",
    "nm_bt, = plt.semilogy(f_nm_bt, label='NM Backtracking')\n",
    "sd_abt, = plt.semilogy(f_sd_abt, label='SD Backtracking+')\n",
    "nm_abt, = plt.semilogy(f_nm_abt, label='NM Backtracking+')\n",
    "plt.xlabel('Iteration Index')\n",
    "plt.ylabel('Iterate Value Error')\n",
    "plt.legend(handles=[sd, nm, sd_bt, nm_bt, sd_abt, nm_abt])\n",
    "plt.title('Semilog plot of function values')\n",
    "plt.show()\n",
    "\n",
    "# Compare convergece of iterates to the minimizer\n",
    "sd, =plt.semilogy(np.abs(x_sd), label='Steepest descent')\n",
    "nm, =plt.semilogy(np.abs(x_nm), label='Newton\\'s method')\n",
    "sd_bt, = plt.semilogy(np.abs(x_sd_bt), label='SD Backtracking')\n",
    "nm_bt, = plt.semilogy(np.abs(x_nm_bt), label='NM Backtracking')\n",
    "sd_abt, = plt.semilogy(np.abs(x_sd_abt), label='SD Backtracking+')\n",
    "nm_abt, = plt.semilogy(np.abs(x_nm_abt), label='NM Backtracking+')\n",
    "plt.xlabel('Iteration Index')\n",
    "plt.ylabel('Iterate Error')\n",
    "plt.legend(handles=[sd, nm, sd_bt, nm_bt, sd_abt, nm_abt])\n",
    "plt.title('Semilog plot of iterate error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Problems\n",
    "\n",
    "1. Write down a function $f\\in C^1(\\mathbb{R})$ and an initial point $x^{(0)}$ such that backtracking with steepest descent does not produce a sequence which converges to the *global minimizer* of $f$ over $\\mathbb{R}$.\n",
    "2. Write down a function $f\\in C^1(\\mathbb{R})$ and an initial point $x^{(0)}$ such that backtracking with steepest descent does not produce a sequence which converges to a *local minimizer* of $f$ over $\\mathbb{R}$.\n",
    "3. Suppose you code up a function ``fun`` and its derivative ``dfun`` in python, and you observe that backtracking at a particular $x^{(0)}$ produces the iterates $x^{(1)}=x^{(0)}$, $x^{(2)}=x^{(0)}$, and so forth. That is, backtracking does not move from $x^{(0)}$. What are two possible reasons for this?\n",
    "4. Can backtracking with steepest descent ever converge more rapidly than accelerated backtracking with steepest descent?\n",
    "5. Does accelerated backtracking produce a sequence of iterates for which the iterate values are monotone decreasing?\n",
    "6. If an iterative method converges linearly, what should a semilog plot of the error as a function of the iteration index look like? What about if the method converges quadratically?\n",
    "7. In the above plots, why does backtracking with steepest descent appear to converge quadratically?\n",
    "8. In the above example, why does simple backtracking have a faster convergence rate than accelerated backtracking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V: Constrained Univariate Optimization\n",
    "\n",
    "All of the numerical procedures we have considered so far involve the numerical minimization over $\\mathbb{R}$. If we are instead optimizing over $[a, b]$ for some $a, b\\in\\mathbb{R}$ with $a<b$, then we need to ensure that our iterative methods always return a new point in $[a,b]$. Clearly, backtracking may be modified to be careful of such constraints, but the **log barrier method** also provides a way to carry out optimization in the constrained setting.\n",
    "\n",
    "Instead of solving\n",
    "$$\n",
    "\\min f(x)\\text{ subject to }x\\in[a,b]\n",
    "$$\n",
    "we solve a sequence of programs\n",
    "$$\n",
    "\\min f(x)-\\frac{1}{t}\\log(x-a)-\\frac{1}{t}\\log(b-x)\n",
    "$$\n",
    "where $t\\rightarrow\\infty$. The benefit here is that these functions produce NaNs if $x\\leq a$ or $x\\geq b$, which we can catch in the backtracking loop using the ``not np.isfinite(fx)`` conditional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def lb1D(x, a, b):\n",
    "    '''\n",
    "    Log barrier value of x between a and b\n",
    "    :param x: x between a and b\n",
    "    :param a: lower endpoint\n",
    "    :param b: upper endpoint\n",
    "    :return: evaluation of the log barrier function\n",
    "    '''\n",
    "    return -np.log(x-a)-np.log(b-x)\n",
    "\n",
    "def dlb1D(x, a, b):\n",
    "    '''\n",
    "    Log barrier derivative at x between a and b\n",
    "    :param x: x between a and b\n",
    "    :param a: lower endpoint\n",
    "    :param b: upper endpoint\n",
    "    :return: evaluation of the log barrier derivative\n",
    "    '''\n",
    "    return 1/(b-x) - 1/(x-a)\n",
    "\n",
    "def d2lb1D(x, a, b):\n",
    "    '''\n",
    "    Log barrier 2nd derivative x between a and b\n",
    "    :param x: x between a and b\n",
    "    :param a: lower endpoint\n",
    "    :param b: upper endpoint\n",
    "    :return: evaluation of the log barrier 2nd derivative\n",
    "    '''\n",
    "    return 1/((b-x)**2) + 1/((x-a)**2)\n",
    "\n",
    "a=0\n",
    "b=1\n",
    "x = np.linspace(a+1e-6, b-1e-6, 1000)\n",
    "lb = lambda z: lb1D(z, a=0, b=1)\n",
    "y = lb(x)\n",
    "\n",
    "plt.figure('Log Barrier Function')\n",
    "plt.plot(x, lb(x))\n",
    "plt.plot(x, lb(x)/2)\n",
    "plt.plot(x, lb(x)/4)\n",
    "plt.plot(x, lb(x)/8)\n",
    "plt.plot(x, lb(x)/16)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Barrier Value')\n",
    "plt.title('The log barrier function as t approaches $\\infty$')\n",
    "plt.show()\n",
    "\n",
    "def log_barrier_opt_1D(a, b, x0, f, df, d2f=None, al=0.2, be=0.8, M=10, init_iter=5, out_iter=25, in_iter=15, verbose=False):\n",
    "    '''\n",
    "    Perform optimization of f using the log barrier method in 1D\n",
    "    :param a: lower bound of feasible region\n",
    "    :param b: upper bound of feasible region\n",
    "    :param x0: inital guess -- must satisfy a < x0 < b or this breaks\n",
    "    :param f: objective function\n",
    "    :param df: derivative of objective function\n",
    "    :param d2f: optional second derivative, invokes Newton steps\n",
    "    :param al: alpha for the backtracking calls\n",
    "    :param be: beta for the backtracking calls\n",
    "    :param M: increase factor for t\n",
    "    :param init_iter: number of initial backtracking calls\n",
    "    :param out_iter: number of outer iterations to perform\n",
    "    :param in_iter: number of inner iterations to perform\n",
    "    :param verbose: True generates illustrative plots\n",
    "    '''\n",
    "    \n",
    "    # First, approximate the solution with t=1\n",
    "    x = x0\n",
    "    if verbose:\n",
    "        pts = [x0]\n",
    "    \n",
    "    for i in range(init_iter):\n",
    "        flb = lambda z: f(z) + lb1D(z, a, b)\n",
    "        dflb0 = df(x) + dlb1D(x, a, b)\n",
    "        dx = -dflb0\n",
    "        if d2f is not None:\n",
    "            dx = dx / (d2f(x) + d2lb1D(x, a, b))\n",
    "        x = backtracking1D(x, dx, flb, dflb0, alpha=al, beta=be)\n",
    "        if verbose:\n",
    "            pts.append(x)\n",
    "            \n",
    "    if verbose:\n",
    "        s = np.linspace(a+1e-6, b-1e-6, 100)\n",
    "        y = np.zeros(100)\n",
    "        q = np.zeros(len(pts))\n",
    "        for i in range(100):\n",
    "            y[i] = flb(s[i])\n",
    "        for i in range(len(pts)):\n",
    "            q[i] = flb(pts[i])\n",
    "        \n",
    "        fl = min(np.min(q), 0)\n",
    "        fu = max(np.max(q), 0)\n",
    "            \n",
    "        interval_length = np.max(pts) - np.min(pts)\n",
    "        range_length = fu - fl\n",
    "            \n",
    "        l = np.min(pts) - 0.1*interval_length\n",
    "        u = np.max(pts) + 0.1*interval_length\n",
    "        fl = np.min(q) - 0.1*range_length\n",
    "        fu = np.max(q) + 0.1*range_length\n",
    "        \n",
    "        plt.plot([s[0], s[-1]], [0, 0], 'k--')\n",
    "        obj, =plt.plot(s, y, label='Objective plus barrier')\n",
    "        bt =plt.scatter(pts, np.zeros(len(pts)), label='Backtracking')\n",
    "        vals =plt.scatter(pts, q, label='Values')\n",
    "        init =plt.scatter([pts[-1]], 0, label='Initial center')\n",
    "        plt.axis([l, u, min(fl,0), max(fu,0)])\n",
    "        plt.legend(handles=[obj, bt, vals, init])\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('f(x) plus barrier')\n",
    "        plt.title('Initial centering steps')\n",
    "        plt.show()\n",
    "    \n",
    "    # Now begin the outer iterations\n",
    "    t=1\n",
    "    for i in range(out_iter):\n",
    "        t = M * t\n",
    "        if verbose:\n",
    "            pts = [x]\n",
    "        for j in range(in_iter):\n",
    "            flb = lambda z: f(z) + lb1D(z, a, b)/t\n",
    "            dflb0 = df(x) + dlb1D(x, a, b)/t\n",
    "            dx = -dflb0\n",
    "            if d2f is not None:\n",
    "                dx = dx / (d2f(x) + d2lb1D(x, a, b)/t)\n",
    "            x = backtracking1D(x, dx, flb, dflb0, alpha=al, beta=be)\n",
    "            pts.append(x)\n",
    "            \n",
    "        if verbose:\n",
    "            s = np.linspace(a+1e-6, b-1e-6, 100)\n",
    "            y = np.zeros(100)\n",
    "            q = np.zeros(len(pts))\n",
    "            for k in range(100):\n",
    "                y[k] = flb(s[k])\n",
    "            for k in range(len(pts)):\n",
    "                q[k] = flb(pts[k])\n",
    "                \n",
    "            fl = min(np.min(q), 0)\n",
    "            fu = max(np.max(q), 0)\n",
    "            \n",
    "            interval_length = np.max(pts) - np.min(pts)\n",
    "            range_length = fu - fl\n",
    "            \n",
    "            l = np.min(pts) - 0.1*interval_length\n",
    "            u = np.max(pts) + 0.1*interval_length\n",
    "            fl = np.min(q) - 0.1*range_length\n",
    "            fu = np.max(q) + 0.1*range_length\n",
    "        \n",
    "            obj, =plt.plot(s, y, label=('Objective plus barrier at t=%f' % t))\n",
    "            bt =plt.scatter(pts, np.zeros(len(pts)), label='Inner loop iterates')\n",
    "            outer =plt.scatter([pts[-1]], 0, label='Outer loop iterate', color='red')\n",
    "            vals =plt.scatter(pts, q, label='Values at iterates')\n",
    "            plt.axis([l, u, min(fl, 0), max(fu, 0)])\n",
    "            plt.legend(handles=[obj, bt, outer, vals])\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('f(x) plus barrier')\n",
    "            plt.title('Log barrier steps at outer iteration %d' % i)\n",
    "            plt.show()\n",
    "            \n",
    "    return x\n",
    "\n",
    "a=0.5\n",
    "b=3\n",
    "x0=1\n",
    "f=lambda x: np.exp(x)/(x**2)\n",
    "df=lambda x: f(x) - 2* np.exp(x)/(x**3)\n",
    "\n",
    "x_approx = log_barrier_opt_1D(a, b, x0, f, df, verbose=True, out_iter=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Problems\n",
    "\n",
    "1. Perform one step of backtracking use steepest descent increments on the function $f(x)=x^4+x^2+1$ with $x^{(0)}=1$.\n",
    "2. Perform one step of backtracking use Newton's method increments on the function $f(x)=x^4+x^2+1$ with $x^{(0)}=1$.\n",
    "2. Perform one step of backtracking using steepest descent increments on the function $f(x)=x^2-\\log(x-1)-\\log(3-x)$ with $x^{(0)}=2$\n",
    "4. Perform one step of backtracking using Newton's method increments on the function $f(x)=x^2-\\log(x-1)-\\log(3-x)$ with $x^{(0)}=2$?\n",
    "5. Perform one step of backtracking using steepest descent increments on the function $f(x)=x^2-\\frac{1}{2}\\log(x-1)-\\frac{1}{2}\\log(3-x)$ with $x^{(0)}=3/2$\n",
    "6. Perform one step of backtracking using Newton's method increments on the function $f(x)=x^2-\\frac{1}{2}\\log(x-1)-\\frac{1}{4}\\log(3-x)$ with $x^{(0)}=3/2$\n",
    "7. Perform one step of backtracking using steepest descent increments on the function $f(x)=x^2-\\frac{1}{4}\\log(x-1)-\\frac{1}{4}\\log(3-x)$ with $x^{(0)}=5/4$\n",
    "8. Perform one step of backtracking using Newton's method  increments on the function $f(x)=x^2-\\frac{1}{4}\\log(x-1)-\\frac{1}{4}\\log(3-x)$ with $x^{(0)}=5/4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
