{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4 Part II: Backpropagation\n",
    "\n",
    "## Block Jacobians and the Chain Rule\n",
    "\n",
    "Suppose $f\\in C^1\\left(\\mathcal{T}_{{\\bf n}_1}\\times\\mathcal{T}_{{\\bf n}_2};\\mathcal{T}_{{\\bf k}_1}\\right)$, $g\\in C^1\\left(\\mathcal{T}_{{\\bf m}_1}\\times\\mathcal{T}_{{\\bf m}_2};\\mathcal{T}_{{\\bf k}_2}\\right)$, and $h\\in C^1\\left(\\mathcal{T}_{{\\bf k}_1}\\times\\mathcal{T}_{{\\bf k}_2}, \\mathcal{T}_{{\\bf l}}\\right)$, then the function $\\varphi(\\mathcal{V}, \\mathcal{W},\\mathcal{X},\\mathcal{Y}) = h(f(\\mathcal{V},\\mathcal{W}),g(\\mathcal{X},\\mathcal{Y}))$ satisfies $\\varphi\\in C^1\\left(\\mathcal{T}_{{\\bf n}_1}\\times\\mathcal{T}_{{\\bf n}_2}\\times\\mathcal{T}_{{\\bf m}_1}\\times\\mathcal{T}_{{\\bf m}_2};\\mathcal{T}_{\\bf l}\\right)$. Because of the Cartesian product, we can't write down a single tensor for our Jacobian. Instead, we can get Jacobians for each block of variables:\n",
    "$$\n",
    "D_{\\mathcal{V}}\\varphi(\\mathcal{V}, \\mathcal{W},\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf n}_1},\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{W}}\\varphi(\\mathcal{V}, \\mathcal{W},\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf n}_2},\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{X}}\\varphi(\\mathcal{V}, \\mathcal{W},\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf m}_1},\n",
    "$$\n",
    "and\n",
    "$$\n",
    "D_{\\mathcal{Y}}\\varphi(\\mathcal{V}, \\mathcal{W},\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf m}_2},\n",
    "$$\n",
    "Thinking of $h$ as $h(\\mathcal{F},\\mathcal{G})$, we also have the Jacobian blocks\n",
    "$$\n",
    "D_{\\mathcal{F}} h(\\mathcal{F},\\mathcal{G})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf k}_1},\\: D_{\\mathcal{G}} h(\\mathcal{F},\\mathcal{G})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf k}_2},\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{V}} f(\\mathcal{V},\\mathcal{W})\\in \\mathcal{T}_{{\\bf k}_1\\oplus{\\bf n}_1},\\: D_{\\mathcal{W}} f(\\mathcal{V},\\mathcal{W})\\in \\mathcal{T}_{{\\bf k}_1\\oplus{\\bf n}_2},\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{X}} g(\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf k}_2\\oplus{\\bf m}_1},\\: D_{\\mathcal{Y}} g(\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf k}_2\\oplus{\\bf m}_2}\n",
    "$$\n",
    "The chain rule in this block formulation is then (suppressing arguments)\n",
    "$$\n",
    "D_{\\mathcal{V}}\\varphi_{({\\bf i},{\\bf j})} = c(D_{\\mathcal{F}}h, D_{\\mathcal{V}} f)_{({\\bf i},{\\bf j})}=\\left(\\sum_{\\bf k} \\frac{\\partial h_{\\bf i}}{\\partial f_{\\bf k}} \\frac{\\partial f_{\\bf k}}{\\partial v_{\\bf j}}\\right)_{({\\bf i},{\\bf j})}\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{W}}\\varphi_{({\\bf i},{\\bf j})} = c(D_{\\mathcal{F}}h, D_{\\mathcal{W}} f)_{({\\bf i},{\\bf j})}=\\left(\\sum_{\\bf k} \\frac{\\partial h_{\\bf i}}{\\partial f_{\\bf k}} \\frac{\\partial f_{\\bf k}}{\\partial w_{\\bf j}}\\right)_{({\\bf i},{\\bf j})}\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{X}}\\varphi_{({\\bf i},{\\bf j})} = c(D_{\\mathcal{G}}h, D_{\\mathcal{X}} g)_{({\\bf i},{\\bf j})}=\\left(\\sum_{\\bf k} \\frac{\\partial h_{\\bf i}}{\\partial g_{\\bf k}} \\frac{\\partial g_{\\bf k}}{\\partial x_{\\bf j}}\\right)_{({\\bf i},{\\bf j})}\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{Y}}\\varphi_{({\\bf i},{\\bf j})} = c(D_{\\mathcal{G}}h, D_{\\mathcal{Y}} g)_{({\\bf i},{\\bf j})}=\\left(\\sum_{\\bf k} \\frac{\\partial h_{\\bf i}}{\\partial g_{\\bf k}} \\frac{\\partial g_{\\bf k}}{\\partial y_{\\bf j}}\\right)_{({\\bf i},{\\bf j})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "Consider a feedforward neural network with a single hidden layer. The objective function may be written as\n",
    "$$\n",
    "\\sum_{i=1}^N f_2({\\bf y}^{(i)}, f_1({\\bf x}^{(i)}; W, {\\bf b}); V, {\\bf c})\n",
    "$$\n",
    "for $W\\in \\mathcal{T}_{(k,d)}, {\\bf b}\\in \\mathcal{T}_{(k)}, V\\in\\mathcal{T}_{(k, m)}, {\\bf c}\\in\\mathcal{T}_{(m)}$. We will compute the gradient in blocks. First, we note that\n",
    "$$\n",
    "\\nabla_W\\sum_{i=1}^N f_2({\\bf y}^{(i)}, f_1({\\bf x}^{(i)}; W, {\\bf b}); V, {\\bf c})=\\sum_{i=1}^N \\nabla_W f_2({\\bf y}^{(i)}, f_1({\\bf x}^{(i)}; W, {\\bf b}); V, {\\bf c}).\n",
    "$$\n",
    "Moreover, the gradient is simply the transpose of the Jacobian, so we will simply compute\n",
    "$$\n",
    "D_W f_2({\\bf y}, f_1({\\bf x}; W, {\\bf b}); V, {\\bf c}).\n",
    "$$\n",
    "Viewing $f_2$ as $f_2({\\bf y},\\xi; V, {\\bf c})$, we then have that this Jacobian is the standard contraction of $D_\\xi f_2({\\bf y}, f_1({\\bf x}; W, {\\bf b}); V, {\\bf c})$ with $D_W f_1({\\bf x}; W, {\\bf b})$. Similarly, \n",
    "$$\n",
    "D_{\\bf b} f_2({\\bf y}, f_1({\\bf x}; W, {\\bf b}); V, {\\bf c})\n",
    "$$\n",
    "is the contraction of $D_\\xi f_2({\\bf y}, f_1({\\bf x}; W, {\\bf b}); V, {\\bf c})$ with $D_{\\bf b} f_1({\\bf x}; W, {\\bf b})$. Because $V$ and ${\\bf c}$ do not factor through compositions, their blocks are computable without the chain rule.\n",
    "\n",
    "This is a good start, but a single layer is generally determined by the composition of two functions. Let's consider a full composition of the form\n",
    "\n",
    "$$\n",
    "r(W_1, {\\bf b}^{(1)}, W_2, {\\bf b}^{(2)})=\\ell(Y; \\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})))\n",
    "$$\n",
    "\n",
    "with \n",
    "\n",
    "\\begin{align}\n",
    "\\ell(Y; Q) &= -\\sum_{n=1}^N y_0^{(i)}\\log q_0^{(i)} + y_1^{(i)}\\log q_1^{(i)},\\\\\n",
    "\\psi_2(Z_2)&=\\text{softmax}(Z_2) \\text{ (applied to each row)},\\\\\n",
    "\\phi_2(X_2; W_2, {\\bf b}^{(2)}) &= X_2 W_2 +{\\bf b}^{(2)},\\\\\n",
    "\\psi_1(Z_1)&=\\text{logit}(Z_1)\\text{ (applied to each entry), and}\\\\\n",
    "\\phi_1(X_1; W_1, {\\bf b}^{(1)})&=X_1 W_1+{\\bf b}^{(1)}.\n",
    "\\end{align}\n",
    "\n",
    "And the chain rule gives us\n",
    "$$\\tiny\n",
    "D_{{\\bf b}^{(1)}} r = D_Q\\ell(Y;\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})))\\star D_{Z_2}\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)}))\\star D_{X_2}\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})\\star D_{Z_1}\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})) \\star D_{{\\bf b}^{(1)}} \\phi_1(X; W_1, {\\bf b}^{(1)})\n",
    "$$\n",
    "$$\\tiny\n",
    "D_{W_1} r = D_Q\\ell(Y;\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})))\\star D_{Z_2}\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)}))\\star D_{X_2}\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})\\star D_{Z_1}\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})) \\star D_{W_1} \\phi_1(X; W_1, {\\bf b}^{(1)})\n",
    "$$\n",
    "\n",
    "$$\\small\n",
    "D_{{\\bf b}^{(2)}} r = D_Q\\ell(Y;\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})))\\star D_{Z_2}\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)}))\\star D_{{\\bf b}^{(2)}}\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})\n",
    "$$\n",
    "\n",
    "$$\\small\n",
    "D_{W_2} r = D_Q\\ell(Y;\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})))\\star D_{Z_2}\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)}))\\star D_{W_2}\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})\n",
    "$$\n",
    "\n",
    "where we have used $\\star$ to denote contraction over indices associated with the variables of differentiation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/n8/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:38: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Avg Cross Entropy: 0.655622, Gradient Norm: 6.340024, Training Accuracy: 77.0 percent\n",
      "Step: 1, Avg Cross Entropy: 0.645090, Gradient Norm: 3.860301, Training Accuracy: 92.2 percent\n",
      "Step: 2, Avg Cross Entropy: 0.642563, Gradient Norm: 3.222856, Training Accuracy: 88.8 percent\n",
      "Step: 3, Avg Cross Entropy: 0.641745, Gradient Norm: 1.341964, Training Accuracy: 90.8 percent\n",
      "Step: 4, Avg Cross Entropy: 0.641555, Gradient Norm: 0.446901, Training Accuracy: 89.8 percent\n",
      "Step: 5, Avg Cross Entropy: 0.641458, Gradient Norm: 0.559665, Training Accuracy: 91.2 percent\n",
      "Step: 6, Avg Cross Entropy: 0.641177, Gradient Norm: 0.804886, Training Accuracy: 90.0 percent\n",
      "Step: 7, Avg Cross Entropy: 0.641008, Gradient Norm: 0.406676, Training Accuracy: 91.0 percent\n",
      "Step: 8, Avg Cross Entropy: 0.640909, Gradient Norm: 0.515593, Training Accuracy: 89.5 percent\n",
      "Step: 9, Avg Cross Entropy: 0.640662, Gradient Norm: 0.723036, Training Accuracy: 90.8 percent\n",
      "Step: 10, Avg Cross Entropy: 0.640509, Gradient Norm: 0.372525, Training Accuracy: 89.2 percent\n",
      "Step: 11, Avg Cross Entropy: 0.640180, Gradient Norm: 0.863126, Training Accuracy: 90.5 percent\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-52fb61df1398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mngrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_backtracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-52fb61df1398>\u001b[0m in \u001b[0;36mdf\u001b[0;34m(var)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Jacobian for psi_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mD_Z1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ1shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDlogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mback_prop1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mback_prop2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_X2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_Z1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ1shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# Jacobians for phi_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-52fb61df1398>\u001b[0m in \u001b[0;36mchain_rule\u001b[0;34m(Dg, Df, var_shape)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mDg_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mDf_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDg_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDf_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Compute the Jacobian blocks of X @ W + b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/n8/anaconda/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes)\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \u001b[0mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewaxes_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewshape_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m     \u001b[0mbt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewaxes_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewshape_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0molda\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moldb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "def chain_rule(Dg, Df, var_shape):\n",
    "    # Computes the Jacobian D (g o f)\n",
    "    dim = len(var_shape) # Extracting the shape of the intermediate variable (i.e Y for g(Y), Y=f(X))\n",
    "    Dg_axes = list(range(Dg.ndim-dim, Dg.ndim)) # Last \"dim\" indices of the Dg \n",
    "    Df_axes = list(range(dim)) # First \"dim\" indices of the Df tensor\n",
    "    return np.tensordot(Dg, Df, axes=(Dg_axes, Df_axes))\n",
    "\n",
    "# Compute the Jacobian blocks of X @ W + b\n",
    "\n",
    "def DX_affine(X, W, b):\n",
    "    # (d_{x_{i, j}} (X @ W))_{a, b} = e_a^T e_ie_j^T W e_b, so a,i slices equal W.T\n",
    "    D = np.zeros((X.shape[0], W.shape[1], X.shape[0], X.shape[1])) # Number of elements: X.shape[0]**2 * W.shape[1] * X.shape[1] \n",
    "    for k in range(X.shape[0]):\n",
    "        D[k,:,k,:]=W.T # Changes X.shape[0]*W.shape[1]*X.shape[1]\n",
    "    # The fraction of nonzero entries of D to total entries of D is 1/X.shape[0]\n",
    "    # For the BCW dataset with X.shape[0]=400, the D tensor is very *sparse*\n",
    "    return D, X.shape\n",
    "\n",
    "# We note that this Jacobian is large and sparse. However, the information for constructing D is contained\n",
    "# entirely in W. The whole goal of defining this Jacobian is to use it in some chain rule. If the affine function\n",
    "# is acting as an outer function, then the chain rule requires summation over the last two index positions. If it is\n",
    "# the inner function of a composition, we would sum over the first two indices. \n",
    "\n",
    "def DX_affine_outer_closure(X,W):\n",
    "    def chn_rule(df):\n",
    "        # df is the Jacobian of the inner function\n",
    "        l = len(df.shape)\n",
    "        D = np.zeros((X.shape[0], W.shape[1], df.shape[2], df.shape[3])\n",
    "        for  k in range(X.shape[0])\n",
    "            D[k,:,:,:] =  np.tensordot(W, df[k,:,:,:], axes=([0], [0]))\n",
    "        return D\n",
    "    return chn_rule\n",
    "                     \n",
    "def DW_affine(X, W, b):\n",
    "    # (d_{w_{i, j}} (X @ W))_{a, b} = e_a^T X e_ie_j^T e_b, so b, j slices equal x\n",
    "    D = np.zeros((X.shape[0], W.shape[1], W.shape[0], W.shape[1]))\n",
    "    for k in range(W.shape[1]):\n",
    "        D[:,k,:,k]=X\n",
    "    return D, W.shape\n",
    "\n",
    "def Db_affine(X, W, b):\n",
    "    # (d_{b_i} (1 @ b))_{a, b} = e_a^T 1 e_i^T e_b, so b, i slices are all ones\n",
    "    D = np.zeros((X.shape[0], W.shape[1], b.shape[1]))\n",
    "    for k in range(b.shape[1]):\n",
    "        D[:,k,k]=1\n",
    "    return D, b.shape\n",
    "    \n",
    "def logit(z):\n",
    "    # This is vectorized\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def Dlogit(Z):\n",
    "    # The Jacobian of the matrix logit\n",
    "    D = np.zeros((Z.shape[0], Z.shape[1], Z.shape[0], Z.shape[1]))\n",
    "    A = logit(Z) * logit(-Z)\n",
    "    for i in range(Z.shape[0]):\n",
    "        for j in range(Z.shape[1]):\n",
    "            D[i,j,i,j] = A[i,j]\n",
    "    return D, Z.shape\n",
    "\n",
    "def softmax(z):\n",
    "    v = np.exp(z)\n",
    "    return v / np.sum(v)\n",
    "\n",
    "def matrix_softmax(Z):\n",
    "    return np.apply_along_axis(softmax, 1, Z)\n",
    "\n",
    "def Dmatrix_softmax(Z):\n",
    "    D = np.zeros((Z.shape[0], Z.shape[1], Z.shape[0], Z.shape[1]))\n",
    "    for k in range(Z.shape[0]):\n",
    "        v = np.exp(Z[k,:])\n",
    "        v = v / np.sum(v)\n",
    "        D[k,:,k,:] = np.diag(v) - np.outer(v,v)\n",
    "        #print(D[k,:,k,:])\n",
    "    return D, Z.shape\n",
    "\n",
    "def cross_entropy(P, Q):\n",
    "    return -np.sum(P * np.log(Q))/P.shape[0]\n",
    "\n",
    "def DQcross_entropy(P, Q):\n",
    "    return - P * (1/Q)/P.shape[0], Q.shape\n",
    "\n",
    "def nn_loss_closure(X, Y):\n",
    "    # vars[0]=W_1, vars[1]=b_1, vars[2]=W_2, vars[3]=b_2\n",
    "    # cross_entropy(Y, matrix_softmax(affine(logit(affine(X; W_1, b_1))); W_2, b_2))\n",
    "    def f(var):\n",
    "        return cross_entropy(Y, matrix_softmax((logit((X @ var[0]) + var[1]) @ var[2]) + var[3]))\n",
    "    return f\n",
    "\n",
    "def nn_loss_gradient_closure(X, Y):\n",
    "    def df(var):\n",
    "        # Activation of first layer\n",
    "        Z1 = (X @ var[0]) + var[1]\n",
    "        X2 = logit(Z1)\n",
    "        \n",
    "        # Activation of second layer\n",
    "        Z2 = (X2 @ var[2]) + var[3]\n",
    "        Q = matrix_softmax(Z2)\n",
    "        \n",
    "        # Backpropagation tells us we can immediately contract DQ DZ2\n",
    "        D_Q, Qshape = DQcross_entropy(Y, Q)\n",
    "        D_Z2, Z2shape = Dmatrix_softmax(Z2)\n",
    "        back_prop2 = chain_rule(D_Q, D_Z2, Qshape)\n",
    "        \n",
    "        # Jacobians for phi_2\n",
    "        D_X2, X2shape = DX_affine(X2, var[2], var[3])\n",
    "        D_W2, W2shape = DW_affine(X2, var[2], var[3])\n",
    "        D_b2, b2shape = Db_affine(X2, var[2], var[3])\n",
    "        \n",
    "        # Jacobian for psi_1\n",
    "        D_Z1, Z1shape = Dlogit(Z1)\n",
    "        back_prop1 = chain_rule(chain_rule(back_prop2, D_X2, X2shape), D_Z1, Z1shape)\n",
    "        \n",
    "        # Jacobians for phi_1\n",
    "        D_W1, W1shape = DW_affine(X, var[0], var[1])\n",
    "        D_b1, b1shape = Db_affine(X, var[0], var[1])\n",
    "        \n",
    "        # Compute all the gradients\n",
    "        W1grad = chain_rule(back_prop1, D_W1, W1shape)\n",
    "        b1grad = chain_rule(back_prop1, D_b1, b1shape)\n",
    "        W2grad = chain_rule(back_prop2, D_W2, W2shape)\n",
    "        b2grad = chain_rule(back_prop2, D_b2, b2shape)\n",
    "        \n",
    "        return [W1grad, b1grad, W2grad, b2grad]\n",
    "    return df\n",
    "\n",
    "def update_blocks(x,y,t):\n",
    "    # An auxiliary function for backtracking with blocks of variables\n",
    "    num_blocks = len(x)\n",
    "    z = [None]*num_blocks\n",
    "    for i in range(num_blocks):\n",
    "        z[i] = x[i] + t*y[i]\n",
    "    return z\n",
    "                           \n",
    "def block_backtracking(x0, f, dx, df0, alpha=0.1, beta=0.5, verbose=False):\n",
    "    num_blocks = len(x0)\n",
    "    \n",
    "    delta = 0\n",
    "    for i in range(num_blocks):\n",
    "        delta = delta + np.sum(dx[i] * df0[i])\n",
    "    delta = alpha * delta\n",
    "    \n",
    "    f0 = f(x0)\n",
    "    \n",
    "    t = 1\n",
    "    x = update_blocks(x0, dx, t)\n",
    "    fx = f(x)\n",
    "    while (not np.isfinite(fx)) or f0+t*delta<fx:\n",
    "        t = beta*t\n",
    "        x = update_blocks(x0, dx, t)\n",
    "        fx = f(x)\n",
    "        \n",
    "    if verbose:\n",
    "        print((t, delta))\n",
    "        l=-1e-5\n",
    "        u=1e-5\n",
    "        s = np.linspace(l, u, 64)\n",
    "        fs = np.zeros(s.size)\n",
    "        crit = f0 + s*delta\n",
    "        tan = f0 + s*delta/alpha\n",
    "        for i in range(s.size):\n",
    "            fs[i] = f(update_blocks(x0, dx, s[i]))\n",
    "        plt.plot(s, fs)\n",
    "        plt.plot(s, crit, '--')\n",
    "        plt.plot(s, tan, '.')\n",
    "        plt.scatter([0], [f0])\n",
    "        plt.show()\n",
    "            \n",
    "    return x, fx\n",
    "\n",
    "def negate_blocks(x):\n",
    "    # Helper function for negating the gradient of block variables\n",
    "    num_blocks = len(x)\n",
    "    z = [None]*num_blocks\n",
    "    for i in range(num_blocks):\n",
    "        z[i] = -x[i]\n",
    "    return z\n",
    "\n",
    "def block_norm(x):\n",
    "    num_blocks=len(x)\n",
    "    z = 0\n",
    "    for i in range(num_blocks):\n",
    "        z = z + np.sum(x[i]**2)\n",
    "    return np.sqrt(z)\n",
    "\n",
    "def random_matrix(shape, sigma=0.1):\n",
    "    # Helper for random initialization\n",
    "    return np.reshape(sigma*rd.randn(shape[0]*shape[1]), shape)\n",
    "\n",
    "### Begin gradient descent example\n",
    "\n",
    "### Random seed\n",
    "rd.seed(1234)\n",
    "\n",
    "data = load_breast_cancer() # Loads the Wisconsin Breast Cancer dataset (569 examples in 30 dimensions)\n",
    "\n",
    "# Parameters for the data\n",
    "dim_data = 30\n",
    "num_labels = 2\n",
    "num_examples = 569\n",
    "\n",
    "# Parameters for training\n",
    "num_train = 400\n",
    "\n",
    "X = data['data'] # Data in rows\n",
    "targets = data.target # 0-1 labels\n",
    "labels = np.zeros((num_examples, num_labels))\n",
    "for i in range(num_examples):\n",
    "    labels[i,targets[i]]=1 # Conversion to one-hot representations\n",
    "\n",
    "# Prepare hyperparameters of the network\n",
    "hidden_nodes = 20\n",
    "\n",
    "# Initialize variables\n",
    "W1_init = random_matrix((dim_data, hidden_nodes))\n",
    "b1_init = np.zeros((1, hidden_nodes))\n",
    "\n",
    "W2_init = random_matrix((hidden_nodes, num_labels))\n",
    "b2_init = np.zeros((1, num_labels))\n",
    "\n",
    "x = [W1_init, b1_init, W2_init, b2_init]\n",
    "f = nn_loss_closure(X[:num_train,:], labels[:num_train,:])\n",
    "df = nn_loss_gradient_closure(X[:num_train,:], labels[:num_train,:])\n",
    "dx = lambda v: negate_blocks(df(v))\n",
    "    \n",
    "for i in range(100):\n",
    "    ngrad = dx(x)\n",
    "    x, fval = block_backtracking(x, f, ngrad, df(x), alpha=0.1, verbose=False)\n",
    "    \n",
    "    train_data = matrix_softmax(logit(X[:num_train,:]@x[0] + x[1]) @ x[2] + x[3])\n",
    "    train_labels = np.argmax(train_data, axis=1)\n",
    "    per_correct = 100*(1 - np.count_nonzero(train_labels - targets[:num_train])/num_train)\n",
    "\n",
    "    print(\"Step: %d, Avg Cross Entropy: %f, Gradient Norm: %f, Training Accuracy: %.1f percent\" % (i,fval,block_norm(ngrad), per_correct))\n",
    "    \n",
    "test_data = matrix_softmax(logit(X[num_train:,:]@x[0] + x[1]) @ x[2] + x[3])\n",
    "test_labels = np.argmax(test_data, axis=1)\n",
    "per_correct = 100*(1 - np.count_nonzero(test_labels - targets[num_train:])/(num_examples-num_train))\n",
    "\n",
    "print('Final test accuracy: %.1f percent' % per_correct)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Consider the functions $f_1, f_2, f_3:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ as $f_1(x_1,\\theta_1)$, $f_2(x_2,\\theta_2)$, and $f_3(x_3,\\theta_3)$. Then define\n",
    "\n",
    "$$\n",
    "g(x; \\theta_1, \\theta_2, \\theta_3) = f_3(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3).\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\frac{\\partial g}{\\partial\\theta_1}(x;\\theta_1, \\theta_2, \\theta_3)=\\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\\frac{\\partial}{\\partial\\theta_1}\\left[f_2(f_1(x,\\theta_1),\\theta_2)\\right] + \\frac{\\partial f_3}{\\partial \\theta_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\\frac{\\partial \\theta_3}{\\partial\\theta_1}=\\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\\left[\\frac{\\partial f_2}{\\partial x_2}(f_1(x,\\theta_1),\\theta_2)\\frac{\\partial f_1}{\\partial\\theta_1}(x,\\theta_1) + \\frac{\\partial f_2}{\\partial \\theta_2}(f_1(x,\\theta_1),\\theta_2)\\frac{\\partial \\theta_2}{\\partial \\theta_1}\\right] = \\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\\frac{\\partial f_2}{\\partial x_2}(f_1(x,\\theta_1),\\theta_2)\\frac{\\partial f_1}{\\partial\\theta_1}(x,\\theta_1).\n",
    "$$\n",
    "\n",
    "Similarly,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial g}{\\partial\\theta_2}(x;\\theta_1, \\theta_2, \\theta_3) = \\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\\frac{\\partial f_2}{\\partial \\theta_2}(f_1(x,\\theta_1),\\theta_2)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial g}{\\partial\\theta_3}(x;\\theta_1, \\theta_2, \\theta_3) = \\frac{\\partial f_3}{\\partial \\theta_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3).\n",
    "$$\n",
    "\n",
    "Another way to see this is to view this as the sequence of maps\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\theta_1\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_1(x,\\theta_1)\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_2(f_1(x,\\theta_1),\\theta_2)\\\\\n",
    "\\theta_3\n",
    "\\end{pmatrix}\\longmapsto f_3(f_2(f_1(x,\\theta_1),\\theta_2)\\theta_3).\n",
    "$$\n",
    "\n",
    "The Jacobians of these maps are\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f_1}{\\partial\\theta_1}(x,\\theta_1) & 0 & 0\\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}, \\begin{pmatrix}\n",
    "\\frac{\\partial f_2}{\\partial x_2}(f_1(x,\\theta_1),\\theta_2) & \\frac{\\partial f_2}{\\partial \\theta_2}(f_1(x,\\theta_1),\\theta_2) & 0\\\\\n",
    "0 & 0 & 1\\\\\n",
    "\\end{pmatrix},\\text{ and } \\begin{pmatrix}\n",
    "\\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3) & \\frac{\\partial f_3}{\\partial \\theta_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The interesting thing to note here is that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\n",
    "$$\n",
    "\n",
    "is a factor for two of these partial derivatives. This redundancy is more pronounced as we get a deeper composition. Consider the composition of maps\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\theta_1\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\\\\\n",
    "\\theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_1(x,\\theta_1)\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\\\\\n",
    "\\theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_2(f_1(x,\\theta_1),\\theta_2)\\\\\n",
    "\\theta_3\\\\\n",
    "\\theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_3(f_2(f_1(x,\\theta_1),\\theta_2)\\theta_3)\\\\\n",
    "\\theta_4\n",
    "\\end{pmatrix}\\longmapsto f_4(f_3(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3),\\theta_4).\n",
    "$$\n",
    "\n",
    "The Jacobian of this composition (by the chain rule)\n",
    "\n",
    "$$\n",
    "\\tiny\\begin{pmatrix}\n",
    "\\frac{\\partial f_4}{\\partial x_4}(f_3(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3), \\theta_4) & \\frac{\\partial f_4}{\\partial \\theta_4}(f_3(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3), \\theta_4)\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "\\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3) & \\frac{\\partial f_3}{\\partial \\theta_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3) & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "\\frac{\\partial f_2}{\\partial x_2}(f_1(x,\\theta_1),\\theta_2) & \\frac{\\partial f_2}{\\partial \\theta_2}(f_1(x,\\theta_1),\\theta_2) & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "\\frac{\\partial f_1}{\\partial\\theta_1}(x,\\theta_1) & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{pmatrix}, \n",
    "$$\n",
    "\n",
    "This means that the gradient has the form (with suppression of arguments):\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f_4}{\\partial x_4} \\frac{\\partial f_3}{\\partial x_3} \\frac{\\partial f_2}{\\partial x_2} \\frac{\\partial f_1}{\\partial \\theta_1}\\\\\n",
    "\\frac{\\partial f_4}{\\partial x_4} \\frac{\\partial f_3}{\\partial x_3} \\frac{\\partial f_2}{\\partial \\theta_2}\\\\\n",
    "\\frac{\\partial f_4}{\\partial x_4} \\frac{\\partial f_3}{\\partial \\theta_3}\\\\\n",
    "\\frac{\\partial f_4}{\\partial \\theta_4}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "This suggests the following computational structure for computing a gradient descent update using step size $\\eta>0$:\n",
    "\n",
    "1. $\\theta_4^\\prime = \\theta_4 - \\eta \\frac{\\partial f_4}{\\partial \\theta_4}$ and set $q=\\frac{\\partial f_4}{\\partial x_4}$.\n",
    "2. For $i=3, 2, 1$: set $\\theta_i^\\prime = \\theta_i -\\eta q \\frac{\\partial f_i}{\\partial \\theta_i}$ and $q= q \\frac{\\partial f_i}{\\partial x_i}$\n",
    "\n",
    "This is a simplified version of the **backpropagation** algorithm (or, backprop).  Now, let's suppose that $f_1({\\bf x}_1, \\Theta_1)$, $f_2({\\bf x}_2, \\Theta_2)$, $f_3({\\bf x}_3,\\Theta_3)$, and $f_4({\\bf x}_4, \\Theta_4)$ where the ${\\bf x}$'s and $\\Theta$'s are vectors of parameters. Then our composition has a *block form* given by\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\Theta_1\\\\\n",
    "\\Theta_2\\\\\n",
    "\\Theta_3\\\\\n",
    "\\Theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_1({\\bf x},\\Theta_1)\\\\\n",
    "\\Theta_2\\\\\n",
    "\\Theta_3\\\\\n",
    "\\Theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_2(f_1({\\bf x},\\Theta_1),\\Theta_2)\\\\\n",
    "\\Theta_3\\\\\n",
    "\\Theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2)\\Theta_3)\\\\\n",
    "\\Theta_4\n",
    "\\end{pmatrix}\\longmapsto f_4(f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2),\\Theta_3),\\Theta_4).\n",
    "$$\n",
    "\n",
    "and the chain rule gives the Jacobian (in block form)\n",
    "\n",
    "$$\n",
    "\\tiny\\begin{pmatrix}\n",
    "D_{{\\bf x}_4}f_4(f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2),\\Theta_3), \\Theta_4) & D_{\\Theta_4}f_4(f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2),\\Theta_3), \\Theta_4)\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "D_{{\\bf x}_3} f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2),\\Theta_3) & D_{\\Theta_3} f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2),\\Theta_3) & {\\bf 0}\\\\\n",
    "{\\bf 0} & {\\bf 0} & I\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "D_{{\\bf x}_2} f_2(f_1({\\bf x},\\Theta_1),\\Theta_2) & D_{\\Theta_2} f_2(f_1({\\bf x},\\Theta_1),\\Theta_2) & {\\bf 0} & {\\bf 0}\\\\\n",
    "{\\bf 0} & {\\bf 0} & I & {\\bf 0}\\\\\n",
    "{\\bf 0} & {\\bf 0} & {\\bf 0} & I\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "D_{\\Theta_1} f_1({\\bf x},\\Theta_1) & {\\bf 0} & {\\bf 0} & {\\bf 0}\\\\\n",
    "{\\bf 0} & I & {\\bf 0} & {\\bf 0}\\\\\n",
    "{\\bf 0} & {\\bf 0} & I & {\\bf 0}\\\\\n",
    "{\\bf 0} & {\\bf 0} & {\\bf 0} & I\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Therefore we can represent the gradient in the block form\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\left(D_{\\Theta_1}\\: f_1\\right)^T \\left(D_{{\\bf x}_2}\\: f_2\\right)^T \\left(D_{{\\bf x}_3}\\: f_3\\right)^T\\nabla_{{\\bf x}_4}\\: f_4\\\\\n",
    "\\left(D_{\\Theta_2}\\: f_2\\right)^T \\left(D_{{\\bf x}_3}\\: f_3\\right)^T\\nabla_{{\\bf x}_4}\\: f_4\\\\\n",
    "\\left(D_{\\Theta_3}\\: f_3\\right)^T\\nabla_{{\\bf x}_4}\\: f_4\\\\\n",
    "\\nabla_{\\Theta_4}\\: f_4\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The most general form of backpropagation can be explained in terms of tensor contraction. \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
